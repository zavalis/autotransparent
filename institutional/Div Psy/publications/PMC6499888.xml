<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2023-02-07T13:04:23Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:6499888" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:6499888</identifier>
        <datestamp>2019-05-17</datestamp>
        <setSpec>scirep</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
              <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
              <journal-title-group>
                <journal-title>Scientific Reports</journal-title>
              </journal-title-group>
              <issn pub-type="epub">2045-2322</issn>
              <publisher>
                <publisher-name>Nature Publishing Group UK</publisher-name>
                <publisher-loc>London</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC6499888</article-id>
              <article-id pub-id-type="pmcid">PMC6499888</article-id>
              <article-id pub-id-type="pmc-uid">6499888</article-id>
              <article-id pub-id-type="pmid">31053793</article-id>
              <article-id pub-id-type="publisher-id">39697</article-id>
              <article-id pub-id-type="doi">10.1038/s41598-019-39697-y</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Regularity is not a key factor for encoding repetition in rapid image streams</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author" corresp="yes">
                  <name>
                    <surname>Thunell</surname>
                    <given-names>Evelina</given-names>
                  </name>
                  <address>
                    <email>evelina.thunell@cnrs.fr</email>
                  </address>
                  <xref ref-type="aff" rid="Aff1"/>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Thorpe</surname>
                    <given-names>Simon J.</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff1"/>
                </contrib>
                <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0723 035X</institution-id><institution-id institution-id-type="GRID">grid.15781.3a</institution-id><institution>Centre de Recherche Cerveau et Cognition (CerCo), Centre National de la Recherche Scientifique (CNRS), </institution><institution>Université Paul Sabatier, </institution></institution-wrap>Toulouse, France </aff>
              </contrib-group>
              <pub-date pub-type="epub">
                <day>3</day>
                <month>5</month>
                <year>2019</year>
              </pub-date>
              <pub-date pub-type="pmc-release">
                <day>3</day>
                <month>5</month>
                <year>2019</year>
              </pub-date>
              <pub-date pub-type="collection">
                <year>2019</year>
              </pub-date>
              <volume>9</volume>
              <elocation-id>6872</elocation-id>
              <history>
                <date date-type="received">
                  <day>30</day>
                  <month>7</month>
                  <year>2018</year>
                </date>
                <date date-type="accepted">
                  <day>29</day>
                  <month>1</month>
                  <year>2019</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© The Author(s) 2019</copyright-statement>
                <license license-type="OpenAccess">
                  <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
                </license>
              </permissions>
              <abstract id="Abs1">
                <p id="Par1">Human observers readily detect targets and repetitions in streams of rapidly presented visual stimuli. It seems intuitive that regularly spaced repeating items should be easier to detect than irregularly spaced ones, since regularity adds predictability and in addition has ecological relevance. Here, we show that this is not necessarily the case, and we point out the intrinsic difficulty in addressing this question. We presented long RSVP streams of never-before-seen natural images containing repetition sequences; an image appearing six times interleaved by one or more non-repeating distractors, and asked participants to detect the repetitions and to afterwards identify the repeated images. We found that the ability to detect and memorize repeated images was preserved even with irregular sequences, and conclude that temporal regularity is not a key factor for detection and memory for repeating images in RSVP streams. These findings have implications for models of repetition processing.</p>
              </abstract>
              <kwd-group kwd-group-type="npg-subject">
                <title>Subject terms</title>
                <kwd>Cognitive neuroscience</kwd>
                <kwd>Learning and memory</kwd>
                <kwd>Visual system</kwd>
              </kwd-group>
              <funding-group>
                <award-group>
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100000781</institution-id>
                      <institution>EC | European Research Council (ERC)</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>323711</award-id>
                  <award-id>323711</award-id>
                  <principal-award-recipient>
                    <name>
                      <surname>Thunell</surname>
                      <given-names>Evelina</given-names>
                    </name>
                    <name>
                      <surname>Thorpe</surname>
                      <given-names>Simon J.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
              </funding-group>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>issue-copyright-statement</meta-name>
                  <meta-value>© The Author(s) 2019</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
          </front>
          <body>
            <sec id="Sec1" sec-type="introduction">
              <title>Introduction</title>
              <p id="Par2">When studying for an exam or trying to learn a new word, repetition certainly helps. In certain cases, mere repetition without task relevance or explicit instructions can even be enough for plasticity and learning. This is the case in “statistical learning”, where regularities in the sensory input are stored implicitly without conscious awareness<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Both in the auditory<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> and in the visual<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> domains, human observers discriminating between looped (two auditory noise snippets or visual noise sequences presented back-to-back) and uncorrelated stimuli show preferential processing of looped stimuli that re-occur in several trials. This typically happens unknowingly to the participant, again pointing to some automatic or unconscious component of the underlying mechanisms. To further explore the efficacy of repetition processing on short time-scales, we recently developed a novel paradigm based on rapid serial visual presentation (RSVP)<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup> aimed at testing the capacity for detecting and remembering repeating natural images embedded in continuous streams of never-before-seen items. RSVP is ubiquitous in the literature on visual processing. Since its appearance almost 50 years ago, RSVP has been used to demonstrate a wide range of effects including the attentional blink<sup><xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR10">10</xref></sup>, repetition blindness<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup>, and more recently negative priming<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> (but see also for example<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>) and enhanced memory for images that deviate in terms of emotional content from the rest of the stream<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. Common to most of these paradigms is that participants are asked to report one or more pre-defined targets such as for example faces, or are required to remember all presented items. The streams typically comprise rather few items. In our new paradigm, the task is instead to spot repetitions in streams of hundreds or even thousands of images. The repeated images are chosen at random without any specific criteria and appear at random time points, meaning that it is not possible to foresee which image will become the next target. We previously showed that human observers can spot and remember repeating images under a wide range of conditions (Thunell, E. &amp; Thorpe, S. J. (2019). Memory for repeated images in rapid-serial-visual-presentation streams of thousands of images. Psychological Science. Advance online publication. 10.1177/0956797619842251). Both detection and memory for repeated items was above chance already with two or three presentations of the repeated image and increased with the number of presentations up to a ceiling level at around 7. The image presentation rate also strongly influenced performance, with the slowest streams being the easiest and the difficulty increasing with the presentation rate, at least for low rates. At around 15 Hz, performance stabilized and remained above chance up to 120 Hz. This remarkable capacity has implications for computational models of repetition learning. Importantly, in our previous experiments the repetitions were always regularly spaced; each repeated image (target; T) was interspaced by either one or two non-repeated distractors (D), i.e. the sequences were of the form T-D-T-D-T-D-T or T-D-D-T-D-D-T-D-D-T. Here, we ask whether this regularity is a key factor for repetition perception. Are regularly and irregularly spaced repetition sequences equally easy to detect and remember, as predicted by for example spike-timing dependent plasticity (STDP; see explanation below) based models of learning<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>? Or does irregular spacing perturb the processing? The latter seems intuitive considering our instinctive liking of rhythmic patterns and their importance in natural sensory input –arising from for example animal and human locomotion, and the added predictability of regular stimulation. A deleterious effect of irregularity was also suggested by Rajendran, Harper, Abdel-Latif, &amp; Schnupp<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> in the auditory domain, and taken as evidence against STDP-like mechanisms. STDP is a Hebbian learning rule where synaptic strength varies as a function of the relative timing of pre- and post-synaptic spikes, capable of causing neurons to become sensitive to repeated input spike patterns<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR18">18</xref>–<xref ref-type="bibr" rid="CR20">20</xref></sup>. It has recently been proposed as a possible mediator of statistical learning<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, and could potentially support also the repetition perception in our paradigm.</p>
              <p id="Par3">The role of regularity in repetition detection is not trivial to address: It is important to note that when manipulating the regularity of a repetition sequence, other crucial aspects of the stimulus necessarily also change. For example, if we choose to preserve the length of the repetition sequence and introduce irregularity by jittering the target presentations within the sequence, new inter-target intervals are introduced which do not exist in the corresponding regular sequence. As we showed in our previous study, the length of these intervals can greatly influence performance (Thunell, E. &amp; Thorpe, S. J. (2019). Memory for repeated images in rapid-serial-visual-presentation streams of thousands of images. Psychological Science. Advance online publication. 10.1177/0956797619842251). On the other hand, if we balance the total distribution of inter-target intervals, the sequence lengths will instead vary between regular and irregular sequences – a factor that might also in itself influence performance. Here, in an attempt to make the most complete comparison possible, we used both methods described above. Our results show that even with strongly irregular repetition sequences, the ability to detect and memorize repeated images is preserved.</p>
            </sec>
            <sec id="Sec2" sec-type="results">
              <title>Results</title>
              <p id="Par4">We recently developed a novel RSVP paradigm that we used to probe repetition perception as a function of the number of repetitions of an image, the number of distractors between repetitions, and the image presentation rate. Here, we adapted this paradigm to explore the effect of temporal regularity of the repetition sequences. We presented 15 Hz RSVP streams of never-before-seen natural images, each lasting about 1.5 minutes and containing 20 repetition sequences. A repetition sequence consisted of an image presented six times either in a regular fashion (with the same number of distractors between each presentation) or irregularly (with a varying number of distractors; Fig. <xref rid="Fig1" ref-type="fig">1</xref>). The task was to detect the repetitions (push a button every time a repetition sequence was spotted) and to afterward identify all 20 repetition targets in a 4-alternative forced choice (4AFC) task. The targets were chosen at random and independently of image content, meaning that there were no target-defining features. Further, the repetition sequences appeared at random time intervals (~3 s after the last one finished), making it impossible to foresee which image would become the next target.<fig id="Fig1"><label>Figure 1</label><caption><p>Stimuli. Each 15 Hz RSVP stream (left) contained over 1300 images. Here, a part of the stream is shown; each vertical line depicts an image. Each image was presented for 58.3 ms followed by an 8.3 ms blank. At random time points, an image was presented six times (longer red lines). In the Regular repetition sequence shown in detail, there are 3 distractors between each presentation of the target (the eggs). Note that there were also Regular repetition sequences with 1, 2, 4, and 5 distractors. In the Irregular repetition sequence, there is a varying number of distractors (1–5) between each presentation of the target (the pink magnolia flowers). In the Memory task that followed each RSVP stream (right), the repeated images had to be identified among sets of three non-repeated images from the same stream, along with a confidence rating. The red frames marking the targets were not present in the actual stimulation.</p></caption><graphic xlink:href="41598_2019_39697_Fig1_HTML" id="d29e298"/></fig></p>
              <sec id="Sec3">
                <title>Repetition detection task</title>
                <p id="Par5">First, we note that our participants were able to detect both Regular and Irregular repetition sequences: The detection rate was well above the “chance level” computed by assigning random time stamps to the button-presses in all conditions (Fig. <xref rid="Fig2" ref-type="fig">2</xref>; the 95% confidence intervals (CIs) for the difference between the real and the scrambled data do not span zero). Note that the Irregular “conditions” are here created somewhat arbitrarily based on the number of distractors in the <italic>first</italic> inter-target interval of each trial, in order to make a comparable plot as for the Regular conditions. The task seems to have been easiest with only 1 distractor and increasingly difficult with more distractors, as confirmed by a one-way ANOVA showing a significant effect of number of distractors (F(4, 90) = 10.5, p = 5.4 × 10<sup>−7</sup>; post-hoc Tukey-Kramer testing revealed significant differences between the 1 distractor condition and the 3, 4, and 5 distractor conditions (p values = 2.4 × 10<sup>−4</sup>, 2.0 × 10<sup>−5</sup>, and 2.5 × 10<sup>−6</sup>, respectively), and between the 2 and 5 distractor conditions (p = 0.024)). This effect is not due to a speed-accuracy trade-off since the corresponding RTs increased with the number of distractors (Fig. <xref rid="Fig2" ref-type="fig">2a</xref>, lower panel).<fig id="Fig2"><label>Figure 2</label><caption><p>Repetition detection compared to scrambled data (chance level) and RTs. In the upper panel, the repetition detection performance is shown together with the corresponding results for the scrambled data. In the Irregular conditions, the data are somewhat arbitrarily plotted as a function of the number of distractors before the first repetition. The percentage point (pp) difference between the real and scrambled data shown in the middle panel can be taken as a chance-level corrected measure of the detection performance. In the lower panel, RT = 0 is the onset of the second presentation (the first repetition) of the target. There were 13.4 ± 1.0 button-presses per block (mean ± SD across participants); 2.2 ± 2.1% of all trials contained more than one response in the response interval. Each data point depicts the mean per participant and condition. The lines indicate the mean across participants, and the shaded areas are 95% bootstrap confidence intervals.</p></caption><graphic xlink:href="41598_2019_39697_Fig2_HTML" id="d29e334"/></fig></p>
                <p id="Par6">As a first comparison between the performance for Regular and Irregular repetition sequences, we contrasted the Regular 3-distractor condition against a random sub-sample of equally many trials of the Irregular conditions, and found no apparent differences between the two groups: The CIs of the difference between the Regular and Irregular conditions span zero both for detection performance and RTs (Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). The 95% CIs provide a range of plausible values of the true difference, as we would expect the CI to include the true value 95% of the time when replicating the experiment. Additional Bayesian analyses revealed moderate evidence for the null hypothesis (detection performance BF10 = 0.279, RT BF10 = 0.248; see Supplementary material for details for all Bayesian tests). All repetition sequences included in this analysis have the same length, and the average number of distractors between repetitions is the same in the two groups. Any manipulation of the regularity of the repetition sequence will entail a possibly confounding modification of low-level properties, which is why we made also a second comparison, contrasting all Regular against all Irregular conditions. In this case, the <italic>average</italic> sequence length is the same in the two groups, and all inter-repetition intervals (1, 2, 3, 4, and 5 distractors) occur equally often in both. Again, the CIs of the difference span zero for both detection performance and RT (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>). The CIs are here much smaller than for the first comparison, as there are about five times as many trials in this analysis. Bayesian analyses again showed moderate evidence for the null hypothesis (detection performance BF10 = 0.238, RT BF10 = 0.243).<fig id="Fig3"><label>Figure 3</label><caption><p>Repetition detection for Regular vs. Irregular sequences. In (<bold>a</bold>), repetition detection (upper panel) and RTs (lower panel) are shown for the Regular 3-distractor condition and an equal-size random sub-sample of the Irregular trials, together with the difference Irregular-Regular. Each line depicts the mean per participant and condition, and the dots mark the overall means. In (<bold>b</bold>), all Regular are contrasted against all Irregular trials. The error bars are 95% confidence intervals.</p></caption><graphic xlink:href="41598_2019_39697_Fig3_HTML" id="d29e360"/></fig></p>
              </sec>
              <sec id="Sec4">
                <title>Memory task</title>
                <p id="Par7">Each RSVP stream was followed by a 4AFC Memory task where the participants were asked to identify all the repeated images from the stream (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, right). As in the repetition detection task, the participants performed well above chance in all conditions (Fig. <xref rid="Fig4" ref-type="fig">4</xref>, upper panel; the CIs do not span zero). The performance again dropped with number of distractors in the Regular sequences (one-way ANOVA: F(4, 90) = 6.2, p = 1.8 × 10<sup>−4</sup>; post-hoc Tukey-Kramer testing revealed significant differences between the 1 distractor condition and the 3, 4, and 5 distractor conditions (p values = 0.037, 5.7 × 10<sup>−3</sup>, and 2.2 × 10<sup>−4</sup>, respectively), and between the 2 and 5 distractor conditions (p = 0.027)). The RTs were rather stable, except perhaps for a slight increase with the number of distractors (Fig. <xref rid="Fig4" ref-type="fig">4a</xref>, middle panel), speaking against a speed-accuracy trade-off. The confidence ratings mimic the identification performance, indicating that the participants had an accurate perception of the relative difficulty levels in the different conditions (Fig. <xref rid="Fig4" ref-type="fig">4</xref>, lower panel). As with the Repetition detection data, the Irregular trials were arbitrarily grouped according to the number of distractors in the <italic>first</italic> inter-target interval.<fig id="Fig4"><label>Figure 4</label><caption><p>Memory task. In (<bold>a</bold>), identification performance (upper panel), RTs (middle panel), and confidence ratings (lower panel) are shown separately for each of the five regularly spaced repetition sequences. The graphs indicate the mean across participants. In the Irregular conditions (<bold>b</bold>), the data are plotted as a function of the number of distractors before the first repetition. The shaded areas are 95% confidence intervals.</p></caption><graphic xlink:href="41598_2019_39697_Fig4_HTML" id="d29e405"/></fig></p>
                <p id="Par8">The first comparisons between Regular and Irregular trials (Regular 3-distractor condition against a random sub-sample of equally many trials of the Irregular conditions; Fig. <xref rid="Fig5" ref-type="fig">5</xref>) revealed comparable identification performance in the two groups, but faster RTs for Regular repetition sequences than for Irregular ones. Bayesian analyses showed anecdotal evidence for the null hypothesis for the identification performance (BF10 = 0.355) and strong evidence for a RT difference (BF10 = 20.8). In the second comparison (all Regular against all Irregular conditions), identification performance, RTs, and confidence ratings are similar in the two groups. The Bayesian analyses revealed moderate evidence for the null hypothesis for identification performance (BF10 = 0.244) and RTs (BF = 0.238), and anecdotal evidence for the null for the confidence ratings (BF10 = 0.561). Thus, the results largely resemble those for the Repetition detection task.<fig id="Fig5"><label>Figure 5</label><caption><p>Memory for Regular vs. Irregular sequences. In (<bold>a</bold>), identification performance (upper panel), RTs (middle panel), and confidence ratings (lower panel) are shown for the Regular 3-distractor condition and an equal-size random sub-sample of the Irregular trials, together with the difference Irregular-Regular. Each line depicts the mean per participant and condition, and the dots mark the overall means. In (<bold>b</bold>), all Regular are contrasted against all Irregular trials. The error bars are 95% confidence intervals.</p></caption><graphic xlink:href="41598_2019_39697_Fig5_HTML" id="d29e425"/></fig></p>
                <p id="Par9">Last, we looked for signs of memory decay during the blocks. Since the order of target appearance was randomized in the Memory task, there are different delays between the appearances of the same target in the RSVP stream and the Memory task. Is there for example a recency effect (increased memory strength for the last targets in the stream) or a primacy effect (increased memory strength for the first targets in the stream)? We compared the identification performance for short versus long delays between the appearance of the same target in the RSVP stream and the Memory task (Supplementary Material, Fig. <xref rid="MOESM1" ref-type="media">S1</xref>). The CIs of the difference span zero for performance, RTs and confidence ratings. Bayesian analyses of these differences revealed moderate evidence for a null effect for the identification performance (BF10 = 0.245) and confidence (BF10 = 0.250), and anecdotal evidence in the case of the RTs (BF10 = 0.340).</p>
              </sec>
            </sec>
            <sec id="Sec5" sec-type="discussion">
              <title>Discussion</title>
              <p id="Par10">We recently revealed a remarkable capacity of the human visual system for detecting and remembering repeating images in RSVP streams of thousands of images (Thunell, E. &amp; Thorpe, S. J. (2019). Memory for repeated images in rapid-serial-visual-presentation streams of thousands of images. Psychological Science. Advance online publication. 10.1177/0956797619842251). We showed that at 15 Hz, one single repetition can be enough for detection and memory, and that observers can do the task in fast streams at up to 120 Hz when each target is presented five times. Here, we modified the paradigm to include both regularly and irregularly spaced repetition sequences, in order to investigate whether temporal regularity is a crucial factor for repetition detection and memory. It is important to note that it is not trivial to isolate the effect of regularity, since it typically co-varies with other aspects of the stimulation. We made two different comparisons between regularly and irregularly spaced repetition sequences (equalizing first the sequence length, then the distribution of number of distractors between repetitions, between the two groups) and observed only minor behavioural differences with either method. Importantly, the stimulation contained equally many regularly as irregularly spaced repetition sequences, meaning that there was no advantage of preferentially processing either type. Our participants performed well above chance on both the repetition detection and the memory task in all regular and irregular conditions, indicating that the mechanisms supporting repetition processing in this paradigm do not strongly depend on regularity. In fact, behaviour for regular and irregular repetitions was comparable in both tasks except for faster identification RTs for the former in the 3-distractor comparison. We might add that even the authors, who were highly trained on regular repetition sequences, performed equally well with irregular as regular sequences in pilot studies (none of the authors participated in the experiment reported here). It should be noted that our Repetition detection task is conceptually similar to an n-back task, and even though classic n-back paradigms typically involve longer presentation times, both necessitate some form of short-term memory function for keeping the items stored for comparison.</p>
              <p id="Par11">In the Regular conditions, the difficulty in both tasks increased with the number of distractors between repeats (Figs <xref rid="Fig2" ref-type="fig">2a</xref> and <xref rid="Fig4" ref-type="fig">4a</xref>), in agreement with our previous finding of better performance for 1-distractor than 2-distractor streams. (Thunell, E. &amp; Thorpe, S. J. (2019). Memory for repeated images in rapid-serial-visual-presentation streams of thousands of images. Psychological Science. Advance online publication. 10.1177/0956797619842251). We were surprised to find above-chance performance even when the repetitions were interspaced with as many as five distractors. Notably, in this condition the repetition detection involves a 6-back comparison and each target is masked by five other images (333 ms) before its next appearance.</p>
              <p id="Par12">In our previous study (Thunell, E. &amp; Thorpe, S. J. (2019). Memory for repeated images in rapid-serial-visual-presentation streams of thousands of images. Psychological Science. Advance online publication. 10.1177/0956797619842251), any forgetting or lack of retention of the repeated images seemed to have happened to a similar extent for all the targets regardless of the number of other items intervening between presentation and prompting of a target, on the time-scale of minutes. As in that study, short and long delays are not balanced with respect to conditions in the current design, and so the results should not be over interpreted. Still, it is interesting to again notice the lack of apparent dependencies on trial order, a potential effect that merits further investigation. Previous RSVP studies research have revealed decaying target retention with the number of intervening test items and elapsed time on the time scale of seconds (but not from presentation order except possibly for the first and last picture)<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>.</p>
              <p id="Par13">Rajendran and colleagues<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> asked a similar question about temporal regularity in the auditory domain. In their study, participants were presented with white noise sequences which either contained a repeated snippet (8 presentations), no repetition, or a transition to pink noise. The participants were instructed to push a button if they detected either a repetition or a transition. There were three irregular conditions (300 ms long repeated noise snippets separated by 200 ms ± 10, 50 or 100 ms; mean ± standard deviation), and four regular conditions (200, 300, 400 or 500 ms long repeated noise snippets appearing every 500 ms). The authors reported impaired performance with the most severe jitter (100 ms). The apparent discrepancy between our results and those of Rajendran and colleagues might be explained by several differences between the two studies. First, repetitions might be processed differently in the auditory as compared to the visual domain, or for noise stimuli as in the Rajendran study as compared to the meaningful stimuli used here. Further, in our study regular and irregular sequences were equally common, while in the study by Rajendran there were more regular than irregular trials. Although it is unlikely that this had any major influence, observers might implicitly learn the statistical properties of the stimulation and preferentially process regular sequences if they are more common. Rajendran and colleagues argue that STDP-like mechanisms alone cannot account for their results. The apparent lack of disruption of repetition processing by irregularity in our study, on the other hand, is in line with what would be predicted by STDP-based models of learning: With reasonably short repetition sequences, repetition processing with these algorithms does not depend on whether the items appear at regular or irregular intervals<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>.</p>
              <p id="Par14">Theories of periodic sampling of perception and attention propose rhythmicity at different frequencies between 4–10 Hz<sup><xref ref-type="bibr" rid="CR24">24</xref>–<xref ref-type="bibr" rid="CR26">26</xref></sup>. Based on these theories, we might expect certain particularly beneficial presentation rates of either the image stream or the targets in our paradigm. For example, visual attentional rhythms have been found at around 7 Hz<sup><xref ref-type="bibr" rid="CR27">27</xref>–<xref ref-type="bibr" rid="CR31">31</xref></sup>. Does this effect manifest as better performance around this frequency in our paradigm? In our previous study (Thunell, E. &amp; Thorpe, S. J. (2019). Memory for repeated images in rapid-serial-visual-presentation streams of thousands of images. Psychological Science. Advance online publication. 10.1177/0956797619842251), where the image presentation rate varied between 4 and 120 Hz, there was no clearly emerging such “sweet spot”. In the current study, the presentation rate was fixed at 15 Hz and the repeated images appeared at 7.5, 5, 3.75, 3, and 2 Hz in the Regular conditions (for 1, 2, 3, 4, and 5 distractors, respectively). One could imagine a dependence not on the image presentation rate but on the rate of target appearance. However, the behavioural results seem to depend monotonically on this parameter across the tested values. In addition, the similar behavioural results for the regular and irregular sequences argues against any strong facilitating entraining specifically to temporally regular target occurrences. In line with this, Quek &amp; Rossion<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> found similar EEG responses to faces that appeared regularly as compared to irregularly in 12 Hz RSVP streams, and in addition no specific EEG correlate of missing faces in otherwise regular sequences.</p>
              <p id="Par15">To conclude, we found that the remarkable ability to process repetitions that we previously reported (Thunell, E. &amp; Thorpe, S. J. (2019). Memory for repeated images in rapid-serial-visual-presentation streams of thousands of images. Psychological Science. Advance online publication. 10.1177/0956797619842251) is not restricted to temporally regular repetition sequences but generalizes also to irregular sequences. This finding adds to the constraints of plausible neural mechanisms supporting this capacity.</p>
            </sec>
            <sec id="Sec6">
              <title>Methods</title>
              <sec id="Sec7">
                <title>Participants</title>
                <p id="Par16">Twenty paid participants took part in the experiment, one of which was excluded due to overall poor performance. The remaining 19 (9 male) were aged 21–33 years (mean 27). All were naïve to the purpose to the study and had not previously seen any of the images or participated in any similar experiments. The procedures were in accordance with the Declaration of Helsinki and approved by the local ethics committee “Comité pour l’Evaluation de l’Ethique de l’INSERM” (CEEI protocol number 2015-004), and all participants gave written informed consent before starting the experiment.</p>
              </sec>
              <sec id="Sec8">
                <title>Stimuli and task</title>
                <p id="Par17">The paradigm used here is similar to that of (Thunell, E. &amp; Thorpe, S. J. (2019). Memory for repeated images in rapid-serial-visual-presentation streams of thousands of images. Psychological Science. Advance online publication. 10.1177/0956797619842251), where we presented repeated items embedded in long Rapid Serial Visual Presentation (RSVP) streams and varied the number of presentations of the repeated images and the image presentation rate. Here, both these parameters were fixed, and we instead manipulated the regularity of the repetition sequences. We presented RSVP streams of natural images at 15 Hz; 58.3 ms image duration followed by an 8.3 ms gap (one screen refresh of background grey). The streams lasted between 86.9 and 91.2 s (1303–1368 images) with an average duration of 88.8 s (1332 images). In each of these streams of never-before-seen images, we embedded 20 repetition sequences; an image appearing six times interspaced with between one and five distractor images that were not repeated (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). In half of the repetition sequences (Regular spacing conditions), the same number of distractors (1, 2, 3, 4 or 5) appeared between each of the six presentations of the repeated image. In the other half (Irregular spacing conditions), the number of distractors varied <italic>within</italic> each repetition sequence rather than across sequences. Here, the sequences each had 1, 2, 3, 4, <italic>and</italic> 5 distractors between the target presentations, and all 120 permutations of the order of these were used for each participant. It was not possible to anticipate when the next repetition sequence would occur, since there was an interval of variable length of between 2.5 and 3.5 s (37–52 images) between sequences and the images to be repeated were chosen at random. The participants were asked to push “s” on the keyboard every time they detected a repetition sequence, and to remember the repeated images for a subsequent Memory task. They were encouraged to push the button as soon as they noticed a repeating image, and they were reassured that this button-press could come after the repetition sequence, or even after the image stream had finished. We explicitly told the participants that all repetitions would appear in the form of relatively short repetition sequences, and that only one image at a time would be repeating. The Memory task that followed each RSVP stream was a four-alternative forced choice (4AFC) task where the repeated images had to be identified among sets of three other, non-repeated, images from the same stream. The participants were asked to rate their confidence for each response on a scale from 0 (guessing) to 4 (completely sure). They were informed that in each response frame, there was one image that had been repeated, and that the other three images had also appeared in the stream but only once. The order of the targets in the Memory task was randomized and did not match that of the RSVP stream. It was thus not directly useful for the Memory task to learn an ordered sequence of the targets in the RSVP stream. No feedback was given during the experiment.</p>
                <p id="Par18">To get an impression of the type of paradigm used here, the reader might try our freely available iPad game Brainspotting (<ext-link ext-link-type="uri" xlink:href="https://itunes.apple.com/app/id1246763569">https://itunes.apple.com/app/id1246763569</ext-link>), where the task is to identify regular repetitions in short RSVP sequences.</p>
                <p id="Par19">Each participant took part in one experimental session comprising 12 blocks (image stream with accompanying Memory task). The 120 permutations of the Irregular conditions and 24 instances of each of the five Regular conditions were presented in a random order and were thus not balanced across blocks. This design resulted in equally many Regular and Irregular spacing trials (120 of each). Before starting the experiment, the participants practiced on six blocks containing five or six repetitions sequences each at 2, 8 and finally 15 Hz. The experimenter was in the room during the practice and gave verbal feedback. The participants were informed that the image streams would last longer than during the practice. The whole session including practice lasted 1–1.5 h.</p>
              </sec>
              <sec id="Sec9">
                <title>Procedure and stimulus details</title>
                <p id="Par20">The images were a randomly chosen subset of the ImageNet database training set<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> (<ext-link ext-link-type="uri" xlink:href="http://image-net.org/">http://image-net.org/</ext-link>), cropped symmetrically from all sides to square shape and equalized in resolution to 150 * 150 pixels. Each participant viewed approximately 16.000 different images. No images other than the repeated images and the images in the response frames were presented more than once to any participant. All images were equally likely to be used as a target, and which images were targets varied between participants.</p>
                <p id="Par21">The participants were seated in a dimly lit room (~0.3 cd/m<sup>2</sup>) approximately 75 cm from the screen. The images were 8.3 cm wide (~6.3 degrees of visual angle). The RSVP streams were presented in the middle of the screen. The four images in the response frames were centred on the horizontal meridian of the screen, separated by approximately one-third of the image width. The background grey was chosen to match the average grey-level of a sample of 14.000 randomly chosen images from the ImageNet subset (grey-level 114 out of 255). Both the RSVP streams and the Memory tasks were preceded by a reminder of the task, which disappeared when the participants launched the next step by pushing the space bar. Before each stream, the block number was indicated. The participants were instructed to look at the image streams, but were free to fixate any part of the images. They were encouraged to take short breaks between the blocks, when needed. Eye movements were not recorded.</p>
              </sec>
              <sec id="Sec10">
                <title>Apparatus</title>
                <p id="Par22">The stimuli were presented on a BenQ XL2411Z monitor (120 Hz refresh rate, resolution 1920 × 1080 pixels, 24”), controlled by a PC. The reliability of the screen refresh rate and the stimulus generating script were verified using a photodiode connected to an oscilloscope.</p>
              </sec>
              <sec id="Sec11">
                <title>Analysis</title>
                <p id="Par23">Since the RSVP stimulation was continuous and there were no response intervals indicated to the participants, and in addition the conditions were mixed, signal detection theory based analyses of the button-presses would not have been trivial. (See for example<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> for a discussion of older methods and a new alternative for assessing detection sensitivity per block in fast or continuous stimulation). Instead, we considered a repetition sequence detected if there was one and only one button-press in an interval between 200 and 2500 ms after the first repetition (second presentation) of the target. The onset of the second presentation was defined as reaction time (RT) = 0. To get a meaningful assessment of the performance, the results were compared to a “chance-level” computed by performing the same analysis on scrambled data where, for each participant and block, the button-presses were given random time stamps.</p>
                <p id="Par24">We defined the delay between the appearances of a particular target in the RSVP stream and in the Memory task as the number of intervening targets in the stream and in the Memory task, plus one. The data were then split in half with respect to this delay and averaged within the two groups in order to contrast short and long delays. The very longest and shortest delays were not included since they were uncommon. Short delays were 3–20 and long were 21–37 trials. Note that the data is pooled over the conditions in a random, non-balanced, way in this analysis.</p>
                <p id="Par25">For analysis and stimulus generation we used Matlab and Psychtoolbox-3<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. The 95% confidence intervals (CIs) of the participant means were computed using the Matlab bootci function with 2000 iterations and the default bias corrected and accelerated percentile method. For the Bayesian t-tests, we used the freely available software JASP<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>.</p>
              </sec>
            </sec>
            <sec sec-type="supplementary-material">
              <title>Supplementary information</title>
              <sec id="Sec12">
                <p>
                  <supplementary-material content-type="local-data" id="MOESM1">
                    <media xlink:href="41598_2019_39697_MOESM1_ESM.docx">
                      <caption>
                        <p>Fig. S1</p>
                      </caption>
                    </media>
                  </supplementary-material>
                </p>
              </sec>
            </sec>
          </body>
          <back>
            <fn-group>
              <fn>
                <p><bold>Publisher’s note:</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
              </fn>
            </fn-group>
            <sec>
              <title>Electronic supplementary material</title>
              <p><bold>Supplementary information</bold> accompanies this paper at 10.1038/s41598-019-39697-y.</p>
            </sec>
            <ack>
              <title>Acknowledgements</title>
              <p>We would like to thank Tim Masquelier and Jean-Michel Hupé for useful discussions, and Céline Cappe for lending us her equipment. This work has received funding from the European Research Council under the European Union’s Seventh Framework Programme (FP/2007–2013)/ERC Grant Agreement n.323711 (M4 project).</p>
            </ack>
            <notes notes-type="author-contribution">
              <title>Author Contributions</title>
              <p>E.T. and S.J.T. developed the study concept and the study design and interpreted the results. Data collection and analysis was performed by E.T. The manuscript was drafted by E.T. and S.J.T. provided critical revisions. Both authors approved the final version of the manuscript for submission.</p>
            </notes>
            <notes notes-type="data-availability">
              <title>Data Availability</title>
              <p>Neither of the experiments reported in this article was formally preregistered. The modified images from the ImageNet database training set are available at the open science framework https://osf.io/t7ej9/?view_only=ead6d92f5c8b4e90891edfc0c136b21c. Raw data and the details of the Bayesian analysis are available at https://osf.io/m4fy6/?view_only=730253996a6e489f82d393a3751ca1be.</p>
            </notes>
            <notes notes-type="COI-statement">
              <sec id="FPar1">
                <title>Competing Interests</title>
                <p>The authors declare no competing interests.</p>
              </sec>
            </notes>
            <ref-list id="Bib1">
              <title>References</title>
              <ref id="CR1">
                <label>1.</label>
                <mixed-citation publication-type="other">Turk-Browne, N. B. Statistical learning and its consequences. In <italic>The influence of attention</italic>, <italic>learning</italic>, <italic>and motivation on visual search</italic> (eds Dodd, M. &amp; Flowers, J.) 117–146 (Springer, New York, NY, 2012).</mixed-citation>
              </ref>
              <ref id="CR2">
                <label>2.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Agus</surname>
                      <given-names>TR</given-names>
                    </name>
                    <name>
                      <surname>Thorpe</surname>
                      <given-names>SJ</given-names>
                    </name>
                    <name>
                      <surname>Pressnitzer</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <article-title>Rapid formation of robust auditory memories: Insights from noise</article-title>
                  <source>Neuron</source>
                  <year>2010</year>
                  <volume>66</volume>
                  <fpage>610</fpage>
                  <lpage>618</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.neuron.2010.04.014</pub-id>
                  <pub-id pub-id-type="pmid">20510864</pub-id>
                </element-citation>
              </ref>
              <ref id="CR3">
                <label>3.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Gold</surname>
                      <given-names>JM</given-names>
                    </name>
                    <name>
                      <surname>Aizenman</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Bond</surname>
                      <given-names>SM</given-names>
                    </name>
                    <name>
                      <surname>Sekuler</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <article-title>Memory and incidental learning for visual frozen noise sequences</article-title>
                  <source>Vision Res.</source>
                  <year>2014</year>
                  <volume>99</volume>
                  <fpage>19</fpage>
                  <lpage>36</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.visres.2013.09.005</pub-id>
                  <pub-id pub-id-type="pmid">24075900</pub-id>
                </element-citation>
              </ref>
              <ref id="CR4">
                <label>4.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Potter</surname>
                      <given-names>MC</given-names>
                    </name>
                    <name>
                      <surname>Levy</surname>
                      <given-names>EI</given-names>
                    </name>
                  </person-group>
                  <article-title>Recognition memory for a rapid sequence of pictures</article-title>
                  <source>J. Exp. Psychol.</source>
                  <year>1969</year>
                  <volume>81</volume>
                  <fpage>10</fpage>
                  <lpage>15</lpage>
                  <pub-id pub-id-type="doi">10.1037/h0027470</pub-id>
                  <pub-id pub-id-type="pmid">5812164</pub-id>
                </element-citation>
              </ref>
              <ref id="CR5">
                <label>5.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Potter</surname>
                      <given-names>MC</given-names>
                    </name>
                  </person-group>
                  <article-title>Short-term conceptual memory for pictures</article-title>
                  <source>J. Exp. Psychol. Hum. Learn.</source>
                  <year>1976</year>
                  <volume>2</volume>
                  <fpage>509</fpage>
                  <lpage>522</lpage>
                  <pub-id pub-id-type="doi">10.1037/0278-7393.2.5.509</pub-id>
                  <pub-id pub-id-type="pmid">1003124</pub-id>
                </element-citation>
              </ref>
              <ref id="CR6">
                <label>6.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Broadbent</surname>
                      <given-names>DE</given-names>
                    </name>
                    <name>
                      <surname>Broadbent</surname>
                      <given-names>MHP</given-names>
                    </name>
                  </person-group>
                  <article-title>From detection to identification: Response to multiple targets in rapid serial visual presentation</article-title>
                  <source>Percept. Psychophys.</source>
                  <year>1987</year>
                  <volume>42</volume>
                  <fpage>105</fpage>
                  <lpage>113</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03210498</pub-id>
                  <pub-id pub-id-type="pmid">3627930</pub-id>
                </element-citation>
              </ref>
              <ref id="CR7">
                <label>7.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Raymond</surname>
                      <given-names>JE</given-names>
                    </name>
                    <name>
                      <surname>Shapiro</surname>
                      <given-names>KL</given-names>
                    </name>
                    <name>
                      <surname>Arnell</surname>
                      <given-names>KM</given-names>
                    </name>
                  </person-group>
                  <article-title>Temporary supression of visual processing in an RSVP task: An attentional blink?</article-title>
                  <source>J. Exp. Psychol. Hum. Perform. Percept.</source>
                  <year>1992</year>
                  <volume>18</volume>
                  <fpage>849</fpage>
                  <lpage>860</lpage>
                  <pub-id pub-id-type="doi">10.1037/0096-1523.18.3.849</pub-id>
                </element-citation>
              </ref>
              <ref id="CR8">
                <label>8.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Einhäuser</surname>
                      <given-names>W</given-names>
                    </name>
                    <name>
                      <surname>Koch</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Makeig</surname>
                      <given-names>S</given-names>
                    </name>
                  </person-group>
                  <article-title>The duration of the attentional blink in natural scenes depends on stimulus category</article-title>
                  <source>Vision Res.</source>
                  <year>2007</year>
                  <volume>47</volume>
                  <fpage>597</fpage>
                  <lpage>607</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.visres.2006.12.007</pub-id>
                  <pub-id pub-id-type="pmid">17275058</pub-id>
                </element-citation>
              </ref>
              <ref id="CR9">
                <label>9.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Potter</surname>
                      <given-names>MC</given-names>
                    </name>
                    <name>
                      <surname>Chun</surname>
                      <given-names>MM</given-names>
                    </name>
                    <name>
                      <surname>Banks</surname>
                      <given-names>BS</given-names>
                    </name>
                    <name>
                      <surname>Muckenhoupt</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <article-title>Two attentional deficits in serial target search: The visual attentional blink and an amodal task-switch deficit</article-title>
                  <source>J. Exp. Psychol. Learn. Mem. Cogn.</source>
                  <year>1998</year>
                  <volume>24</volume>
                  <fpage>979</fpage>
                  <lpage>992</lpage>
                  <pub-id pub-id-type="doi">10.1037/0278-7393.24.4.979</pub-id>
                  <pub-id pub-id-type="pmid">9699304</pub-id>
                </element-citation>
              </ref>
              <ref id="CR10">
                <label>10.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Di Lollo</surname>
                      <given-names>V</given-names>
                    </name>
                    <name>
                      <surname>Kawahara</surname>
                      <given-names>JI</given-names>
                    </name>
                    <name>
                      <surname>Shahab Ghorashi</surname>
                      <given-names>SM</given-names>
                    </name>
                    <name>
                      <surname>Enns</surname>
                      <given-names>JT</given-names>
                    </name>
                  </person-group>
                  <article-title>The attentional blink: Resource depletion or temporary loss of control?</article-title>
                  <source>Psychol. Res.</source>
                  <year>2005</year>
                  <volume>69</volume>
                  <fpage>191</fpage>
                  <lpage>200</lpage>
                  <pub-id pub-id-type="doi">10.1007/s00426-004-0173-x</pub-id>
                  <pub-id pub-id-type="pmid">15597184</pub-id>
                </element-citation>
              </ref>
              <ref id="CR11">
                <label>11.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Kanwisher</surname>
                      <given-names>NG</given-names>
                    </name>
                  </person-group>
                  <article-title>Repetition blindness: Type recognition without token individuation</article-title>
                  <source>Cognition</source>
                  <year>1987</year>
                  <volume>27</volume>
                  <fpage>117</fpage>
                  <lpage>143</lpage>
                  <pub-id pub-id-type="doi">10.1016/0010-0277(87)90016-3</pub-id>
                  <pub-id pub-id-type="pmid">3691023</pub-id>
                </element-citation>
              </ref>
              <ref id="CR12">
                <label>12.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bavelier</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <article-title>Repetition blindness between visually different items: the case of pictures and words</article-title>
                  <source>Cognition</source>
                  <year>1994</year>
                  <volume>51</volume>
                  <fpage>199</fpage>
                  <lpage>236</lpage>
                  <pub-id pub-id-type="doi">10.1016/0010-0277(94)90054-X</pub-id>
                  <pub-id pub-id-type="pmid">8194301</pub-id>
                </element-citation>
              </ref>
              <ref id="CR13">
                <label>13.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Li</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>Neumann</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Chen</surname>
                      <given-names>Z</given-names>
                    </name>
                  </person-group>
                  <article-title>Identity and semantic negative priming in rapid serial visual presentation streams</article-title>
                  <source>Attention, Perception, Psychophys.</source>
                  <year>2017</year>
                  <volume>79</volume>
                  <fpage>1755</fpage>
                  <lpage>1776</lpage>
                  <pub-id pub-id-type="doi">10.3758/s13414-017-1327-4</pub-id>
                </element-citation>
              </ref>
              <ref id="CR14">
                <label>14.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Subramaniam</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Biederman</surname>
                      <given-names>I</given-names>
                    </name>
                    <name>
                      <surname>Madigan</surname>
                      <given-names>S</given-names>
                    </name>
                  </person-group>
                  <article-title>Accurate identification but no priming and chance recognition memory for pictures in RSVP sequences</article-title>
                  <source>Vis. cogn.</source>
                  <year>2000</year>
                  <volume>7</volume>
                  <fpage>511</fpage>
                  <lpage>535</lpage>
                  <pub-id pub-id-type="doi">10.1080/135062800394630</pub-id>
                </element-citation>
              </ref>
              <ref id="CR15">
                <label>15.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Quinlan</surname>
                      <given-names>PT</given-names>
                    </name>
                    <name>
                      <surname>Yue</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name>
                      <surname>Cohen</surname>
                      <given-names>DJ</given-names>
                    </name>
                  </person-group>
                  <article-title>The processing of images of biological threats in visual short-term memory</article-title>
                  <source>Proceedings. Biol. Sci.</source>
                  <year>2017</year>
                  <volume>284</volume>
                  <fpage>20171283</fpage>
                  <pub-id pub-id-type="doi">10.1098/rspb.2017.1283</pub-id>
                </element-citation>
              </ref>
              <ref id="CR16">
                <label>16.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Masquelier</surname>
                      <given-names>Timothée</given-names>
                    </name>
                  </person-group>
                  <article-title>STDP Allows Close-to-Optimal Spatiotemporal Spike Pattern Detection by Single Coincidence Detector Neurons</article-title>
                  <source>Neuroscience</source>
                  <year>2018</year>
                  <volume>389</volume>
                  <fpage>133</fpage>
                  <lpage>140</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.neuroscience.2017.06.032</pub-id>
                  <pub-id pub-id-type="pmid">28668487</pub-id>
                </element-citation>
              </ref>
              <ref id="CR17">
                <label>17.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Rajendran</surname>
                      <given-names>VG</given-names>
                    </name>
                    <name>
                      <surname>Harper</surname>
                      <given-names>NS</given-names>
                    </name>
                    <name>
                      <surname>Abdel-Latif</surname>
                      <given-names>KHA</given-names>
                    </name>
                    <name>
                      <surname>Schnupp</surname>
                      <given-names>JWH</given-names>
                    </name>
                  </person-group>
                  <article-title>Rhythm facilitates the detection of repeating sound patterns</article-title>
                  <source>Front. Neurosci.</source>
                  <year>2016</year>
                  <volume>10</volume>
                  <fpage>1</fpage>
                  <lpage>7</lpage>
                  <pub-id pub-id-type="doi">10.3389/fnins.2016.00009</pub-id>
                  <pub-id pub-id-type="pmid">26858586</pub-id>
                </element-citation>
              </ref>
              <ref id="CR18">
                <label>18.</label>
                <mixed-citation publication-type="other">Masquelier, T., Guyonneau, R. &amp; Thorpe, S. J. Spike timing dependent plasticity finds the start of repeating patterns in continuous spike trains. <italic>PLoS One</italic><bold>3</bold> (2008).</mixed-citation>
              </ref>
              <ref id="CR19">
                <label>19.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bichler</surname>
                      <given-names>O</given-names>
                    </name>
                    <name>
                      <surname>Querlioz</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Thorpe</surname>
                      <given-names>SJ</given-names>
                    </name>
                    <name>
                      <surname>Bourgoin</surname>
                      <given-names>JP</given-names>
                    </name>
                    <name>
                      <surname>Gamrat</surname>
                      <given-names>C</given-names>
                    </name>
                  </person-group>
                  <article-title>Extraction of temporally correlated features from dynamic vision sensors with spike-timing-dependent plasticity</article-title>
                  <source>Neural Networks</source>
                  <year>2012</year>
                  <volume>32</volume>
                  <fpage>339</fpage>
                  <lpage>348</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.neunet.2012.02.022</pub-id>
                  <pub-id pub-id-type="pmid">22386501</pub-id>
                </element-citation>
              </ref>
              <ref id="CR20">
                <label>20.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Song</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Miller</surname>
                      <given-names>KD</given-names>
                    </name>
                    <name>
                      <surname>Abbott</surname>
                      <given-names>LF</given-names>
                    </name>
                  </person-group>
                  <article-title>Competitive Hebbian learning through spike-time dependent synaptic plasticity</article-title>
                  <source>Nat. Neurosci.</source>
                  <year>2000</year>
                  <volume>3</volume>
                  <fpage>919</fpage>
                  <lpage>926</lpage>
                  <pub-id pub-id-type="doi">10.1038/78829</pub-id>
                  <pub-id pub-id-type="pmid">10966623</pub-id>
                </element-citation>
              </ref>
              <ref id="CR21">
                <label>21.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Goujon</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Didierjean</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Thorpe</surname>
                      <given-names>S</given-names>
                    </name>
                  </person-group>
                  <article-title>Investigating implicit statistical learning mechanisms through contextual cueing</article-title>
                  <source>Trends Cogn. Sci.</source>
                  <year>2015</year>
                  <volume>19</volume>
                  <fpage>524</fpage>
                  <lpage>533</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.tics.2015.07.009</pub-id>
                  <pub-id pub-id-type="pmid">26255970</pub-id>
                </element-citation>
              </ref>
              <ref id="CR22">
                <label>22.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Potter</surname>
                      <given-names>MC</given-names>
                    </name>
                    <name>
                      <surname>Staub</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>O’Connor</surname>
                      <given-names>DH</given-names>
                    </name>
                  </person-group>
                  <article-title>Pictorial and conceptual representation of glimpsed pictures</article-title>
                  <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
                  <year>2004</year>
                  <volume>30</volume>
                  <fpage>478</fpage>
                  <lpage>489</lpage>
                  <pub-id pub-id-type="doi">10.1037/0096-1523.30.3.478</pub-id>
                  <pub-id pub-id-type="pmid">15161380</pub-id>
                </element-citation>
              </ref>
              <ref id="CR23">
                <label>23.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Potter</surname>
                      <given-names>MC</given-names>
                    </name>
                    <name>
                      <surname>Staub</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Rado</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>O’Connor</surname>
                      <given-names>DH</given-names>
                    </name>
                  </person-group>
                  <article-title>Recognition memory for briefly presented pictures: The time course of rapid forgetting</article-title>
                  <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
                  <year>2002</year>
                  <volume>28</volume>
                  <fpage>1163</fpage>
                  <lpage>1175</lpage>
                  <pub-id pub-id-type="doi">10.1037/0096-1523.28.5.1163</pub-id>
                  <pub-id pub-id-type="pmid">12421062</pub-id>
                </element-citation>
              </ref>
              <ref id="CR24">
                <label>24.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Fiebelkorn</surname>
                      <given-names>IC</given-names>
                    </name>
                    <name>
                      <surname>Saalmann</surname>
                      <given-names>YB</given-names>
                    </name>
                    <name>
                      <surname>Kastner</surname>
                      <given-names>S</given-names>
                    </name>
                  </person-group>
                  <article-title>Rhythmic sampling within and between objects despite sustained attention at a cued location</article-title>
                  <source>Curr. Biol.</source>
                  <year>2013</year>
                  <volume>23</volume>
                  <fpage>2553</fpage>
                  <lpage>2558</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.cub.2013.10.063</pub-id>
                  <pub-id pub-id-type="pmid">24316204</pub-id>
                </element-citation>
              </ref>
              <ref id="CR25">
                <label>25.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Dugué</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>McLelland</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Lajous</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>VanRullen</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <article-title>Attention searches nonuniformly in space and in time</article-title>
                  <source>Proc. Natl. Acad. Sci.</source>
                  <year>2015</year>
                  <volume>112</volume>
                  <fpage>15214</fpage>
                  <lpage>15219</lpage>
                  <pub-id pub-id-type="doi">10.1073/pnas.1511331112</pub-id>
                  <pub-id pub-id-type="pmid">26598671</pub-id>
                </element-citation>
              </ref>
              <ref id="CR26">
                <label>26.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>VanRullen</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <article-title>Perceptual cycles</article-title>
                  <source>Trends Cogn. Sci.</source>
                  <year>2016</year>
                  <volume>20</volume>
                  <fpage>723</fpage>
                  <lpage>735</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.tics.2016.07.006</pub-id>
                  <pub-id pub-id-type="pmid">27567317</pub-id>
                </element-citation>
              </ref>
              <ref id="CR27">
                <label>27.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Busch</surname>
                      <given-names>NA</given-names>
                    </name>
                    <name>
                      <surname>VanRullen</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <article-title>Spontaneous EEG oscillations reveal periodic sampling of visual attention</article-title>
                  <source>Proc. Natl. Acad. Sci.</source>
                  <year>2010</year>
                  <volume>107</volume>
                  <fpage>16048</fpage>
                  <lpage>16053</lpage>
                  <pub-id pub-id-type="doi">10.1073/pnas.1004801107</pub-id>
                  <pub-id pub-id-type="pmid">20805482</pub-id>
                </element-citation>
              </ref>
              <ref id="CR28">
                <label>28.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>McLelland</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Lavergne</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>VanRullen</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <article-title>The phase of ongoing EEG oscillations predicts the amplitude of peri-saccadic mislocalization</article-title>
                  <source>Sci. Rep.</source>
                  <year>2016</year>
                  <volume>6</volume>
                  <fpage>1</fpage>
                  <lpage>8</lpage>
                  <pub-id pub-id-type="doi">10.1038/s41598-016-0001-8</pub-id>
                  <pub-id pub-id-type="pmid">28442746</pub-id>
                </element-citation>
              </ref>
              <ref id="CR29">
                <label>29.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Chakravarthi</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>VanRullen</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <article-title>Conscious updating is a rhythmic process</article-title>
                  <source>Proc. Natl. Acad. Sci.</source>
                  <year>2012</year>
                  <volume>109</volume>
                  <fpage>10599</fpage>
                  <lpage>10604</lpage>
                  <pub-id pub-id-type="doi">10.1073/pnas.1121622109</pub-id>
                  <pub-id pub-id-type="pmid">22689974</pub-id>
                </element-citation>
              </ref>
              <ref id="CR30">
                <label>30.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Busch</surname>
                      <given-names>NA</given-names>
                    </name>
                    <name>
                      <surname>Dubois</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>VanRullen</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <article-title>The Phase of Ongoing EEG Oscillations Predicts Visual Perception</article-title>
                  <source>J. Neurosci.</source>
                  <year>2009</year>
                  <volume>29</volume>
                  <fpage>7869</fpage>
                  <lpage>7876</lpage>
                  <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0113-09.2009</pub-id>
                  <pub-id pub-id-type="pmid">19535598</pub-id>
                </element-citation>
              </ref>
              <ref id="CR31">
                <label>31.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Dugué</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>Marque</surname>
                      <given-names>P</given-names>
                    </name>
                    <name>
                      <surname>VanRullen</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <article-title>Theta Oscillations Modulate Attentional Search Performance Periodically</article-title>
                  <source>J. Cogn. Neurosci.</source>
                  <year>2015</year>
                  <volume>27</volume>
                  <fpage>945</fpage>
                  <lpage>958</lpage>
                  <pub-id pub-id-type="doi">10.1162/jocn_a_00755</pub-id>
                  <pub-id pub-id-type="pmid">25390199</pub-id>
                </element-citation>
              </ref>
              <ref id="CR32">
                <label>32.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Quek</surname>
                      <given-names>GL</given-names>
                    </name>
                    <name>
                      <surname>Rossion</surname>
                      <given-names>B</given-names>
                    </name>
                  </person-group>
                  <article-title>Neuropsychologia Category-selective human brain processes elicited in fast periodic visual stimulation streams are immune to temporal predictability</article-title>
                  <source>Neuropsychologia</source>
                  <year>2017</year>
                  <volume>104</volume>
                  <fpage>182</fpage>
                  <lpage>200</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.08.010</pub-id>
                  <pub-id pub-id-type="pmid">28811258</pub-id>
                </element-citation>
              </ref>
              <ref id="CR33">
                <label>33.</label>
                <mixed-citation publication-type="other">Deng, Jia <italic>et al</italic>. ImageNet: A large-scale hierarchical image database. <italic>2009 IEEE Conf</italic>. <italic>Comput</italic>. <italic>Vis</italic>. <italic>Pattern Recognit</italic>. 248–255, 10.1109/CVPRW.2009.5206848 (2009).</mixed-citation>
              </ref>
              <ref id="CR34">
                <label>34.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bendixen</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Andersen</surname>
                      <given-names>SK</given-names>
                    </name>
                  </person-group>
                  <article-title>Measuring target detection performance in paradigms with high event rates</article-title>
                  <source>Clin. Neurophysiol.</source>
                  <year>2013</year>
                  <volume>124</volume>
                  <fpage>928</fpage>
                  <lpage>940</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.clinph.2012.11.012</pub-id>
                  <pub-id pub-id-type="pmid">23266090</pub-id>
                </element-citation>
              </ref>
              <ref id="CR35">
                <label>35.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Brainard</surname>
                      <given-names>David H.</given-names>
                    </name>
                  </person-group>
                  <article-title>The Psychophysics Toolbox</article-title>
                  <source>Spatial Vision</source>
                  <year>1997</year>
                  <volume>10</volume>
                  <issue>4</issue>
                  <fpage>433</fpage>
                  <lpage>436</lpage>
                  <pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id>
                  <pub-id pub-id-type="pmid">9176952</pub-id>
                </element-citation>
              </ref>
              <ref id="CR36">
                <label>36.</label>
                <mixed-citation publication-type="other">JASP team. JASP (Version 0.9) (2018).</mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
