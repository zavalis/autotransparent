<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2023-02-07T13:04:08Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:6764502" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:6764502</identifier>
        <datestamp>2019-10-08</datestamp>
        <setSpec>apasd</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Emotion</journal-id>
              <journal-id journal-id-type="iso-abbrev">Emotion</journal-id>
              <journal-title-group>
                <journal-title>Emotion (Washington, D.C.)</journal-title>
              </journal-title-group>
              <issn pub-type="ppub">1528-3542</issn>
              <issn pub-type="epub">1931-1516</issn>
              <publisher>
                <publisher-name>American Psychological Association</publisher-name>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC6764502</article-id>
              <article-id pub-id-type="pmcid">PMC6764502</article-id>
              <article-id pub-id-type="pmc-uid">6764502</article-id>
              <article-id pub-id-type="pmid">30321038</article-id>
              <article-id pub-id-type="publisher-id">emo_19_6_1060</article-id>
              <article-id pub-id-type="doi">10.1037/emo0000481</article-id>
              <article-id pub-id-type="publisher-id">2018-51385-001</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Articles</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Changing Interpretations of Emotional Expressions in Working Memory With Aging</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="editor" corresp="no">
                  <name>
                    <surname>Pietromonaco</surname>
                    <given-names>Paula R.</given-names>
                  </name>
                  <role>Editor</role>
                </contrib>
              </contrib-group>
              <contrib-group>
                <contrib contrib-type="author" corresp="no">
                  <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7261-9257</contrib-id>
                  <name>
                    <surname>Mok</surname>
                    <given-names>Robert M.</given-names>
                  </name>
                  <xref rid="aff1" ref-type="aff">1</xref>
                </contrib>
                <contrib contrib-type="author" corresp="no">
                  <name>
                    <surname>Hajonides van der Meulen</surname>
                    <given-names>Jasper E.</given-names>
                  </name>
                  <xref rid="aff1" ref-type="aff">1</xref>
                </contrib>
                <contrib contrib-type="author" corresp="no">
                  <name>
                    <surname>Holmes</surname>
                    <given-names>Emily A.</given-names>
                  </name>
                  <xref rid="aff2" ref-type="aff">2</xref>
                </contrib>
                <contrib contrib-type="author" corresp="yes">
                  <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5762-2802</contrib-id>
                  <name>
                    <surname>Nobre</surname>
                    <given-names>Anna Christina</given-names>
                  </name>
                  <xref rid="aff3" ref-type="aff">3</xref>
                  <xref rid="corr1" ref-type="corresp">*</xref>
                </contrib>
                <aff id="aff1"><label>1</label>Department of Experimental Psychology, and Oxford Centre for Human Brain Activity, Department of Psychiatry, Wellcome Centre for Integrative Neuroimaging, University of Oxford</aff>
                <aff id="aff2"><label>2</label>Department for Clinical Neuroscience, Karolinska Institutet</aff>
                <aff id="aff3"><label>3</label>Department of Experimental Psychology, and Oxford Centre for Human Brain Activity, Department of Psychiatry, Wellcome Centre for Integrative Neuroimaging, University of Oxford</aff>
              </contrib-group>
              <author-notes>
                <p>This study was supported by a Wellcome Trust senior investigator award (Anna Christina Nobre) 104571/Z/14/Z, European Union FP7 Marie Curie Initial Training Network Grant 606901 (Individualized Diagnostics &amp; Rehabilitation of Attention Disorders), the National Institute of Health Research Oxford Health Biomedical Research Centre, and the National Institute of Health Research Oxford Cognitive Health Clinical Research Facility. The Wellcome Centre for Integrative Neuroimaging is supported by core funding from the Wellcome Trust (203139/Z/16/Z). Robert M. Mok and Anna Christina Nobre conceived and designed the study. Robert M. Mok and Jasper E. Hajonides van der Meulen performed testing and data collection. Robert M. Mok and Jasper E. Hajonides van der Meulen performed data analysis under the supervision of Anna Christina Nobre. Robert M. Mok and Anna Christina Nobre interpreted the findings and wrote the manuscript with input from Emily A. Holmes. We thank Nick Myers for discussions on data analysis and Shen Ning for her assistance on pilot studies. All authors approved the final version of the manuscript for submission. For access to the stimulus materials, please contact Robert M. Mok (<email>robmmok@gmail.com</email>).</p>
                <corresp id="corr1"><label>*</label>Correspondence concerning this article should be addressed to Anna Christina Nobre, Department of Experimental Psychology, and Oxford Centre for Human Brain Activity, Department of Psychiatry, Wellcome Centre for Integrative Neuroimaging, University of Oxford, Oxford, UK, OX3 7JX <email>kia.nobre@ohba.ox.ac.uk</email></corresp>
              </author-notes>
              <pub-date pub-type="epub">
                <day>15</day>
                <month>10</month>
                <year>2018</year>
              </pub-date>
              <pub-date pub-type="ppub">
                <month>9</month>
                <year>2019</year>
              </pub-date>
              <volume>19</volume>
              <issue>6</issue>
              <fpage>1060</fpage>
              <lpage>1069</lpage>
              <history>
                <date date-type="received">
                  <day>6</day>
                  <month>2</month>
                  <year>2018</year>
                </date>
                <date date-type="rev-recd">
                  <day>30</day>
                  <month>5</month>
                  <year>2018</year>
                </date>
                <date date-type="accepted">
                  <day>1</day>
                  <month>6</month>
                  <year>2018</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2018 The Author(s)</copyright-statement>
                <copyright-year>2018</copyright-year>
                <copyright-holder>The Author(s)</copyright-holder>
                <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/3.0/">
                  <license-p>This article has been published under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/" specific-use="live">http://creativecommons.org/licenses/by/3.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Copyright for this article is retained by the author(s). Author(s) grant(s) the American Psychological Association the exclusive right to publish the article and identify itself as the original publisher.</license-p>
                </license>
              </permissions>
              <related-article related-article-type="correction-forward" id="d37e170" xlink:href="10.1037/emo0000604" ext-link-type="doi">
                <article-title>Correction to Mok et al. (2018)</article-title>
              </related-article>
              <abstract>
                <p>Working memory (WM) shows significant decline with age. It is interesting to note that some research has suggested age-related impairments can be reduced in tasks that involve emotion-laden stimuli. However, only a few studies have explored how WM for emotional material changes in aging. Here we developed a novel experimental task to compare and contrast how emotional material is represented in older versus younger adults. The task enabled us to separate overall WM accuracy from emotional biases in the content of affective representations in WM. We found that, in addition to overall decline in WM performance, older adults showed a systematic positivity bias in representing information in WM relative to younger adults (positivity effect). They remembered fearful faces as being less fearful than younger adults and interpreted ambiguous facial expressions more positively. The findings show that aging brings a type of positivity bias when picking up affective information for guiding future behavior.</p>
              </abstract>
              <kwd-group>
                <kwd>emotion</kwd>
                <kwd>working memory</kwd>
                <kwd>aging</kwd>
                <kwd>positivity bias</kwd>
                <kwd>facial expressions</kwd>
              </kwd-group>
            </article-meta>
          </front>
          <body>
            <p>Working memory (WM) is an essential cognitive function, enabling us to hold information in mind for goal-oriented behavior (<xref rid="c1" ref-type="bibr" id="cr1-1">Baddeley &amp; Hitch, 1974</xref>). It plays a role in a wide range of cognitive processes, including attention (<xref rid="c12" ref-type="bibr" id="cr12-1">Desimone &amp; Duncan, 1995</xref>) and planning (<xref rid="c1" ref-type="bibr" id="cr1-2">Baddeley &amp; Hitch, 1974</xref>). WM has highly limited capacity, which shows significant declines with age (<xref rid="c10" ref-type="bibr" id="cr10-1">Cowan, Naveh-Benjamin, Kilb, &amp; Saults, 2006</xref>), which, given its fundamental role across cognitive domains, can have deleterious effects on everyday life (<xref rid="c11" ref-type="bibr" id="cr11-1">Davis, Marra, Najafzadeh, &amp; Liu-Ambrose, 2010</xref>). However, a growing body of research suggests that WM capacity is not fixed, and can be modulated by factors such as attention (<xref rid="c19" ref-type="bibr" id="cr19-1">Griffin &amp; Nobre, 2003</xref>; <xref rid="c25" ref-type="bibr" id="cr25-1">Landman, Spekreijse, &amp; Lamme, 2003</xref>). Notably, recent studies have demonstrated that older adults retain flexibility over WM, such that the ability to use attention to improve WM performance shows little age-related impairment (<xref rid="c18" ref-type="bibr" id="cr18-1">Gilchrist, Duarte, &amp; Verhaeghen, 2016</xref>; <xref rid="c30" ref-type="bibr" id="cr30-1">Mok, Myers, Wallis, &amp; Nobre, 2016</xref>; <xref rid="c33" ref-type="bibr" id="cr33-1">Newsome et al., 2015</xref>; <xref rid="c42" ref-type="bibr" id="cr42-1">Souza, 2016</xref>). However, these studies have used affectively neutral stimuli, which leaves open the possibility that stimulus content might also influence WM performance in older adults.</p>
            <p>WM may also be modulated by affective content, but only a few studies have explored this in the context of aging (<xref rid="c4" ref-type="bibr" id="cr4-1">Bermudez &amp; Souza, 2017</xref>; <xref rid="c20" ref-type="bibr" id="cr20-1">Hartley, Ravich, Stringer, &amp; Wiley, 2015</xref>; <xref rid="c26" ref-type="bibr" id="cr26-1">Mammarella, Borella, Carretti, Leonardi, &amp; Fairfield, 2013</xref>; <xref rid="c29" ref-type="bibr" id="cr29-1">Mikels, Larkin, Reuter-Lorenz, &amp; Carstensen, 2005</xref>; <xref rid="c44" ref-type="bibr" id="cr44-1">Truong &amp; Yang, 2014</xref>). Findings from perceptual and long-term memory tasks suggest that older adults retain sensitivity to the emotional valence of stimuli. Considerable research has shown that negatively valenced stimuli can capture attention and boost perceptual performance in younger adults (emotional salience effect; e.g., <xref rid="c34" ref-type="bibr" id="cr34-1">Öhman, Flykt, &amp; Esteves, 2001</xref>; <xref rid="c35" ref-type="bibr" id="cr35-1">Phelps, Ling, &amp; Carrasco, 2006</xref>), and this boost seems to be retained in older adults (<xref rid="c17" ref-type="bibr" id="cr17-1">Fung &amp; Carstensen, 2003</xref>; <xref rid="c28" ref-type="bibr" id="cr28-1">Mather &amp; Knight, 2006</xref>; <xref rid="c31" ref-type="bibr" id="cr31-1">Murphy &amp; Isaacowitz, 2008</xref>; <xref rid="c39" ref-type="bibr" id="cr39-1">Rösler et al., 2005</xref>). Both younger and older adults show better long-term memory for emotional than for neutral stimuli (e.g., <xref rid="c24" ref-type="bibr" id="cr24-1">Kensinger, Brierley, Medford, Growdon, &amp; Corkin, 2002</xref>). Whereas younger adults put more weight on negative aspects of the environment, older adults have a tendency to attend to positive information (positivity effect; <xref rid="c27" ref-type="bibr" id="cr27-1">Mather &amp; Carstensen, 2005</xref>; <xref rid="c38" ref-type="bibr" id="cr38-1">Reed, Chan, &amp; Mikels, 2014</xref>). Older adults show superior performance on perceptual and memory tasks that use positive compared with neutral or negative stimuli (e.g., <xref rid="c9" ref-type="bibr" id="cr9-1">Charles, Mather, &amp; Carstensen, 2003</xref>; <xref rid="c13" ref-type="bibr" id="cr13-1">Ebner &amp; Johnson, 2009</xref>; <xref rid="c23" ref-type="bibr" id="cr23-1">Kellough &amp; Knight, 2012</xref>; <xref rid="c38" ref-type="bibr" id="cr38-2">Reed et al., 2014</xref>).</p>
            <p>Less is understood about the influence of stimulus valence on WM in older adults. Several studies found a benefit in WM performance for negative compared with neutral stimuli in younger adults (e.g., <xref rid="c22" ref-type="bibr" id="cr22-1">Jackson, Wu, Linden, &amp; Raymond, 2009</xref>). A few studies have tested the interaction between affective content in WM and aging (<xref rid="c4" ref-type="bibr" id="cr4-2">Bermudez &amp; Souza, 2017</xref>; <xref rid="c20" ref-type="bibr" id="cr20-2">Hartley et al., 2015</xref>; <xref rid="c26" ref-type="bibr" id="cr26-2">Mammarella et al., 2013</xref>; <xref rid="c29" ref-type="bibr" id="cr29-2">Mikels et al., 2005</xref>; <xref rid="c44" ref-type="bibr" id="cr44-2">Truong &amp; Yang, 2014</xref>). These studies are limited to paradigms that use reaction-time (RT) measures, which are not ideal for testing older populations, who may have motor problems, or accuracy measures, which cannot tease apart critical questions: namely, whether age-related changes are a result of reductions in WM capacity (independent of emotional content) or are a result of how emotional information-representation changes in WM (as more or less positive or negative). For example, higher accuracy for positive versus negative stimuli might reflect better memory for positive stimuli, a tendency to see positive things as more positive, or a tendency to see negative stimuli as less negative.</p>
            <p>In this study, we developed a new way to measure the quality of WM representations for emotional material and to assess systematic affective biases in perceiving and interpreting emotional material, for a more sensitive test of the positivity effect in aging. The task borrows from WM precision tasks, which test WM for items with features that vary continuously along a given dimension (e.g., bars with orientation of 1–180°), such that participants recall the feature (orientation) stored in memory (<xref rid="c2" ref-type="bibr" id="cr2-1">Bays &amp; Husain, 2008</xref>; <xref rid="c47" ref-type="bibr" id="cr47-1">Zhang &amp; Luck, 2008</xref>). The task produces sensitive estimates of the quantity and quality of items in WM (<xref rid="c48" ref-type="bibr" id="cr48-1">Zokaei, Burnett Heyes, Gorgoraptis, Budhdeo, &amp; Husain, 2015</xref>). It is also possible to identify systematic biases in the patterns of responses (e.g., a bias to report clockwise or counterclockwise). We used facial expressions morphed from neutral to fearful and neutral to happy to test age-related changes in WM for emotional material. Facial expressions were chosen to produce a set of stimuli that varied on a continuous scale of positive and negative emotion. Happy faces and fearful faces were selected after consideration of their common use in previous studies on affective attentional biases (e.g., <xref rid="c14" ref-type="bibr" id="cr14-1">Fox, 2002</xref>; <xref rid="c37" ref-type="bibr" id="cr37-1">Pourtois, Grandjean, Sander, &amp; Vuilleumier, 2004</xref>) and studies that found a relationship between attentional biases for fear-related stimuli (including faces) in anxiety (<xref rid="c46" ref-type="bibr" id="cr46-1">Yiend, 2010</xref>).</p>
            <p>In the current emotional WM task, participants encoded a face into WM with an emotional expression (fearful or happy) with a certain emotional intensity. After a delay, participants used a mouse to adjust a facial expression to match the emotion type and intensity in memory. In a separate perceptual emotion-matching experiment, participants adjusted one face to match the expression of another face on the screen. Using these tasks, we compared performance accuracy and emotional bias between groups of older and younger participants to test how WM and perception for emotional material change with age. Given previous work, we might expect preserved facilitation in tasks with emotional stimuli, or only for positive stimuli, to generalize to WM and therefore mitigate against age-related deficits in WM performance in older adults. Furthermore, we might expect to measure a systematic shift in reporting the valence and emotional intensity of emotional expressions in WM, whereby fearful faces would be reported as less fearful and/or happy faces as more positive.</p>
            <sec id="s2">
              <title>Method</title>
              <sec id="s3">
                <title>Participants</title>
                <p>Fifty-four young participants and 54 older participants volunteered to participate in the study and received compensation and travel expenses where required. The study was approved by the Central University Research Ethics Committee of the University of Oxford, and was carried out in accordance with the provisions of the World Medical Association Declaration of Helsinki (<xref rid="c51" ref-type="bibr" id="cr51-1">World Medical Association Declaration of Helsinki, 2013</xref>). Before taking part in the study, individuals were sent an electronic screening questionnaire, which included <xref rid="c43" ref-type="bibr" id="cr43-1">Spielberger’s (1983)</xref> trait-anxiety questionnaire, the State–Trait Anxiety Inventory–Trait version; STAI) and a series of questions. People who reported current use of psychoactive medication, history of recreational drug use, history of neurological illness, or took part in studies involving WM training or emotional face stimuli in the past 6 months were not invited to participate. Data from one elderly participant were excluded because of a low score (less than 26) on the Montreal Cognitive Assessment (MoCA; <xref rid="c32" ref-type="bibr" id="cr32-1">Nasreddine et al., 2005</xref>), and data from two other elderly participants were not saved because of a technical error. After excluding these participants, 54 younger adults (39 women, <italic>M</italic><sub>age</sub> = 23.42, <italic>SEM</italic> = .60, age range = 18–35 years) and 51 older adults (29 women, <italic>M</italic><sub>age</sub> = 69.25 ± .78, age range = 61–82 years) were included in the current study. All remaining participants were fluent in English, had normal or corrected-to-normal vision and hearing, and all older participants scored &gt;26 on the MoCA, <italic>M</italic> = 28.16, <italic>SEM</italic> = .16; younger adults did not complete the MoCA. Sample sizes were determined with the aim of comparing performance between age groups and investigating the relationship between anxiety and behavioral measures. A survey of studies testing age differences in WM for emotional material that presented sufficient information for a power analysis (<xref rid="c26" ref-type="bibr" id="cr26-3">Mammarella et al., 2013</xref>; <xref rid="c44" ref-type="bibr" id="cr44-3">Truong &amp; Yang, 2014</xref>) revealed that a minimum of 11 to 30 participants per group are required for 80% power, and a minimum of 14 to 41 participants per group are required for 90% power. A survey of previous studies that reported a relationship between measures of anxiety and attentional bias (using Pearson’s correlations) shows that the average correlation coefficient was 0.315 (<xref rid="c5" ref-type="bibr" id="cr5-1">Bradley, Mogg, &amp; Millar, 2000</xref>; <xref rid="c15" ref-type="bibr" id="cr15-1">Fox, Cahill, &amp; Zougkou, 2010</xref>; <xref rid="c16" ref-type="bibr" id="cr16-1">Fox, Mathews, Calder, &amp; Yiend, 2007</xref>). A power calculation indicates that 77 participants provide 80% power to find a significant effect (<xref rid="c21" ref-type="bibr" id="cr21-1">Hulley, Cummings, Browner, Grady, &amp; Newman, 2013</xref>). Our sample of 105 provides 90% power.</p>
              </sec>
              <sec id="s4">
                <title>Stimuli and Apparatus</title>
                <p>Stimuli were adapted from faces in the NimStim Stimulus Set (<ext-link ext-link-type="uri" xlink:href="http://www.macbrain.org/resources.htm" specific-use="live">http://www.macbrain.org/resources.htm</ext-link>) with permission. Forty-eight face stimuli (three emotional expressions for each of 16 identities) were selected. Happy, fearful, and neutral face images were cropped with an elliptical mask and morphed from neutral to fearful and from neutral to happy in 1% steps to produce faces with graded intensities of emotional expressions from 0% to 100% (see <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/emo0000481.supp" specific-use="live">Figures S1</ext-link> and <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/emo0000481.supp" specific-use="live">S2</ext-link> for examples). Ten identities were selected for the main experiment and six for the practice session. Scrambled masks were produced for each stimulus by randomly shuffling pixels within the elliptical mask (see <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/emo0000481.supp" specific-use="live">online supplementary materials</ext-link> for details).</p>
              </sec>
              <sec id="s5">
                <title>Task Design and Procedure</title>
                <sec id="s6">
                  <title>Emotion WM task</title>
                  <p>On each trial, participants encoded a face into memory and were asked to recall this face at the end of the trial. Stimuli were faces with pseudorandomly selected levels of emotional intensity values of 0% to 45% and 55% to 100% in 5% steps (leaving out 50%), with one set of intensity values for each emotion type, happy and fearful. Emotion Type conditions were intermixed within each block (see <xref ref-type="fig" id="fgc1-1" rid="fig1">Figure 1a</xref>).<xref ref-type="fig-anchor" rid="fig1"/></p>
                  <p>On each trial, a “GO” screen signaled to start the trial with a left mouse click. A fixation cross was presented at the center of the screen (800 ms), after which a face (500 ms) and a scrambled mask (100 ms) were presented. After a delay of 3,000 ms, a test face was presented with a neutral expression (0% intensity). Participants adjusted its expression to match the emotion type and intensity of the face in memory. Participants adjusted the face with a trackball mouse, scrolling left for one emotion and right for the other emotion (happy/fearful, counterbalanced across participants) and clicked to confirm their responses. After each block, feedback was given (percent correct; computed by 100 minus the average deviation of responses from the target emotional intensity, or mean error). Participants were asked to fixate centrally, and, if they consistently broke fixation, they were reminded to refrain from doing so at the next break. Accuracy was stressed over RT. Maximum response time was 11 s, but participants were encouraged to respond within 6 s in the interest of time and to reduce memory degradation. At 11 s, the emotional intensity on the screen was saved as the response.</p>
                  <p>Each participant completed eight blocks of 20 trials. For each emotion type (fear, happy), each emotional intensity level was presented four times, giving 80 trials per emotion type. For each participant, facial identities were pseudorandomly allocated over each emotional intensity condition and all 10 identities were included in both Emotion Type conditions. For each identity, there were 16 trials for each emotional intensity condition (from 0% to 100% with 5% steps, excluding 50%). Because there were 19 intensities per emotion type, plus a neutral face condition (0% intensity), not all intensity conditions were presented for each identity (the smallest range was 5% to 80%, but most identities spanned 0% to 100% for both emotion types). The number of emotional intensity conditions was kept constant (80 per condition per emotion).</p>
                </sec>
                <sec id="s7">
                  <title>Emotion-matching task</title>
                  <p>Participants were presented with a target face on the left of the screen and adjusted the face on the right to match the emotion type and intensity of the target face. As in the WM task, stimuli were happy or fearful faces with the same range of emotion-intensity conditions and identities (but the pairing of emotion-intensity conditions and identities was different). Emotion Type conditions were intermixed within each block (see <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/emo0000481.supp" specific-use="live">Figure S3a</ext-link>).</p>
                  <p>Each trial began with a “GO” screen and the trial started with a mouse click. A fixation cross was presented (800 ms), after which two faces with the same identity appeared on the left and right side of the screen. Participants adjusted the expression of the face on the right to match the emotion type and intensity of the face on the left. The right face had a neutral expression, and participants adjusted the expression using a trackball mouse. As in the WM task, feedback was given after each block, accuracy stressed over RT, with the same time constraints. Eye movements were not constrained.</p>
                  <p>Each participant completed two blocks of 20 trials. For each emotion, each emotion-intensity condition was presented twice, with 20 trials per emotion type. As with the WM task, the facial identities were randomly allocated over each emotional intensity level. The identities associated with each emotion-intensity condition were different than those in the WM task.</p>
                </sec>
                <sec id="s8">
                  <title>Mood questionnaires</title>
                  <p>Participants completed five self-report questionnaires measuring state and trait anxiety (STAI; <xref rid="c43" ref-type="bibr" id="cr43-2">Spielberger, 1983</xref>), Beck’s Depression Inventory (BDI; <xref rid="c3" ref-type="bibr" id="cr3-1">Beck, Ward, Mendelson, Mock, &amp; Erbaugh, 1961</xref>), and positive and negative affective states and traits (short version of the Positive and Negative Affect Scale; PANAS; <xref rid="c45" ref-type="bibr" id="cr45-1">Watson, Clark, &amp; Tellegen, 1988</xref>) immediately before the experimental session.</p>
                </sec>
              </sec>
              <sec id="s9">
                <title>Data Analysis</title>
                <p>The aim of the analyses was to characterize age-related differences in WM for emotional material in terms of error (deviation of responses from target emotional intensities), emotional bias (representing information as more positive or negative), and valence (categorical judgment of a fearful or happy face).</p>
                <p>In both the emotion-WM and emotion-matching tasks, the target facial expressions included 0% intensity (neutral) and ranged from 1% to 100% in 5% steps (excluding 50%) in emotional intensity of the target emotion type, and participants could report emotional intensities, which ranged from 1% to 100% of the target emotion (e.g., fear). They could also report the other emotion (e.g., happy), which was recorded as a response (from −1 to −100%) or a neutral expression (0%). To calculate error, participant responses (positive or negative) were subtracted from the target emotional intensities (positive) on each respective trial, giving an error distribution—the deviation of intensities reported by participants (responses) from the actual intensity values (targets). Responses to the other emotion type produced values with a negative sign. For instance, if a target face was 50% happy and a response was 60% happy, the error was |50 – 60| = 10. A response of 40% happy would also yield an absolute error of 10. If a target was 20% fearful and the response was 15% fearful, the error would be |20 – 15| = 5. If the response was 15% happy, then the error would be |20 – (−15)| = 35. The highest possible error would be 200 (e.g., if target face was 100% fearful and the response was 100% happy), but the maximum error decreases proportionally to the valence of the stimuli (e.g., if target face was 50% fearful, the maximum error would be 150). Error was computed by taking the mean of the absolute (positive) error values across trials. Statistical tests were also performed after excluding trials in which participants reported the incorrect emotion type and trials with neutral targets. See the <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/emo0000481.supp" specific-use="live">online supplementary materials</ext-link> (Trial Numbers) for details of excluded trials. Trials in which participants used up the maximum time for a response (11 s) did not have an effect on the results (for details, see <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/emo0000481.supp" specific-use="live">online supplementary materials</ext-link>, Maximum Response Time Trials).</p>
                <p>Emotional bias was derived from the shift in the psychometric function of responses. Participants’ responses were plotted as a function of the actual emotion type and emotional intensity of the target face, with negative values representing intensities of fearful faces and positive values representing intensities of happy faces (see <xref ref-type="fig" id="fgc2-1" rid="fig2">Figure 2a</xref>). Note that “response” values are the actual emotional intensity values that participants reported, unlike the error values above, which were calculated relative to the target. To obtain an overall measure of bias, we computed the mean of this curve (mean response across all intensity conditions, −100% to 100%). Positive bias values corresponded to the tendency to report faces as either more positive or less negative (or both); negative bias values reflected the tendency to report faces as less positive or more negative (or both), and a value of zero corresponded to no bias. For instance, if a target face was 50% happy and a response was 60% happy or 40% happy, the bias value on those trials would be 10% and −10% respectively. If a target face was 20% fearful and a response was 15% happy, the bias would be 35% (15 – [−20]), whereas if the target face were 15% happy and response was 20% fearful, the bias would be −35% (−20 – 15). Note that intensity values of happy faces are positive, and values of fearful faces are negative. The most negative possible bias would be −200% (if target face was 100% happy and the response was 100% fearful) and most positive bias would be 200% (if target face was 100% fearful and the response was 100% happy), but would normally be lower. Statistical tests were also performed after excluding trials in which participants reported the incorrect emotion type, because trials in which participants judged happy faces to be fearful might contribute to an overall negative bias, and trials in which participants judged fearful faces to be happy might contribute to an overall positive bias.<xref ref-type="fig-anchor" rid="fig2"/></p>
                <p>To test for biases that stemmed specifically from the fearful or happy conditions, bias was computed for emotion types separately. First, responses for the fearful faces were flipped to positive to be compared with happy bias values. Second, the trials with neutral faces (0% intensity) were excluded. The mean response was computed across emotional intensities for each emotion type (from 1% to 100%), then normalized by subtracting 50 to match the overall bias measure, so that a value of zero would reflect no bias.</p>
                <p>To characterize judgments of valence (categorical judgment of fearful or happy), we separated responses into the correct and incorrect emotion types, which we excluded in a subset of the analyses above. Reporting the incorrect emotion occurred when a participant adjusted a face to the wrong emotion type (e.g., reported 25% fearful face but the target was a happy face). To inspect the effect of emotional intensity on valence judgments, trials were binned into five equal bins of emotional intensity (i.e., 1–20, 21–40, 41–60, 61–80, 81–100). Proportion correct was computed for each target emotion intensity bin (e.g., a proportion of .7 correct for a given intensity bin meant participants reported the correct emotion type 70% of the time and the incorrect emotion type 30% of the time).</p>
                <p>A mixed repeated-measures analysis of covariance (ANCOVA) was conducted on WM error, with within-subject factor Emotion Type (fearful, happy), between-subjects factor Age (young, old), and continuous factor Anxiety (STAI trait). Anxiety was included to test for the relationship between behavior and mood. A mixed repeated-measures ANCOVA was performed on emotional bias to test between-subjects factor Age with continuous factor Anxiety. To test whether bias effects were driven by happy or fearful faces, a mixed repeated-measures ANCOVA was conducted on WM bias for happy and fearful face conditions with Emotion Type, Age, and Anxiety factors. For the emotion-matching task, the same ANCOVAs listed above were conducted. All ANCOVAs above were recomputed after excluding trials to the incorrect emotion type and neutral target face trials. A mixed repeated-measures ANCOVA was conducted on proportion of correctly categorized faces in the emotion WM and emotion-matching task separately, with Emotion Type, Intensity (1–20, 21–40, 41–60, 61–80, 81–100), Age, and Anxiety factors. Gender was included in all ANCOVAs as a covariate of no interest. Degrees of freedom were corrected using Greenhouse–Geisser estimates of sphericity when normality assumptions were violated.</p>
                <p>Paired <italic>t</italic> tests were used to test for paired-condition differences, and independent-samples <italic>t</italic> tests to compare between age groups. To test for the direction of linear contrasts, we used a one-sample <italic>t</italic> test to see if slopes and differences in slopes (between emotion types) were different from zero. Cohen’s <italic>d</italic> was used to determine effect sizes. All analyses conducted are reported in this section. Confidence intervals for Cohen’s <italic>d</italic> and η<sub>p</sub><sup>2</sup> (for ANCOVAs) were calculated using the MBESS package in R (for between-subjects effects), or from custom R code (for within-subject effects, see <ext-link ext-link-type="uri" xlink:href="https://github.com/Lakens/perfect-t-test" specific-use="live">https://github.com/Lakens/perfect-t-test</ext-link>).</p>
                <p>Statistical analyses were conducted in Matlab R2015a, Matlab’s Statistics Toolbox, and R Version 3.2.1 (R Core Team, Vienna, Austria) using the afex package (<xref rid="c41" ref-type="bibr" id="cr41-1">Singmann, Bolker, &amp; Westfall, 2015</xref>) and MBESS package. The code to run the experiment (Matlab, Psychtoolbox), data-analysis code (Matlab, R), and the behavioral data are available online (see <ext-link ext-link-type="uri" xlink:href="https://osf.io/a47xe/" specific-use="live">https://osf.io/a47xe/</ext-link>). The authors are happy to share the data and the experimental scripts. However, before we are able to share the stimuli, which are necessary for the task, permission needs to be obtained to use the NimStim faces from the original creators. These stimuli are for research purposes only (see <ext-link ext-link-type="uri" xlink:href="http://danlab7.wixsite.com/nimstim" specific-use="live">http://danlab7.wixsite.com/nimstim</ext-link>)<xref ref-type="fn" rid="fn1"><sup>1</sup></xref>.</p>
              </sec>
            </sec>
            <sec id="s10">
              <title>Results</title>
              <sec id="s11">
                <title>Accuracy for Matching Emotional Faces in WM and Perception</title>
                <p>Participants completed the emotion-WM task with high accuracy (percent error: <italic>M</italic><sub>youngfear</sub> = 17.20, <italic>SEM</italic> = .48, <italic>M</italic><sub>oldfear</sub> = 20.30, <italic>SEM</italic> = .53; <italic>M</italic><sub>younghappy</sub> = 15.34, <italic>SEM</italic> = .53, <italic>M</italic><sub>oldhappy</sub> = 17.13, <italic>SEM</italic> = .56) and showed better WM performance for happy than fearful faces, Emotion Type: <italic>F</italic>(1, 101) = 40.20, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .28, 90% CI [.17, .39]. Older adults showed a general deficit in WM for emotional content, Age: <italic>F</italic>(1, 101) = 14.10, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .12, 90% CI [.04, .22], which was more prominent for fearful faces, Age × Emotion Type interaction: <italic>F</italic>(1, 101) = 4.84, <italic>p</italic> = .03, η<sub>p</sub><sup>2</sup> = .05, CI 90% [.002, .12]; young versus old fear: <italic>t</italic>(101.04) = −4.29, <italic>p</italic> &lt; .001, <italic>d</italic> = −.84, 95% CI [−1.24, −.44]; happy: <italic>t</italic>(102.43) = −2.31, <italic>p</italic> = .023, <italic>d</italic> = −.45, 95% CI [−0.84, −0.06]; see <xref ref-type="fig" id="fgc1-2" rid="fig1">Figure 1b</xref>. The results were similar after excluding trials in which participants reported the incorrect emotion type (error: <italic>M</italic><sub>youngfear</sub> = 16.42, <italic>SEM</italic> = .48, <italic>M</italic><sub>oldfear</sub> = 18.54, <italic>SEM</italic> = .46; <italic>M</italic><sub>younghappy</sub> = 11.97, <italic>SEM</italic> = .33, <italic>M</italic><sub>oldhappy</sub> = 13.58, <italic>SEM</italic> = .40), with better performance for happy than for fearful faces, Emotion Type: <italic>F</italic>(1, 101) = 221.61, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .69, 90% CI [.60, .74]. Older adults were still significantly worse than the younger group, Age: <italic>F</italic>(1, 101) = 14.35, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .12, 90% CI [.04, .22], but there was no longer an interaction between Age and Emotion Type, <italic>F</italic>(1, 101) = .89, <italic>p</italic> = .35, η<sub>p</sub><sup>2</sup> = .01, 90% CI [0, .06].</p>
                <p>Although there were fewer trials in the perceptual matching task, the pattern of results was similar to the WM task (error: <italic>M</italic><sub>youngfear</sub> = 10.42, <italic>SEM</italic> = .42, <italic>M</italic><sub>oldfear</sub> = 12.67, <italic>SEM</italic> = .52; <italic>M</italic><sub>younghappy</sub> = 7.09, <italic>SEM</italic> = .34, <italic>M</italic><sub>oldhappy</sub> = 8.15, <italic>SEM</italic> = .40), with better performance for happy than for fearful matching, Emotion Type: <italic>F</italic>(1, 101) = 125.34, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .55, 90% CI [.44, .63]. Older adults were worse than younger adults at matching the emotional faces, Age: <italic>F</italic>(1, 101) = 10.00, <italic>p</italic> = .002, η<sub>p</sub><sup>2</sup> = .09, 90% CI [.02, .18]. After excluding trials in which participants erroneously reported the incorrect emotion type, the pattern of performance was similar, error: <italic>M</italic><sub>youngfear</sub> = 10.23, <italic>SEM</italic> = .43, <italic>M</italic><sub>oldfear</sub> = 12.06, <italic>SEM</italic> = .49; <italic>M</italic><sub>younghappy</sub> = 6.72, <italic>SEM</italic> = .32, <italic>M</italic><sub>oldhappy</sub> = 7.54, <italic>SEM</italic> = .33; Emotion Type: <italic>F</italic>(1, 101) = 145.48, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .59, 90% CI [.49, .66], with a strong effect of Age, <italic>F</italic>(1, 101) = 7.40, <italic>p</italic> = .008, η<sub>p</sub><sup>2</sup> = .07, 90% CI [.01, .15].</p>
              </sec>
              <sec id="s12">
                <title>Age-Related Shifts of Emotional Bias in WM</title>
                <p>Younger adults exhibited a stronger negative shift in their WM psychometric curves than older adults, <italic>M</italic><sub>young</sub> = −3.25, <italic>SEM</italic> = .52; <italic>M</italic><sub>old</sub> = −1.26 <italic>SEM</italic> = .68; <italic>F</italic>(101) = 6.55, <italic>p</italic> = .01, η<sub>p</sub><sup>2</sup> = .06, 90% CI [.007, .15]; see <xref ref-type="fig" id="fgc2-2" rid="fig2">Figure 2a–b</xref>. They reported fearful faces as more emotionally intense than older adults: see <xref ref-type="fig" id="fgc2-3" rid="fig2">Figure 2c</xref>; Age × Emotion Type interaction, <italic>F</italic>(1, 101) = 6.27, <italic>p</italic> = .01, η<sub>p</sub><sup>2</sup> = .06, 90% CI [.006, .14]; young versus old: fear, <italic>t</italic>(97.51) = 2.87, <italic>p</italic> = .005, <italic>d</italic> = .56, 95% CI [.17, .95]; happy, <italic>t</italic>(94.65) = .52, <italic>p</italic> = .60, <italic>d</italic> = .10, 95% CI [–.28, 0.48]). After excluding responses to the incorrect emotion type, there was still a difference between age groups, <italic>M</italic><sub>young</sub> = −2.24, <italic>SEM</italic> = .48; <italic>M</italic><sub>old</sub> = –.46, <italic>SEM</italic> = .61; <italic>F</italic>(1, 101) = 5.83, <italic>p</italic> = .02, η<sub>p</sub><sup>2</sup> = .05, 90% CI [.005, .14], and the effect was likely due to the difference in fearful faces, Age × Emotion Type interaction: <italic>F</italic>(1, 101) = 3.20, <italic>p</italic> = .08, η<sub>p</sub><sup>2</sup> = .03, 90% CI [0, 10]; young versus old: fear, <italic>t</italic>(100.5) = 1.84, <italic>p</italic> = .069, <italic>d</italic> = .36, 95% CI [–.27, .74]; younger versus old: happy, <italic>t</italic>(88.49) = .26, <italic>p</italic> = .79, <italic>d</italic> = .05, 95% CI [–.33, .43]. There were no significant results in the perceptual matching task for these effects (see <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/emo0000481.supp" specific-use="live">Figure S4</ext-link>).</p>
              </sec>
              <sec id="s13">
                <title>Age-Related Changes in Emotion Interpretation in WM</title>
                <p>Although participants generally reported the correct emotion type in the emotion WM task, they also mistakenly interpreted the face to have the incorrect emotion type on a sizable proportion of trials (proportion of happy faces reported fearful: <italic>M</italic><sub>young</sub> = .14, <italic>SEM</italic> = .01/<italic>M</italic><sub>old</sub> = .12 ± .01; proportion of fearful faces reported happy: <italic>M</italic><sub>young</sub> = .05, <italic>SEM</italic> = .01, <italic>M</italic><sub>old</sub>= .08, <italic>SEM</italic> = .01) and on a minority of the trials in the perceptual expression-matching task (proportion of happy faces reported fearful: <italic>M</italic><sub>young</sub> = .04, <italic>SEM</italic> = .01, <italic>M</italic><sub>old</sub> = .05, <italic>SEM</italic> = .01, proportion of fearful faces reported happy: <italic>M</italic><sub>young</sub>= .04, <italic>SEM</italic> = .01, <italic>M</italic><sub>old</sub> = .07, <italic>SEM</italic> = .01). <xref ref-type="fig" id="fgc3-1" rid="fig3">Figure 3</xref> shows the proportion of trials in which participants reported the correct emotion type for each intensity bin (see <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/emo0000481.supp" specific-use="live">Figure S6</ext-link> for scatterplots that illustrate the pattern of responses across intensities).<xref ref-type="fig-anchor" rid="fig3"/></p>
                <p>Participants were more likely to report the correct emotion type for fearful-face trials compared with happy-face trials, Emotion Type: <italic>F</italic>(1, 101) = 53.78, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .35, 90% CI [.22, .45]. Ambiguous, low-intensity emotional faces were more likely to be misinterpreted as the other emotion type, <italic>F</italic>(1.89, 191.08) = 374.25, <italic>p</italic> &lt; .001; η<sub>p</sub><sup>2</sup> = .79, 90% CI [.74, .82]; <italic>M</italic><sub>slope</sub> = .07, <italic>SEM</italic> = .003; <italic>t</italic>(104) = 23.85, <italic>p</italic> &lt; .001, <italic>d</italic> = 2.33, 95% CI [1.96, 2.70], and this effect was stronger for happy compared with fearful faces, Emotion Type × Intensity: <italic>F</italic>(1.74, 175.95) = 43.57, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .30, 90% CI [.21, .38]; happy <italic>M</italic><sub>slope</sub> = .09 ± .005, <italic>t</italic>(104) = 18.8, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.84, 95% CI [1.52, 2.15]; fear <italic>M</italic><sub>slope</sub> = .04, <italic>SEM</italic> = .004, <italic>t</italic>(104) = 11.7, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.15, 95% CI [.90, 1.39]; <italic>M</italic><sub>slopediff</sub> =.049, <italic>SEM</italic> = .007, <italic>t</italic>(104) = −7.42, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.09, 95% CI [.77, 1.42].</p>
                <p>Crucially, older adults were more likely to judge a fearful face as a happy one in the WM task, Age × Emotion Type interaction: <italic>F</italic>(1, 101) = 10.06, <italic>p</italic> = .002, η<sub>p</sub><sup>2</sup> = .09, 90% CI [.02, .18]; <italic>t</italic>(77.75) = −3.56, <italic>p</italic> &lt; .001, <italic>d</italic> = −.70, 95% CI [−1.09, −.30], whereas both age groups judged happy faces as fearful to a similar extent, <italic>t</italic>(102.98) = .80, <italic>p</italic> = .43, <italic>d</italic> = .15, 95% CI [–.23, .54]. This effect was modulated by the emotional intensity of the face stored in WM, Age × Emotion Type × Intensity interaction: <italic>F</italic>(1.74,175.95) = 8.17, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .07, 90% CI [.02, .14], whereas older adults tended to judge fearful faces with low-to-medium intensities as happier than did the younger adults, <italic>M</italic><sub>slopediff</sub> = .02, <italic>SEM</italic> = .04; <italic>t</italic>(82.15) = 2.87, <italic>p</italic> = .005, <italic>d</italic> = .56, 95% CI [0.17, 0.95], but not for the happy faces, <italic>M</italic><sub>slopediff</sub> = −.01, <italic>SEM</italic> = .04; <italic>t</italic>(102.18) = −1.50, <italic>p</italic> = .14, <italic>d</italic> = −.29, 95% CI [–.68, .09].</p>
                <p>In the perceptual matching task, participants showed the opposite pattern for Emotion Type, such that they mistakenly interpreted fearful faces as happy more than they judged happy faces as fearful, Emotion Type: <italic>F</italic>(1, 101) = 8.71, <italic>p</italic> = .004, η<sub>p</sub><sup>2</sup> = .08, 90% CI [.02, .17]. Participants incorrectly reported the Emotion Type for faces with low emotional intensity, <italic>F</italic>(1.44,145.05) = 63.84, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .39, 90% CI [.28, .47]; <italic>M</italic><sub>slope</sub> = .03, <italic>SEM</italic> = .004, <italic>t</italic>(104) = 10.05, <italic>p</italic> &lt; .001, <italic>d</italic> = .98, 95% CI [.75, 1.21] and this effect was slightly stronger for fearful than for happy faces, <italic>F</italic>(1.59,160.68) = 5.64, <italic>p</italic> = .008, η<sub>p</sub><sup>2</sup> = .05, 90% CI [.01, .11]; fear <italic>M</italic><sub>slope</sub> = .05 ± .006, <italic>t</italic>(104) = 7.56, <italic>p</italic> &lt; .001, <italic>d</italic> = .74, 95% CI [.52, .95]; happy <italic>M</italic><sub>slope</sub> = .03 ± .004, <italic>t</italic>(104) = 7.07, <italic>p</italic> &lt; .001, <italic>d</italic> = .69, 95% CI [.48, .90]; <italic>M</italic><sub>slopediff</sub> = .015, <italic>SEM</italic> = .007, <italic>t</italic>(104) = 2.03, <italic>p</italic> = .045, <italic>d</italic> = .27, 95% CI [.006, .54]. Notably, there was only a trend for an interaction of Emotion Type with Age, <italic>F</italic>(1, 101) = 3.45, <italic>p</italic> = .07, η<sub>p</sub><sup>2</sup> = .03, 90% CI [0, .11] and no significant three-way interaction with Intensity, <italic>F</italic>(1.59,160.68) = 1.96, <italic>p</italic> = .15, η<sub>p</sub><sup>2</sup> = .02, 90% CI [0, .06]; see <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/emo0000481.supp" specific-use="live">Figure S5</ext-link>.</p>
              </sec>
              <sec id="s14">
                <title>Self-Reported Mood Measures</title>
                <p>There were no significant differences between age groups for measures on Trait Anxiety, <italic>t</italic>(102.4) = .97, <italic>p</italic> = .33, <italic>d</italic> = .19, 95% CI [–.19, .57]; State Anxiety, <italic>t</italic>(102.6) = .50, <italic>p</italic> = .61, <italic>d</italic> = .10, 95% CI [–.29, .48]; BDI, <italic>t</italic>(89.8) = −1.14, <italic>p</italic> = .26, <italic>d</italic> = −.22, 95% CI [–.61, 0.16]; short PANAS Positive, <italic>t</italic>(101.5) = −1.61, <italic>p</italic> = .11, <italic>d</italic> = −.31, 95% CI [–.70, 0.07]; or short PANAS Negative, <italic>t</italic>(96.9) = 1.73, <italic>p</italic> = .09, <italic>d</italic> = .34, 95% CI, [–.05, 0.72] questionnaires.</p>
                <p>Trait Anxiety was correlated with a small number of measures in the emotion-WM task, but these effects were relatively weak and inconsistent when including versus excluding trials in which participants reported the wrong emotion type, which suggests no strong relationship between our behavioral measures and Trait Anxiety in the present sample.</p>
              </sec>
            </sec>
            <sec id="s15">
              <title>Discussion</title>
              <p>We tested younger and older adults on novel precision emotion-WM and emotion-matching tasks and found age-related changes in the way emotional content was represented in WM. Specifically, older adults recalled fearful faces from WM as being less fearful than did younger adults, indicating an age-related attenuation in the representation of negative information in WM. Furthermore, older adults exhibited a positive interpretation bias whereby they were more likely to categorize low-intensity, fearful faces as being happy compared with younger adults. There were similarities between the patterns of results for the perceptual matching and WM tasks, but the results were relatively weak in the perception task and did not reach statistical significance. Separate from the changes in emotional bias, we found a general age-related impairment, such that older adults performed worse than younger adults in the WM and perceptual emotion-matching task for both happy and fearful faces.</p>
              <p>By developing a novel task and analysis procedure, we revealed that the representation of emotional expressions in WM changes with age; older adults exhibited a systematic bias to remember fearful faces as less fearful than younger adults. There was no difference in bias for happy faces, suggesting that it is the representation of negative information in WM, and not positive information, that changes with age. Of interest, the pattern of results was dissociable from a general decline in WM accuracy: Older adults showed worse performance in WM and in the emotion-matching task for both happy and fearful faces.</p>
              <p>The age-related difference in bias for fearful faces was partly driven by more ambiguous expressions closer to neutral emotion. Low-valence fearful faces were sometimes mistakenly interpreted as happy faces. Exclusion of such miscategorization trials dampened some of the relevant statistics, partly by lowering statistical power, but did not affect the overall pattern of results showing a shift toward a positivity bias in older participants. The interaction between Age and Emotion Type became a marginal trend, but the affective bias reflected in the shift in the psychometric curve remained robust after exclusion of incorrect responses. Furthermore, inspecting the curves suggests that the age-related bias occurred not only for low-intensity ambiguous faces, but also extended to faces displaying medium- and high-intensity fear. Overall, the findings suggest that the age-related difference in bias may partly reflect reinterpretation of ambiguous expressions, but is not confined to such a process, extending also to attenuating emotional content in stimuli with higher emotional valence.</p>
              <p>To date, most studies that have explored age-related changes in emotional processing have used accuracy-based measures of bias, which give a measure of preferential processing (e.g., attending more to positive than negative stimuli), but leave open how the information was represented that lead to the behavioral effect. We note that, although performance impairments were greater for fearful than for happy faces, categorization and memory performance are often better for happy faces (e.g., <xref rid="c7" ref-type="bibr" id="cr7-1">Calder et al., 2003</xref>), suggesting effects related to perceptual features. Thus, our task was able to show how negative affective information in WM is attenuated with age, and that this was separate from age-related declines in WM.</p>
              <p>Our task also enabled us to inspect age-related changes for interpreting ambiguous emotional expressions in WM. Older adults tended to judge low-intensity, ambiguously fearful faces as happier than did younger adults, suggestive of a positive interpretation bias in WM. Although participants were more likely to misinterpret low-intensity happy faces as fearful (cf. <xref rid="c36" ref-type="bibr" id="cr36-1">Phillips et al., 1998</xref>), older adults were more likely to report low-intensity fearful faces as happy, reflecting a tendency to interpret ambiguous expressions from WM positively. These results are consistent with emotion-categorization studies with ambiguous expressions (<xref rid="c6" ref-type="bibr" id="cr6-1">Bucks, Garner, Tarrant, Bradley, &amp; Mogg, 2008</xref>; <xref rid="c23" ref-type="bibr" id="cr23-2">Kellough &amp; Knight, 2012</xref>). Together, our findings indicate that older adults show an attenuation of negative information in WM, and a positive intepretation bias when dealing with ambiguous information. Our results are consistent with the positivity effect in aging (<xref rid="c8" ref-type="bibr" id="cr8-1">Carstensen, Isaacowitz, &amp; Charles, 1999</xref>; <xref rid="c27" ref-type="bibr" id="cr27-2">Mather &amp; Carstensen, 2005</xref>), but demonstrate that age-related differences can stem from multiple sources. With standard accuracy-based measures, it can be hard to determine why accuracy differences between positive and negative emotion conditions arise. New experimental paradigms and analysis methods designed to measure different types of emotional bias, such as those presented here, could lead to deeper insights into group and individual differences in affective processing.</p>
              <p>Age-related differences in the perceptual matching task somewhat resembled the WM results, but did not reach statistical significance. Our perceptual matching task was primarily designed to ensure that older participants could perceive the task stimuli sufficiently well and were able to produce responses that reproduced emotional content with high levels of precision. The task worked well in this regard, showing high levels of accuracy. Of interest, however, though not statistically significant, the pattern of results is suggestive that positivity biases may even operate when making purely perceptual judgments. Unfortunately, because of the purpose for which we designed the perceptual matching task, the smaller number of trials may have precluded robust testing of this possibility. It will be interesting, therefore, for future studies to extend on the current findings to test for potential emotional biases in interpreting perceptual stimuli.</p>
              <p>Three previous studies have reported that WM for emotional content is preserved in aging, regardless of the valence (<xref rid="c20" ref-type="bibr" id="cr20-3">Hartley et al., 2015</xref>; <xref rid="c26" ref-type="bibr" id="cr26-4">Mammarella et al., 2013</xref>; <xref rid="c44" ref-type="bibr" id="cr44-4">Truong &amp; Yang, 2014</xref>), but the way they tested WM was fundamentally different from our task. <xref rid="c26" ref-type="bibr" id="cr26-5">Mammarella et al. (2013)</xref> tested WM for emotional and neutral words, whereas we used faces. Semantic meaning may be more similar across age groups, which might lead to a similar meaning-based memory benefit for emotional words (also see <xref rid="c44" ref-type="bibr" id="cr44-5">Truong &amp; Yang, 2014</xref>). <xref rid="c20" ref-type="bibr" id="cr20-4">Hartley et al. (2015)</xref> used change-detection WM tasks with emotional faces, and found that older adults performed as well as younger adults in the emotional expression task but were impaired in the identity task. However, in the expression change-detection task, participants only had to recall the expressions without needing to remember visual features, which may have encouraged use of emotional expression labels. Furthermore, because they used an accuracy-based measure, it is unclear why there was a performance benefit. Another study using a judgment-based measure of performance found that older adults performed better on positive than negative images on a WM task, whereas younger adults showed the opposite pattern (<xref rid="c29" ref-type="bibr" id="cr29-3">Mikels et al., 2005</xref>). It should be noted that the task used in this study had participants judge whether the image encoded into WM was more or less emotionally intense than the subsequently presented “test” item (which were images of different things), and accuracy was based on concordance with emotional intensity ratings from an independent group of younger adults. Finally, <xref rid="c4" ref-type="bibr" id="cr4-3">Bermudez and Souza (2017)</xref> used a serial presentation WM task with positive, neutral, and negative images, and found an interaction between valence and age, revealing that older adults showed poorer performance on negative images than on positive and neutral images, consistent with the age-related positivity effect. In the current study, we showed that older adults had a deficit in both WM and emotion-matching tasks with a particular deficit for fearful faces, consistent with deficits in emotion recognition (<xref rid="c40" ref-type="bibr" id="cr40-1">Ruffman, Henry, Livingstone, &amp; Phillips, 2008</xref>), and found age-related shifts in the affective content in WM unobtainable using accuracy measures alone.</p>
              <p>Face stimuli in this study comprised images of young adults. Although a previous study found no own-age bias for recognizing emotional expressions in younger and older participant groups (<xref rid="c13" ref-type="bibr" id="cr13-2">Ebner &amp; Johnson, 2009</xref>), it will be useful to extend the current findings using emotional faces of older adults. Another limitation of the current study was the focus on only fearful and happy emotional expressions. The precision-WM method we introduced should prove informative in charting to what extent biases are introduced in judging other emotional expressions, such as anger and disgust.</p>
              <p>Although previous studies have reported a relationship between measures of anxiety and performance with emotional stimuli (e.g., see <xref rid="c46" ref-type="bibr" id="cr46-2">Yiend, 2010</xref>), we did not find any reliable results to suggest that this is the case for WM. It could be that we did not recruit participants with a large enough range of anxiety scores, or that our measures might correlate with depressed mood (for which we did not have a good range). It will be interesting for future researchers to test participants with a larger range of mood scores (e.g., patients) to test whether there are biases in WM for emotional material linked to mood, and if this changes with age. Another interesting possibility for future work is to test the specificity of our age-related performance deficits to emotional stimuli. It would be interesting to test participants on both the emotion-WM task and a comparable WM task with nonemotional features, such as faces morphed from male to female, to test if age-related deficits would be worse than or similar to WM for emotion-relevant features.</p>
              <p>Our study employed a novel emotion-WM task that captured age-related impairments in cognition while revealing positive changes in emotional bias in WM that come with normal aging. Our findings provide support to the positivity-effect hypothesis in aging (<xref rid="c8" ref-type="bibr" id="cr8-2">Carstensen et al., 1999</xref>), revealing a more nuanced picture of the origin for this bias within WM. With our sensitive new approach, we were able to reveal multiple aspects of affective processing that undergo change in aging, including attenuation of negative information perception and a tendency for positive interpretation in WM. In future work, tasks and response methods that include continual measures of accuracy as well as measures of bias will be able to further reveal behavioral patterns in aging and characterize the emotional biases across individuals in mood and other psychological disorders.</p>
            </sec>
            <sec sec-type="supplementary-material">
              <title>Supplementary Material</title>
              <supplementary-material content-type="local-data">
                <object-id pub-id-type="doi">10.1037/emo0000481.supp</object-id>
                <media id="suppmat1" xlink:href="EmoAge_Emotion_sub_Supplementary_round3_final.docx" specific-use="live"/>
                <media id="suppmat2" xlink:href="Figure_S1_faces_example_round2.eps" specific-use="live"/>
                <media id="suppmat3" xlink:href="Figure_S2_faces_morph_example_round2.eps" specific-use="live"/>
                <media id="suppmat4" xlink:href="Figure_S3_p_task_error.eps" specific-use="live"/>
                <media id="suppmat5" xlink:href="Figure_S4_p_bias_round2.eps" specific-use="live"/>
                <media id="suppmat6" xlink:href="Figure_S5_p_interpretation.eps" specific-use="live"/>
                <media id="suppmat7" xlink:href="Figure_S6_scatters.eps" specific-use="live"/>
              </supplementary-material>
            </sec>
          </body>
          <back>
            <fn-group content-type="footnotes">
              <fn id="fn1">
                <label>1</label>
                <p>To access the stimuli, please follow the instructions in the link, forward the email with permission to use the stimuli to the authors, and a link to download the images will be shared.</p>
              </fn>
            </fn-group>
            <ref-list>
              <title>References</title>
              <ref id="c1">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baddeley</surname><given-names>A. D.</given-names></name>, &amp; <name><surname>Hitch</surname><given-names>G.</given-names></name></person-group> (<year>1974</year>). <article-title>Working memory</article-title>. <source>Psychology of Learning and Motivation</source>, <volume>8</volume>, <fpage>47</fpage>–<lpage>89</lpage>. <pub-id pub-id-type="doi">10.1016/S0079-7421(08)60452-1</pub-id></mixed-citation>
              </ref>
              <ref id="c2">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname><given-names>P. M.</given-names></name>, &amp; <name><surname>Husain</surname><given-names>M.</given-names></name></person-group> (<year>2008</year>). <article-title>Dynamic shifts of limited working memory resources in human vision</article-title>. <source>Science</source>, <volume>321</volume>, <fpage>851</fpage>–<lpage>854</lpage>. <pub-id pub-id-type="doi">10.1126/science.1158023</pub-id><pub-id pub-id-type="pmid">18687968</pub-id></mixed-citation>
              </ref>
              <ref id="c3">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>A. T.</given-names></name>, <name><surname>Ward</surname><given-names>C. H.</given-names></name>, <name><surname>Mendelson</surname><given-names>M.</given-names></name>, <name><surname>Mock</surname><given-names>J.</given-names></name>, &amp; <name><surname>Erbaugh</surname><given-names>J.</given-names></name></person-group> (<year>1961</year>). <article-title>An inventory for measuring depression</article-title>. <source>Archives of General Psychiatry</source>, <volume>4</volume>, <fpage>561</fpage>–<lpage>571</lpage>. <pub-id pub-id-type="doi">10.1001/archpsyc.1961.01710120031004</pub-id><pub-id pub-id-type="pmid">13688369</pub-id></mixed-citation>
              </ref>
              <ref id="c4">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bermudez</surname><given-names>T.</given-names></name>, &amp; <name><surname>Souza</surname><given-names>A. S.</given-names></name></person-group> (<year>2017</year>). <article-title>Can emotional content reduce the age gap in visual working memory? Evidence from two tasks</article-title>. <source>Cognition and Emotion</source>, <volume>31</volume>, <fpage>1676</fpage>–<lpage>1683</lpage>. <pub-id pub-id-type="doi">10.1080/02699931.2016.1240066</pub-id><pub-id pub-id-type="pmid">27702390</pub-id></mixed-citation>
              </ref>
              <ref id="c5">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradley</surname><given-names>B. P.</given-names></name>, <name><surname>Mogg</surname><given-names>K.</given-names></name>, &amp; <name><surname>Millar</surname><given-names>N. H.</given-names></name></person-group> (<year>2000</year>). <article-title>Covert and overt orienting of attention to emotional faces in anxiety</article-title>. <source>Cognition and Emotion</source>, <volume>14</volume>, <fpage>789</fpage>–<lpage>808</lpage>. <pub-id pub-id-type="doi">10.1080/02699930050156636</pub-id></mixed-citation>
              </ref>
              <ref id="c6">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bucks</surname><given-names>R. S.</given-names></name>, <name><surname>Garner</surname><given-names>M.</given-names></name>, <name><surname>Tarrant</surname><given-names>L.</given-names></name>, <name><surname>Bradley</surname><given-names>B. P.</given-names></name>, &amp; <name><surname>Mogg</surname><given-names>K.</given-names></name></person-group> (<year>2008</year>). <article-title>Interpretation of emotionally ambiguous faces in older adults</article-title>. <source>The Journals of Gerontology: Series B</source>, <volume>63</volume>, <fpage>P337</fpage>–<lpage>P343</lpage>. <pub-id pub-id-type="doi">10.1093/geronb/63.6.P337</pub-id></mixed-citation>
              </ref>
              <ref id="c7">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calder</surname><given-names>A. J.</given-names></name>, <name><surname>Keane</surname><given-names>J.</given-names></name>, <name><surname>Manly</surname><given-names>T.</given-names></name>, <name><surname>Sprengelmeyer</surname><given-names>R.</given-names></name>, <name><surname>Scott</surname><given-names>S.</given-names></name>, <name><surname>Nimmo-Smith</surname><given-names>I.</given-names></name>, &amp; <name><surname>Young</surname><given-names>A. W.</given-names></name></person-group> (<year>2003</year>). <article-title>Facial expression recognition across the adult life span</article-title>. <source>Neuropsychologia</source>, <volume>41</volume>, <fpage>195</fpage>–<lpage>202</lpage>. <pub-id pub-id-type="doi">10.1016/S0028-3932(02)00149-5</pub-id><pub-id pub-id-type="pmid">12459217</pub-id></mixed-citation>
              </ref>
              <ref id="c8">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carstensen</surname><given-names>L. L.</given-names></name>, <name><surname>Isaacowitz</surname><given-names>D. M.</given-names></name>, &amp; <name><surname>Charles</surname><given-names>S. T.</given-names></name></person-group> (<year>1999</year>). <article-title>Taking time seriously. A theory of socioemotional selectivity</article-title>. <source>American Psychologist</source>, <volume>54</volume>, <fpage>165</fpage>–<lpage>181</lpage>. <pub-id pub-id-type="doi">10.1037/0003-066X.54.3.165</pub-id><pub-id pub-id-type="pmid">10199217</pub-id></mixed-citation>
              </ref>
              <ref id="c9">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charles</surname><given-names>S. T.</given-names></name>, <name><surname>Mather</surname><given-names>M.</given-names></name>, &amp; <name><surname>Carstensen</surname><given-names>L. L.</given-names></name></person-group> (<year>2003</year>). <article-title>Aging and emotional memory: The forgettable nature of negative images for older adults</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>132</volume>, <fpage>310</fpage>–<lpage>324</lpage>. <pub-id pub-id-type="doi">10.1037/0096-3445.132.2.310</pub-id><pub-id pub-id-type="pmid">12825643</pub-id></mixed-citation>
              </ref>
              <ref id="c10">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowan</surname><given-names>N.</given-names></name>, <name><surname>Naveh-Benjamin</surname><given-names>M.</given-names></name>, <name><surname>Kilb</surname><given-names>A.</given-names></name>, &amp; <name><surname>Saults</surname><given-names>J. S.</given-names></name></person-group> (<year>2006</year>). <article-title>Life-span development of visual working memory: When is feature binding difficult?</article-title>
<source>Developmental Psychology</source>, <volume>42</volume>, <fpage>1089</fpage>–<lpage>1102</lpage>. <pub-id pub-id-type="doi">10.1037/0012-1649.42.6.1089</pub-id><pub-id pub-id-type="pmid">17087544</pub-id></mixed-citation>
              </ref>
              <ref id="c11">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>J. C.</given-names></name>, <name><surname>Marra</surname><given-names>C. A.</given-names></name>, <name><surname>Najafzadeh</surname><given-names>M.</given-names></name>, &amp; <name><surname>Liu-Ambrose</surname><given-names>T.</given-names></name></person-group> (<year>2010</year>). <article-title>The independent contribution of executive functions to health related quality of life in older women</article-title>. <source>BMC Geriatrics</source>, <volume>10</volume>, <elocation-id>16</elocation-id>
<pub-id pub-id-type="doi">10.1186/1471-2318-10-16</pub-id><pub-id pub-id-type="pmid">20359355</pub-id></mixed-citation>
              </ref>
              <ref id="c12">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>R.</given-names></name>, &amp; <name><surname>Duncan</surname><given-names>J.</given-names></name></person-group> (<year>1995</year>). <article-title>Neural mechanisms of selective visual attention</article-title>. <source>Annual Review of Neuroscience</source>, <volume>18</volume>, <fpage>193</fpage>–<lpage>222</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id></mixed-citation>
              </ref>
              <ref id="c13">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebner</surname><given-names>N. C.</given-names></name>, &amp; <name><surname>Johnson</surname><given-names>M. K.</given-names></name></person-group> (<year>2009</year>). <article-title>Young and older emotional faces: Are there age group differences in expression identification and memory?</article-title>
<source>Emotion</source>, <volume>9</volume>, <fpage>329</fpage>–<lpage>339</lpage>. <pub-id pub-id-type="doi">10.1037/a0015179</pub-id><pub-id pub-id-type="pmid">19485610</pub-id></mixed-citation>
              </ref>
              <ref id="c14">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>E.</given-names></name></person-group> (<year>2002</year>). <article-title>Processing emotional facial expressions: The role of anxiety and awareness</article-title>. <source>Cognitive, Affective &amp; Behavioral Neuroscience</source>, <volume>2</volume>, <fpage>52</fpage>–<lpage>63</lpage>. <pub-id pub-id-type="doi">10.3758/CABN.2.1.52</pub-id></mixed-citation>
              </ref>
              <ref id="c15">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>E.</given-names></name>, <name><surname>Cahill</surname><given-names>S.</given-names></name>, &amp; <name><surname>Zougkou</surname><given-names>K.</given-names></name></person-group> (<year>2010</year>). <article-title>Preconscious processing biases predict emotional reactivity to stress</article-title>. <source>Biological Psychiatry</source>, <volume>67</volume>, <fpage>371</fpage>–<lpage>377</lpage>. <pub-id pub-id-type="doi">10.1016/j.biopsych.2009.11.018</pub-id><pub-id pub-id-type="pmid">20113741</pub-id></mixed-citation>
              </ref>
              <ref id="c16">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>E.</given-names></name>, <name><surname>Mathews</surname><given-names>A.</given-names></name>, <name><surname>Calder</surname><given-names>A. J.</given-names></name>, &amp; <name><surname>Yiend</surname><given-names>J.</given-names></name></person-group> (<year>2007</year>). <article-title>Anxiety and sensitivity to gaze direction in emotionally expressive faces</article-title>. <source>Emotion</source>, <volume>7</volume>, <fpage>478</fpage>–<lpage>486</lpage>. <pub-id pub-id-type="doi">10.1037/1528-3542.7.3.478</pub-id><pub-id pub-id-type="pmid">17683204</pub-id></mixed-citation>
              </ref>
              <ref id="c17">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fung</surname><given-names>H. H.</given-names></name>, &amp; <name><surname>Carstensen</surname><given-names>L. L.</given-names></name></person-group> (<year>2003</year>). <article-title>Sending memorable messages to the old: Age differences in preferences and memory for advertisements</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>85</volume>, <fpage>163</fpage>–<lpage>178</lpage>. <pub-id pub-id-type="doi">10.1037/0022-3514.85.1.163</pub-id><pub-id pub-id-type="pmid">12872892</pub-id></mixed-citation>
              </ref>
              <ref id="c18">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilchrist</surname><given-names>A. L.</given-names></name>, <name><surname>Duarte</surname><given-names>A.</given-names></name>, &amp; <name><surname>Verhaeghen</surname><given-names>P.</given-names></name></person-group> (<year>2016</year>). <article-title>Retrospective cues based on object features improve visual working memory performance in older adults</article-title>. <source>Aging, Neuropsychology, and Cognition</source>, <volume>23</volume>, <fpage>184</fpage>–<lpage>195</lpage>. <pub-id pub-id-type="doi">10.1080/13825585.2015.1069253</pub-id></mixed-citation>
              </ref>
              <ref id="c19">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffin</surname><given-names>I. C.</given-names></name>, &amp; <name><surname>Nobre</surname><given-names>A. C.</given-names></name></person-group> (<year>2003</year>). <article-title>Orienting attention to locations in internal representations</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>15</volume>, <fpage>1176</fpage>–<lpage>1194</lpage>. <pub-id pub-id-type="doi">10.1162/089892903322598139</pub-id><pub-id pub-id-type="pmid">14709235</pub-id></mixed-citation>
              </ref>
              <ref id="c20">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartley</surname><given-names>A. A.</given-names></name>, <name><surname>Ravich</surname><given-names>Z.</given-names></name>, <name><surname>Stringer</surname><given-names>S.</given-names></name>, &amp; <name><surname>Wiley</surname><given-names>K.</given-names></name></person-group> (<year>2015</year>). <article-title>An age-related dissociation of short-term memory for facial identity and facial emotional expression</article-title>. <source>The Journals of Gerontology: Series B</source>, <volume>70</volume>, <fpage>718</fpage>–<lpage>728</lpage>. <pub-id pub-id-type="doi">10.1093/geronb/gbt127</pub-id></mixed-citation>
              </ref>
              <ref id="c21">
                <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hulley</surname><given-names>S. B.</given-names></name>, <name><surname>Cummings</surname><given-names>S. R.</given-names></name>, <name><surname>Browner</surname><given-names>W. S.</given-names></name>, <name><surname>Grady</surname><given-names>D. G.</given-names></name>, &amp; <name><surname>Newman</surname><given-names>T. B.</given-names></name></person-group> (<year>2013</year>). <source>Designing clinical research</source> (<edition>4th ed.</edition>). <publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>Lippincott Williams &amp; Wilkins</publisher-name>.</mixed-citation>
              </ref>
              <ref id="c22">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname><given-names>M. C.</given-names></name>, <name><surname>Wu</surname><given-names>C.-Y.</given-names></name>, <name><surname>Linden</surname><given-names>D. E. J.</given-names></name>, &amp; <name><surname>Raymond</surname><given-names>J. E.</given-names></name></person-group> (<year>2009</year>). <article-title>Enhanced visual short-term memory for angry faces</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>35</volume>, <fpage>363</fpage>–<lpage>374</lpage>. <pub-id pub-id-type="doi">10.1037/a0013895</pub-id><pub-id pub-id-type="pmid">19331494</pub-id></mixed-citation>
              </ref>
              <ref id="c23">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kellough</surname><given-names>J. L.</given-names></name>, &amp; <name><surname>Knight</surname><given-names>B. G.</given-names></name></person-group> (<year>2012</year>). <article-title>Positivity effects in older adults’ perception of facial emotion: The role of future time perspective</article-title>. <source>The Journals of Gerontology: Series B</source>, <volume>67B</volume>, <fpage>150</fpage>–<lpage>158</lpage>. <pub-id pub-id-type="doi">10.1093/geronb/gbr079</pub-id></mixed-citation>
              </ref>
              <ref id="c24">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kensinger</surname><given-names>E. A.</given-names></name>, <name><surname>Brierley</surname><given-names>B.</given-names></name>, <name><surname>Medford</surname><given-names>N.</given-names></name>, <name><surname>Growdon</surname><given-names>J. H.</given-names></name>, &amp; <name><surname>Corkin</surname><given-names>S.</given-names></name></person-group> (<year>2002</year>). <article-title>Effects of normal aging and Alzheimer’s disease on emotional memory</article-title>. <source>Emotion</source>, <volume>2</volume>, <fpage>118</fpage>–<lpage>134</lpage>. <pub-id pub-id-type="doi">10.1037/1528-3542.2.2.118</pub-id><pub-id pub-id-type="pmid">12899186</pub-id></mixed-citation>
              </ref>
              <ref id="c25">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landman</surname><given-names>R.</given-names></name>, <name><surname>Spekreijse</surname><given-names>H.</given-names></name>, &amp; <name><surname>Lamme</surname><given-names>V. A.</given-names></name></person-group> (<year>2003</year>). <article-title>Large capacity storage of integrated objects before change blindness</article-title>. <source>Vision Research</source>, <volume>43</volume>, <fpage>149</fpage>–<lpage>164</lpage>. <pub-id pub-id-type="doi">10.1016/S0042-6989(02)00402-9</pub-id><pub-id pub-id-type="pmid">12536137</pub-id></mixed-citation>
              </ref>
              <ref id="c26">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mammarella</surname><given-names>N.</given-names></name>, <name><surname>Borella</surname><given-names>E.</given-names></name>, <name><surname>Carretti</surname><given-names>B.</given-names></name>, <name><surname>Leonardi</surname><given-names>G.</given-names></name>, &amp; <name><surname>Fairfield</surname><given-names>B.</given-names></name></person-group> (<year>2013</year>). <article-title>Examining an emotion enhancement effect in working memory: Evidence from age-related differences</article-title>. <source>Neuropsychological Rehabilitation</source>, <volume>23</volume>, <fpage>416</fpage>–<lpage>428</lpage>. <pub-id pub-id-type="doi">10.1080/09602011.2013.775065</pub-id><pub-id pub-id-type="pmid">23452136</pub-id></mixed-citation>
              </ref>
              <ref id="c27">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mather</surname><given-names>M.</given-names></name>, &amp; <name><surname>Carstensen</surname><given-names>L. L.</given-names></name></person-group> (<year>2005</year>). <article-title>Aging and motivated cognition: The positivity effect in attention and memory</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>9</volume>, <fpage>496</fpage>–<lpage>502</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2005.08.005</pub-id><pub-id pub-id-type="pmid">16154382</pub-id></mixed-citation>
              </ref>
              <ref id="c28">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mather</surname><given-names>M.</given-names></name>, &amp; <name><surname>Knight</surname><given-names>M. R.</given-names></name></person-group> (<year>2006</year>). <article-title>Angry faces get noticed quickly: Threat detection is not impaired among older adults</article-title>. <source>The Journals of Gerontology: Series B</source>, <volume>61</volume>, <fpage>P54</fpage>–<lpage>P57</lpage>. <pub-id pub-id-type="doi">10.1093/geronb/61.1.P54</pub-id></mixed-citation>
              </ref>
              <ref id="c29">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mikels</surname><given-names>J. A.</given-names></name>, <name><surname>Larkin</surname><given-names>G. R.</given-names></name>, <name><surname>Reuter-Lorenz</surname><given-names>P. A.</given-names></name>, &amp; <name><surname>Carstensen</surname><given-names>L. L.</given-names></name></person-group> (<year>2005</year>). <article-title>Divergent trajectories in the aging mind: Changes in working memory for affective versus visual information with age</article-title>. <source>Psychology and Aging</source>, <volume>20</volume>, <fpage>542</fpage>–<lpage>553</lpage>. <pub-id pub-id-type="doi">10.1037/0882-7974.20.4.542</pub-id><pub-id pub-id-type="pmid">16420130</pub-id></mixed-citation>
              </ref>
              <ref id="c30">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mok</surname><given-names>R. M.</given-names></name>, <name><surname>Myers</surname><given-names>N. E.</given-names></name>, <name><surname>Wallis</surname><given-names>G.</given-names></name>, &amp; <name><surname>Nobre</surname><given-names>A. C.</given-names></name></person-group> (<year>2016</year>). <article-title>Behavioral and neural markers of flexible attention over working memory in aging</article-title>. <source>Cerebral Cortex</source>, <volume>26</volume>, <fpage>1831</fpage>–<lpage>1842</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhw011</pub-id><pub-id pub-id-type="pmid">26865653</pub-id></mixed-citation>
              </ref>
              <ref id="c31">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>N. A.</given-names></name>, &amp; <name><surname>Isaacowitz</surname><given-names>D. M.</given-names></name></person-group> (<year>2008</year>). <article-title>Preferences for emotional information in older and younger adults: A meta-analysis of memory and attention tasks</article-title>. <source>Psychology and Aging</source>, <volume>23</volume>, <fpage>263</fpage>–<lpage>286</lpage>. <pub-id pub-id-type="doi">10.1037/0882-7974.23.2.263</pub-id><pub-id pub-id-type="pmid">18573002</pub-id></mixed-citation>
              </ref>
              <ref id="c32">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasreddine</surname><given-names>Z. S.</given-names></name>, <name><surname>Phillips</surname><given-names>N. A.</given-names></name>, <name><surname>Bédirian</surname><given-names>V.</given-names></name>, <name><surname>Charbonneau</surname><given-names>S.</given-names></name>, <name><surname>Whitehead</surname><given-names>V.</given-names></name>, <name><surname>Collin</surname><given-names>I.</given-names></name>, <etal>. . .</etal><name><surname>Chertkow</surname><given-names>H.</given-names></name></person-group> (<year>2005</year>). <article-title>The Montreal Cognitive Assessment, MoCA: A brief screening tool for mild cognitive impairment</article-title>. <source>Journal of the American Geriatrics Society</source>, <volume>53</volume>, <fpage>695</fpage>–<lpage>699</lpage>. <pub-id pub-id-type="doi">10.1111/j.1532-5415.2005.53221.x</pub-id><pub-id pub-id-type="pmid">15817019</pub-id></mixed-citation>
              </ref>
              <ref id="c33">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newsome</surname><given-names>R. N.</given-names></name>, <name><surname>Duarte</surname><given-names>A.</given-names></name>, <name><surname>Pun</surname><given-names>C.</given-names></name>, <name><surname>Smith</surname><given-names>V. M.</given-names></name>, <name><surname>Ferber</surname><given-names>S.</given-names></name>, &amp; <name><surname>Barense</surname><given-names>M. D.</given-names></name></person-group> (<year>2015</year>). <article-title>A retroactive spatial cue improved VSTM capacity in mild cognitive impairment and medial temporal lobe amnesia but not in healthy older adults</article-title>. <source>Neuropsychologia</source>, <volume>77</volume>, <fpage>148</fpage>–<lpage>157</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.08.017</pub-id><pub-id pub-id-type="pmid">26300388</pub-id></mixed-citation>
              </ref>
              <ref id="c34">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Öhman</surname><given-names>A.</given-names></name>, <name><surname>Flykt</surname><given-names>A.</given-names></name>, &amp; <name><surname>Esteves</surname><given-names>F.</given-names></name></person-group> (<year>2001</year>). <article-title>Emotion drives attention: Detecting the snake in the grass</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>130</volume>, <fpage>466</fpage>–<lpage>478</lpage>. <pub-id pub-id-type="doi">10.1037/0096-3445.130.3.466</pub-id><pub-id pub-id-type="pmid">11561921</pub-id></mixed-citation>
              </ref>
              <ref id="c35">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phelps</surname><given-names>E. A.</given-names></name>, <name><surname>Ling</surname><given-names>S.</given-names></name>, &amp; <name><surname>Carrasco</surname><given-names>M.</given-names></name></person-group> (<year>2006</year>). <article-title>Emotion facilitates perception and potentiates the perceptual benefits of attention</article-title>. <source>Psychological Science</source>, <volume>17</volume>, <fpage>292</fpage>–<lpage>299</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9280.2006.01701.x</pub-id><pub-id pub-id-type="pmid">16623685</pub-id></mixed-citation>
              </ref>
              <ref id="c36">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>M. L.</given-names></name>, <name><surname>Young</surname><given-names>A. W.</given-names></name>, <name><surname>Scott</surname><given-names>S. K.</given-names></name>, <name><surname>Calder</surname><given-names>A. J.</given-names></name>, <name><surname>Andrew</surname><given-names>C.</given-names></name>, <name><surname>Giampietro</surname><given-names>V.</given-names></name>, <etal>. . .</etal><name><surname>Gray</surname><given-names>J. A.</given-names></name></person-group> (<year>1998</year>). <article-title>Neural responses to facial and vocal expressions of fear and disgust</article-title>. <source>Proceedings of the Royal Society of London B: Biological Sciences</source>, <volume>265</volume>, <fpage>1809</fpage>–<lpage>1817</lpage>. <pub-id pub-id-type="doi">10.1098/rspb.1998.0506</pub-id></mixed-citation>
              </ref>
              <ref id="c37">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pourtois</surname><given-names>G.</given-names></name>, <name><surname>Grandjean</surname><given-names>D.</given-names></name>, <name><surname>Sander</surname><given-names>D.</given-names></name>, &amp; <name><surname>Vuilleumier</surname><given-names>P.</given-names></name></person-group> (<year>2004</year>). <article-title>Electrophysiological correlates of rapid spatial orienting towards fearful faces</article-title>. <source>Cerebral Cortex</source>, <volume>14</volume>, <fpage>619</fpage>–<lpage>633</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhh023</pub-id><pub-id pub-id-type="pmid">15054077</pub-id></mixed-citation>
              </ref>
              <ref id="c38">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reed</surname><given-names>A. E.</given-names></name>, <name><surname>Chan</surname><given-names>L.</given-names></name>, &amp; <name><surname>Mikels</surname><given-names>J. A.</given-names></name></person-group> (<year>2014</year>). <article-title>Meta-analysis of the age-related positivity effect: Age differences in preferences for positive over negative information</article-title>. <source>Psychology and Aging</source>, <volume>29</volume>, <fpage>1</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1037/a0035194</pub-id><pub-id pub-id-type="pmid">24660792</pub-id></mixed-citation>
              </ref>
              <ref id="c39">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rösler</surname><given-names>A.</given-names></name>, <name><surname>Ulrich</surname><given-names>C.</given-names></name>, <name><surname>Billino</surname><given-names>J.</given-names></name>, <name><surname>Sterzer</surname><given-names>P.</given-names></name>, <name><surname>Weidauer</surname><given-names>S.</given-names></name>, <name><surname>Bernhardt</surname><given-names>T.</given-names></name>, <etal>. . .</etal><name><surname>Kleinschmidt</surname><given-names>A.</given-names></name></person-group> (<year>2005</year>). <article-title>Effects of arousing emotional scenes on the distribution of visuospatial attention: Changes with aging and early subcortical vascular dementia</article-title>. <source>Journal of the Neurological Sciences</source>, <volume>229–230</volume>, <fpage>109</fpage>–<lpage>116</lpage>. <pub-id pub-id-type="doi">10.1016/j.jns.2004.11.007</pub-id></mixed-citation>
              </ref>
              <ref id="c40">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruffman</surname><given-names>T.</given-names></name>, <name><surname>Henry</surname><given-names>J. D.</given-names></name>, <name><surname>Livingstone</surname><given-names>V.</given-names></name>, &amp; <name><surname>Phillips</surname><given-names>L. H.</given-names></name></person-group> (<year>2008</year>). <article-title>A meta-analytic review of emotion recognition and aging: Implications for neuropsychological models of aging</article-title>. <source>Neuroscience and Biobehavioral Reviews</source>, <volume>32</volume>, <fpage>863</fpage>–<lpage>881</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2008.01.001</pub-id><pub-id pub-id-type="pmid">18276008</pub-id></mixed-citation>
              </ref>
              <ref id="c41">
                <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Singmann</surname><given-names>H.</given-names></name>, <name><surname>Bolker</surname><given-names>B.</given-names></name>, &amp; <name><surname>Westfall</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <source>afex: Analysis of factorial experiments. R Package Version 0.15–2</source> [<comment>Computer software</comment>]. <publisher-loc>Vienna, Austria</publisher-loc>: <publisher-name>R Foundation</publisher-name>.</mixed-citation>
              </ref>
              <ref id="c42">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Souza</surname><given-names>A. S.</given-names></name></person-group> (<year>2016</year>). <article-title>No age deficits in the ability to use attention to improve visual working memory</article-title>. <source>Psychology and Aging</source>, <volume>31</volume>, <fpage>456</fpage>–<lpage>470</lpage>. <pub-id pub-id-type="doi">10.1037/pag0000107</pub-id><pub-id pub-id-type="pmid">27253868</pub-id></mixed-citation>
              </ref>
              <ref id="c43">
                <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Spielberger</surname><given-names>C.</given-names></name></person-group> (<year>1983</year>). <source>Manual for the State–Trait Anxiety Inventory (STAI)</source>. <publisher-loc>Palo Alto, CA</publisher-loc>: <publisher-name>Consulting Psychologists Press</publisher-name>.</mixed-citation>
              </ref>
              <ref id="c44">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Truong</surname><given-names>L.</given-names></name>, &amp; <name><surname>Yang</surname><given-names>L.</given-names></name></person-group> (<year>2014</year>). <article-title>Friend or foe? Decoding the facilitative and disruptive effects of emotion on working memory in younger and older adults</article-title>. <source>Frontiers in Psychology</source>, <volume>5</volume>, <elocation-id>94</elocation-id>
<pub-id pub-id-type="doi">10.3389/fpsyg.2014.00094</pub-id><pub-id pub-id-type="pmid">24624097</pub-id></mixed-citation>
              </ref>
              <ref id="c45">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>D.</given-names></name>, <name><surname>Clark</surname><given-names>L. A.</given-names></name>, &amp; <name><surname>Tellegen</surname><given-names>A.</given-names></name></person-group> (<year>1988</year>). <article-title>Development and validation of brief measures of positive and negative affect: The PANAS scales</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>54</volume>, <fpage>1063</fpage>–<lpage>1070</lpage>. <pub-id pub-id-type="doi">10.1037/0022-3514.54.6.1063</pub-id><pub-id pub-id-type="pmid">3397865</pub-id></mixed-citation>
              </ref>
              <ref id="c51">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>World Medical Association Declaration of Helsinki</collab></person-group> (<year>2013</year>). <article-title>Ethical principles for medical research involving human subjects</article-title>. <source>Journal of the American Medical Association</source>, <volume>310</volume>, <fpage>2191</fpage>–<lpage>2194</lpage>. <pub-id pub-id-type="doi">10.1001/jama.2013.281053</pub-id><pub-id pub-id-type="pmid">24141714</pub-id></mixed-citation>
              </ref>
              <ref id="c46">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yiend</surname><given-names>J.</given-names></name></person-group> (<year>2010</year>). <article-title>The effects of emotion on attention: A review of attentional processing of emotional information</article-title>. <source>Cognition and Emotion</source>, <volume>24</volume>, <fpage>3</fpage>–<lpage>47</lpage>. <pub-id pub-id-type="doi">10.1080/02699930903205698</pub-id></mixed-citation>
              </ref>
              <ref id="c47">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W.</given-names></name>, &amp; <name><surname>Luck</surname><given-names>S. J.</given-names></name></person-group> (<year>2008</year>). <article-title>Discrete fixed-resolution representations in visual working memory</article-title>. <source>Nature</source>, <volume>453</volume>, <fpage>233</fpage>–<lpage>235</lpage>. <pub-id pub-id-type="doi">10.1038/nature06860</pub-id><pub-id pub-id-type="pmid">18385672</pub-id></mixed-citation>
              </ref>
              <ref id="c48">
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zokaei</surname><given-names>N.</given-names></name>, <name><surname>Burnett Heyes</surname><given-names>S.</given-names></name>, <name><surname>Gorgoraptis</surname><given-names>N.</given-names></name>, <name><surname>Budhdeo</surname><given-names>S.</given-names></name>, &amp; <name><surname>Husain</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>Working memory recall precision is a more sensitive index than span</article-title>. <source>Journal of Neuropsychology</source>, <volume>9</volume>, <fpage>319</fpage>–<lpage>329</lpage>. <pub-id pub-id-type="doi">10.1111/jnp.12052</pub-id><pub-id pub-id-type="pmid">25208525</pub-id></mixed-citation>
              </ref>
            </ref-list>
          </back>
          <floats-group>
            <fig id="fig1" position="float">
              <label>Figure 1</label>
              <caption>
                <p>WM task schematic and WM error results. In the WM task (a), participants encoded a facial expression into WM, and maintained it over a delay of 3,000 ms. A test face with the same facial identity but a neutral facial expression (0% intensity) appeared, and participants changed the face to match the expression intensity in memory using a trackball mouse. Target faces were fearful or happy faces from 0% to 100% in emotional intensity. Emotion type was intermixed within blocks. Bar plots in (b) show WM error for fearful (red, left) and happy faces (blue, right) in the young and old participant groups. Error bars represent <italic>SEM</italic>. *** <italic>p</italic> &lt; .001, * <italic>p</italic> &lt; .05. Faces presented are part of the NimStim stimulus set, for which use for publication is permitted.</p>
              </caption>
              <graphic id="fig1a" xlink:href="emo_19_6_1060_fig1a"/>
            </fig>
            <fig id="fig2" position="float">
              <label>Figure 2</label>
              <caption>
                <p>Emotional bias in WM. Responses are plotted as a function of the target-face emotion type and emotional intensity in (a), with negative values representing intensity values of fearful faces and positive values representing intensity values of happy faces. Responses were binned into five equal bins for fearful faces (from −100 to −20% in 20% steps, with the 20% bin including −20 to −1%) and five bins for happy faces (from 20% to 100% in 20% steps) and a 0% bin with only neutral faces for visualization. Perfect performance corresponds to responses on the diagonal (dotted line). On the right side of zero (<italic>y</italic> axis), responses above the line mean that faces were reported to be happier than happy-face targets, whereas responses below the line mean that faces were reported to be less happy than targets. On the left side of zero, responses below the line mean that faces were reported to be more fearful than fearful-face targets, whereas responses above the line mean that faces were reported as less than fearful-face targets. The bias is shown in (b), computed by taking the mean of each participant’s raw psychometric curve—note that (a) is binned for visualization. Bias for each of the emotion types is plotted in (c). Responses for fearful faces were flipped to have a positive sign, and trials with neutral faces were excluded. Mean response was computed for each emotion type (from 1% to 100%) and normalized by subtracting 50 (see Data Analysis for details). *** <italic>p</italic> &lt; .001.</p>
              </caption>
              <graphic id="fig2a" xlink:href="emo_19_6_1060_fig2a"/>
            </fig>
            <fig id="fig3" position="float">
              <label>Figure 3</label>
              <caption>
                <p>Older adults interpreted fearful faces with low emotional intensities as happy more than younger adults. Proportion of trials correctly judged as fearful in the WM task is plotted for each emotional intensity bin from 1% to 80% in 20% steps for younger and older participants in the top panel in (a). An illustration showing how low-to-medium fearful faces are sometimes judged as happy faces in the bottom panel of (a). Proportion of trials correctly judged as happy in the WM task are plotted for each emotional intensity bin in the top panel of (b) for younger and older participants, with an illustration in the bottom panel showing how low-to-medium happy faces are sometimes judged as fearful faces. Faces presented are part of the NimStim stimulus set, for which use for publication is permitted.</p>
              </caption>
              <graphic id="fig3a" xlink:href="emo_19_6_1060_fig3a"/>
            </fig>
          </floats-group>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
