<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2023-02-07T13:03:21Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:7293620" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:7293620</identifier>
        <datestamp>2020-06-18</datestamp>
        <setSpec>pnas</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Proc Natl Acad Sci U S A</journal-id>
              <journal-id journal-id-type="iso-abbrev">Proc. Natl. Acad. Sci. U.S.A</journal-id>
              <journal-id journal-id-type="hwp">pnas</journal-id>
              <journal-id journal-id-type="pmc">pnas</journal-id>
              <journal-id journal-id-type="publisher-id">PNAS</journal-id>
              <journal-title-group>
                <journal-title>Proceedings of the National Academy of Sciences of the United States of America</journal-title>
              </journal-title-group>
              <issn pub-type="ppub">0027-8424</issn>
              <issn pub-type="epub">1091-6490</issn>
              <publisher>
                <publisher-name>National Academy of Sciences</publisher-name>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC7293620</article-id>
              <article-id pub-id-type="pmcid">PMC7293620</article-id>
              <article-id pub-id-type="pmc-uid">7293620</article-id>
              <article-id pub-id-type="pmid">32457153</article-id>
              <article-id pub-id-type="pmid">32457153</article-id>
              <article-id pub-id-type="publisher-id">202003110</article-id>
              <article-id pub-id-type="doi">10.1073/pnas.2003110117</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Biological Sciences</subject>
                  <subj-group>
                    <subject>Psychological and Cognitive Sciences</subject>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Other people’s gaze encoded as implied motion in the human brain</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-3694-1318</contrib-id>
                  <name>
                    <surname>Guterstam</surname>
                    <given-names>Arvid</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-4080-5386</contrib-id>
                  <name>
                    <surname>Wilterson</surname>
                    <given-names>Andrew I.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-8369-3919</contrib-id>
                  <name>
                    <surname>Wachtell</surname>
                    <given-names>Davis</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7727-3475</contrib-id>
                  <name>
                    <surname>Graziano</surname>
                    <given-names>Michael S. A.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                </contrib>
                <aff id="aff1"><sup>a</sup>Department of Psychology, <institution>Princeton University</institution>, Princeton, <addr-line>NJ</addr-line> 08544</aff>
              </contrib-group>
              <author-notes>
                <corresp id="cor1"><sup>1</sup>To whom correspondence may be addressed. Email: <email>arvidg@princeton.edu</email>.</corresp>
                <fn fn-type="edited-by">
                  <p>Edited by Ranulfo Romo, National Autonomous University of Mexico, Mexico City, D.F., Mexico, and approved April 23, 2020 (received for review February 18, 2020)</p>
                </fn>
                <fn fn-type="con">
                  <p>Author contributions: A.G. and M.S.A.G. designed research; A.G., A.I.W., and D.W. performed research; A.G. and A.I.W. analyzed data; and A.G., A.I.W., D.W., and M.S.A.G. wrote the paper.</p>
                </fn>
              </author-notes>
              <pub-date pub-type="ppub">
                <day>9</day>
                <month>6</month>
                <year>2020</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>26</day>
                <month>5</month>
                <year>2020</year>
              </pub-date>
              <pub-date pub-type="pmc-release">
                <day>26</day>
                <month>5</month>
                <year>2020</year>
              </pub-date>
              <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
              <volume>117</volume>
              <issue>23</issue>
              <fpage>13162</fpage>
              <lpage>13167</lpage>
              <permissions>
                <copyright-statement>Copyright © 2020 the Author(s). Published by PNAS.</copyright-statement>
                <copyright-year>2020</copyright-year>
                <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">
                  <ali:license_ref specific-use="vor">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
                  <license-p>This open access article is distributed under <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND)</ext-link>.</license-p>
                </license>
              </permissions>
              <self-uri xlink:title="pdf" xlink:href="pnas.202003110.pdf"/>
              <abstract abstract-type="executive-summary">
                <title>Significance</title>
                <p>This study used fMRI scans of people to show that the brain processes the gaze of others, and visual flow, in a similar manner. Visual motion brain areas and social cognition areas were involved. It is as if the brain draws a quick visual sketch with moving arrows to help keep track of who is attending to what. We propose that this implicit, fluid-flow model of other people’s gaze may help explain culturally universal myths about the mind as an energy-like, flowing essence.</p>
              </abstract>
              <abstract>
                <p>Keeping track of other people’s gaze is an essential task in social cognition and key for successfully reading other people’s intentions and beliefs (theory of mind). Recent behavioral evidence suggests that we construct an implicit model of other people’s gaze, which may incorporate physically incoherent attributes such as a construct of force-carrying beams that emanate from the eyes. Here, we used functional magnetic resonance imaging and multivoxel pattern analysis to test the prediction that the brain encodes gaze as implied motion streaming from an agent toward a gazed-upon object. We found that a classifier, trained to discriminate the direction of visual motion, significantly decoded the gaze direction in static images depicting a sighted face, but not a blindfolded one, from brain activity patterns in the human motion-sensitive middle temporal complex (MT+) and temporo-parietal junction (TPJ). Our results demonstrate a link between the visual motion system and social brain mechanisms, in which the TPJ, a key node in theory of mind, works in concert with MT+ to encode gaze as implied motion. This model may be a fundamental aspect of social cognition that allows us to efficiently connect agents with the objects of their attention. It is as if the brain draws a quick visual sketch with moving arrows to help keep track of who is attending to what. This implicit, fluid-flow model of other people’s gaze may help explain culturally universal myths about the mind as an energy-like, flowing essence.</p>
              </abstract>
              <kwd-group>
                <kwd>gaze</kwd>
                <kwd>social cognition</kwd>
                <kwd>motion perception</kwd>
                <kwd>visual attention</kwd>
                <kwd>theory of mind</kwd>
              </kwd-group>
              <funding-group>
                <award-group id="gs1">
                  <funding-source id="sp1">Princeton University (Princeton)<named-content content-type="funder-id">100006734</named-content></funding-source>
                  <award-id rid="sp1">-</award-id>
                  <principal-award-recipient>Arvid Guterstam</principal-award-recipient>
                  <principal-award-recipient>Andrew I Wilterson</principal-award-recipient>
                  <principal-award-recipient>Davis Wachtell</principal-award-recipient>
                  <principal-award-recipient>Michael S.A. Graziano</principal-award-recipient>
                </award-group>
                <award-group id="gs2">
                  <funding-source id="sp2">Wenner-Gren Foundation<named-content content-type="funder-id">100001388</named-content></funding-source>
                  <award-id rid="sp2">-</award-id>
                  <principal-award-recipient>Arvid Guterstam</principal-award-recipient>
                </award-group>
                <award-group id="gs3">
                  <funding-source id="sp3">Swedish Society of Medicine</funding-source>
                  <award-id rid="sp3">-</award-id>
                  <principal-award-recipient>Arvid Guterstam</principal-award-recipient>
                </award-group>
                <award-group id="gs4">
                  <funding-source id="sp4">Stiftelsen Blanceflor Boncompagni Ludovisi, född Bildt (Foundation Blanceflor Boncompagni Ludovisi, née Bildt)<named-content content-type="funder-id">501100006358</named-content></funding-source>
                  <award-id rid="sp4">-</award-id>
                  <principal-award-recipient>Arvid Guterstam</principal-award-recipient>
                </award-group>
              </funding-group>
              <counts>
                <page-count count="6"/>
              </counts>
            </article-meta>
          </front>
          <body>
            <p content-type="flushleft">Recent behavioral studies suggest that the brain, beyond registering low-level visual cues about the direction of other people’s gaze (<xref rid="r1" ref-type="bibr">1</xref><xref rid="r2" ref-type="bibr"/>–<xref rid="r3" ref-type="bibr">3</xref>), constructs a model of other people’s active visual attention (<xref rid="r4" ref-type="bibr">4</xref><xref rid="r5" ref-type="bibr"/><xref rid="r6" ref-type="bibr"/>–<xref rid="r7" ref-type="bibr">7</xref>). This model may be simplified and schematic, involving the attribution of beams that emanate from the eyes toward the object of attention (<xref rid="r5" ref-type="bibr">5</xref>). The model is constructed at an implicit level—people are generally not aware they are doing it. The adaptive benefit for the brain to model other people’s attentive gaze in such a schematic manner may be related to computational efficiency in processing complex social stimuli with multiple sources and targets of visual attention (<xref rid="r4" ref-type="bibr">4</xref><xref rid="r5" ref-type="bibr"/>–<xref rid="r6" ref-type="bibr">6</xref>, <xref rid="r8" ref-type="bibr">8</xref>). In the present experiment, we used functional magnetic resonance imaging (fMRI) to study the brain regions involved when people process actual motion and when people process the gaze of others. We hypothesized that processing gaze partly engages brain regions that process motion in a direction-specific manner. To test our hypothesis, we used multivoxel pattern analysis to decode the gaze direction in static images of faces looking at objects, with a classifier trained on discriminating the direction of actual visual motion. In addition to a whole-brain search, we focused on two regions of interest (ROIs): the motion-sensitive middle temporal complex (MT+) and the temporo-parietal junction (TPJ). The MT+ is a subregion of the extrastriate visual cortex specialized for visual motion perception (<xref rid="r9" ref-type="bibr">9</xref>, <xref rid="r10" ref-type="bibr">10</xref>). The TPJ is consistently activated in tasks requiring theory of mind (<xref rid="r11" ref-type="bibr">11</xref>, <xref rid="r12" ref-type="bibr">12</xref>). Moreover, in at least some experiments, modeling the active attention of others was associated with activity in the TPJ (<xref rid="r7" ref-type="bibr">7</xref>, <xref rid="r13" ref-type="bibr">13</xref>). The TPJ also overlaps the caudal superior temporal sulcus (STS), which, in both humans and monkeys, is active in association with processing visual cues about the gaze direction of others (<xref rid="r14" ref-type="bibr">14</xref><xref rid="r15" ref-type="bibr"/><xref rid="r16" ref-type="bibr"/><xref rid="r17" ref-type="bibr"/>–<xref rid="r18" ref-type="bibr">18</xref>). We therefore hypothesized that these two regions in particular, the TPJ and MT+, would be involved in the present experiment.</p>
            <sec sec-type="results" id="s1">
              <title>Results</title>
              <p content-type="flushleft">In an fMRI experiment involving 32 healthy human subjects (18 females; mean age, 26 y; range 18 to 52 y; see <xref ref-type="sec" rid="s3"><italic>Materials and Methods</italic></xref> for details), we used a slow, event-related design to estimate the brain activity associated with either viewing actual visual motion or viewing another person gazing at an object. As shown in <xref ref-type="fig" rid="fig01">Fig. 1<italic>A</italic></xref>, the visual motion stimulus consisted of a square-shaped (5° × 5°) random dot motion display (<xref rid="r19" ref-type="bibr">19</xref>) where 100% of the dots moved coherently in the same direction, creating a strong sense of either leftward (dot motion left condition) or rightward motion (dot motion right condition). In the gaze trials, subjects observed a static image of a cartoon face on one side of the screen, gazing at an object (a tree) on the other side of the screen. The face and the tree were spatially aligned such that the empty space in between, where the hypothesized implied motion should occur, corresponded to the size and location of the random dot motion field (5° wide). The face appeared either on the right side gazing leftward (eyes open left condition), or on the left side gazing rightward (eyes open right condition). In two control conditions (eyes covered left and eyes covered right), the eyes of the face were covered by a blindfold, keeping all other aspects of the stimulus identical. We reasoned that the blindfold should prevent any gaze-induced implied motion in the experiment.</p>
              <fig fig-type="featured" id="fig01" orientation="portrait" position="float">
                <label>Fig. 1.</label>
                <caption>
                  <p>Methods. (<italic>A</italic>) Schematic time line of the fMRI design. While subjects continuously fixated on a central spot, they were exposed to 1.5-s-long trials of either a random dot motion stimulus (going left or right), or a static image of a face gazing at a tree (facing left or right), or an image of a blindfolded face (facing left or right). In a catch trial condition, one of the image elements (head or tree) or the moving dots appeared bright green, in response to which the subjects pressed a button. Arrows shown here indicate dot motion directions and were not part of the actual stimuli. There were equal numbers of rightward and leftward facing face-and-tree trials, but only rightward facing images are shown here. (<italic>B</italic>) To test our hypothesis that gaze is encoded as implied motion in motion-sensitive and social brain areas, we used a locally multivariate (Searchlight), leave-one-run-out, cross-classification approach, using the runwise regression (beta) coefficients as model input. We trained a classifier to discriminate the BOLD activity patterns associated with visual motion going left versus right in 19 runs and then tested whether it could decode activity patterns associated with gaze direction (eyes open facing left versus right) significantly better than in the blindfolded condition (eyes covered facing left versus right) in the left-out twentieth run, repeated for all runs.</p>
                </caption>
                <graphic id="gra1" xlink:href="pnas.2003110117fig01"/>
              </fig>
              <p>To make sure that subjects followed instructions and paid attention to the visual stimuli, we also included catch trials in which either the moving dots, the face, or the tree appeared bright green, in response to which subjects pressed a button using their right index finger. Subjects detected these targets on a mean of 98% of the catch trials. All seven trial types were of 1.5 s duration and presented in a randomized order with a jittered intertrial interval of 6.0 to 10.0 s, divided into 20 runs featuring three repetitions per condition per run. To prevent systematic differences in eye movements across conditions, subjects were instructed to fixate on a central fixation spot throughout the experiment, and their eye movements were recorded using an MRI-compatible infrared eye tracker (<xref ref-type="sec" rid="s3"><italic>Materials and Methods</italic></xref>). The results of the eye tracking showed that subjects stayed on fixation on average 97% of the time and that fixation and saccade data alone were not sufficient to successfully decode the experimental conditions of interest (see <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2003110117/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S1</ext-link> for details), suggesting that differences in eye movement dynamics can be excluded as a confound in any decoding results based on the fMRI data.</p>
              <p>To test our hypothesis that other people’s gaze is encoded as implied motion in MT+ and TPJ, or any other brain region outside our ROIs, we used a whole-brain, locally multivariate (Searchlight) (<xref rid="r20" ref-type="bibr">20</xref>), leave-one-run-out, cross-classification approach. We first employed a conventional general linear model (GLM) to estimate regression (beta) coefficients for the six main conditions in each run (one additional regressor of no interest modeled all of the catch trials) and then submitted these runwise beta coefficients to multivariate analyses (<xref rid="r21" ref-type="bibr">21</xref>, <xref rid="r22" ref-type="bibr">22</xref>). We trained a support vector machine (SVM) classifier to discriminate dot motion left versus dot motion right in 19 of 20 runs and then tested whether it could decode gaze direction (i.e., eyes open left versus eyes open right) in the left-out 20th run (so-called cross-classification) (<xref ref-type="fig" rid="fig01">Fig. 1<italic>B</italic></xref>), based on the blood oxygen level-dependent (BOLD) response pattern within 12-mm-radius Searchlight spheres centered on each voxel in the entire brain. This procedure was repeated 20 times so that each run was left-out once, and a run-average decoding accuracy was calculated. The same classifier was also tested on the blindfolded control condition (eyes covered left versus eyes covered right). The key analysis was the comparison in cross-classification decoding accuracy in the eyes open versus eyes covered conditions because any area revealed by this contrast must contain brain activity patterns that are driven by low-level visual motion and specifically decode gaze direction, but only when the eyes of the face have unobscured vision. To make sure this contrast did not identify any area in which the performance of the classifier in the eyes open conditions was below the level of chance (50%), we only searched for voxels in which eyes open left versus eyes open right was decoded significantly (<italic>P</italic> &lt; 0.05, uncorrected) better than chance. To define our ROIs, we delineated the MT+ bilaterally using a visual motion localizer that we ran on each subject, based on previously published localizer tasks (<xref rid="r23" ref-type="bibr">23</xref><xref rid="r24" ref-type="bibr"/>–<xref rid="r25" ref-type="bibr">25</xref>), consisting of two 5-min runs of viewing moving or static dots (see <xref ref-type="sec" rid="s3"><italic>Materials and Methods</italic></xref> and <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2003110117/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S2</ext-link> for details). The TPJ ROIs were defined as 10-mm-radius spheres centered on the left and right TPJ activation peaks in a previous landmark fMRI study using a theory of mind task (<xref rid="r11" ref-type="bibr">11</xref>). In our analysis, we corrected for multiple comparisons both within our ROIs (small-volume corrections), as well as at the whole-brain level, to reveal any potential significant decoding activity in the rest of the brain.</p>
              <p><xref ref-type="fig" rid="fig02">Fig. 2</xref> and <xref rid="t01" ref-type="table">Table 1</xref> show the results. A classifier, trained on discriminating the direction of low-level visual motion, could decode the gaze direction in the eyes open condition significantly better than in the eyes covered condition in the right MT+ (decoding accuracy difference: 53.5% versus 49.7%, t = 3.64, <italic>P</italic> = 0.027, small-volume corrected), right posterior STS (belonging to the TPJ) (decoding accuracy difference: 55.0% versus 47.6%, t = 5.82, <italic>P</italic> &lt; 0.001, small-volume corrected), and left angular gyrus (within the TPJ) (decoding accuracy difference: 52.4% versus 48.5%, t = 4.03, <italic>P</italic> = 0.019, small-volume corrected). No significant voxels were found in the MT+ ROI in the left hemisphere. As shown in <xref rid="t01" ref-type="table">Table 1</xref>, by far the largest cluster of voxels and strongest decoding peak in the brain was found within the predefined ROI in the right TPJ, which also survived correction for multiple comparisons at the whole-brain level (<italic>P</italic> = 0.040). These findings suggest that a set of areas involving the MT+ and TPJ, primarily on the right side, encode the gaze of social agents as implied motion flowing from the eyes across the empty space to the gazed-at object. In addition to the peaks found within the ROIs, we also found decoding peaks (<italic>P</italic> &lt; 0.001, uncorrected), albeit not whole-brain significant after correcting for multiple comparisons, in the right fusiform gyrus, left midinsula, left putamen, and the right ventral striatum (<xref rid="t01" ref-type="table">Table 1</xref> and <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2003110117/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S3</ext-link>).</p>
              <fig fig-type="figure" id="fig02" orientation="portrait" position="float">
                <label>Fig. 2.</label>
                <caption>
                  <p>Results. Brain areas in which a classifier, trained on discriminating the direction of dot motion, significantly decoded the direction of gaze in static images of a face looking at an object (eyes open), using a blindfolded face as control (eyes covered). These results suggest that gaze is encoded as implied motion, in a specific direction, in the motion-sensitive middle temporal cortical complex (MT+, outlined in red) on the right side (<italic>A</italic>), and in the temporo-parietal junction (TPJ, red circles) bilaterally (<italic>B</italic> and <italic>C</italic>). Errors bars show SE, significance shown by *<italic>P</italic> &lt; 0.05 and ***<italic>P</italic> &lt; 0.001, corrected for multiple comparisons. See text for statistical details. The decoding maps are thresholded at <italic>P</italic> &lt; 0.001 (uncorrected), for visualization purposes. pSTS, posterior superior temporal sulcus.</p>
                </caption>
                <graphic id="gra2" xlink:href="pnas.2003110117fig02"/>
              </fig>
              <table-wrap id="t01" orientation="portrait" position="float">
                <label>Table 1.</label>
                <caption>
                  <p>Decoding results</p>
                </caption>
                <table frame="hsides" rules="groups">
                  <thead>
                    <tr>
                      <td rowspan="1" colspan="1">Anatomical region</td>
                      <td align="center" rowspan="1" colspan="1">MNI <italic>x</italic>, <italic>y</italic>, <italic>z</italic></td>
                      <td align="center" rowspan="1" colspan="1">Peak T</td>
                      <td align="center" rowspan="1" colspan="1"><italic>P</italic> value (FWE-corr)</td>
                      <td align="center" rowspan="1" colspan="1">Cluster size</td>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td rowspan="1" colspan="1">Temporal lobe</td>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1"> R. posterior STS (TPJ)</td>
                      <td align="center" rowspan="1" colspan="1">56, −54, 24</td>
                      <td align="center" rowspan="1" colspan="1">5.82</td>
                      <td align="center" rowspan="1" colspan="1">&lt;0.001</td>
                      <td align="center" rowspan="1" colspan="1">201</td>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1"> R. parieto-temporo-occipital cortex (MT+)</td>
                      <td align="center" rowspan="1" colspan="1">46, −70, 0</td>
                      <td align="center" rowspan="1" colspan="1">3.64</td>
                      <td align="center" rowspan="1" colspan="1">0.027</td>
                      <td align="center" rowspan="1" colspan="1">4</td>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1"> R. fusiform gyrus</td>
                      <td align="center" rowspan="1" colspan="1">22, −46, −16</td>
                      <td align="center" rowspan="1" colspan="1">3.62</td>
                      <td align="center" rowspan="1" colspan="1">—</td>
                      <td align="center" rowspan="1" colspan="1">11</td>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1"> R. superior temporal gyrus</td>
                      <td align="center" rowspan="1" colspan="1">72, −28, 6</td>
                      <td align="center" rowspan="1" colspan="1">3.71</td>
                      <td align="center" rowspan="1" colspan="1">—</td>
                      <td align="center" rowspan="1" colspan="1">5</td>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1">Parietal lobe</td>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1"> R. supramarginal gyrus</td>
                      <td align="center" rowspan="1" colspan="1">56, −40, 36</td>
                      <td align="center" rowspan="1" colspan="1">4.18</td>
                      <td align="center" rowspan="1" colspan="1">—</td>
                      <td align="center" rowspan="1" colspan="1">14</td>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1"> R. supramarginal gyrus</td>
                      <td align="center" rowspan="1" colspan="1">58, −38, 44</td>
                      <td align="center" rowspan="1" colspan="1">3.64</td>
                      <td align="center" rowspan="1" colspan="1">—</td>
                      <td align="center" rowspan="1" colspan="1">4</td>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1"> L. angular gyrus (TPJ)</td>
                      <td align="center" rowspan="1" colspan="1">−58, −62, 26</td>
                      <td align="center" rowspan="1" colspan="1">4.03</td>
                      <td align="center" rowspan="1" colspan="1">0.019</td>
                      <td align="center" rowspan="1" colspan="1">10</td>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1">Frontal lobe</td>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1"> R. precentral gyrus (premotor cortex)</td>
                      <td align="center" rowspan="1" colspan="1">58, −4, 42</td>
                      <td align="center" rowspan="1" colspan="1">4.35</td>
                      <td align="center" rowspan="1" colspan="1">—</td>
                      <td align="center" rowspan="1" colspan="1">32</td>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1"> L. inferior frontal gyrus</td>
                      <td align="center" rowspan="1" colspan="1">−46, 10, 20</td>
                      <td align="center" rowspan="1" colspan="1">3.62</td>
                      <td align="center" rowspan="1" colspan="1">—</td>
                      <td align="center" rowspan="1" colspan="1">7</td>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1">Insular cortex</td>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1"> L. midinsula</td>
                      <td align="center" rowspan="1" colspan="1">−36, −2, 16</td>
                      <td align="center" rowspan="1" colspan="1">4.45</td>
                      <td align="center" rowspan="1" colspan="1">—</td>
                      <td align="center" rowspan="1" colspan="1">20</td>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1">Subcortical structures</td>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                      <td rowspan="1" colspan="1"/>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1"> R. ventral striatum</td>
                      <td align="center" rowspan="1" colspan="1">10, 4, −10</td>
                      <td align="center" rowspan="1" colspan="1">4.79</td>
                      <td align="center" rowspan="1" colspan="1">—</td>
                      <td align="center" rowspan="1" colspan="1">25</td>
                    </tr>
                    <tr>
                      <td rowspan="1" colspan="1"> L. putamen</td>
                      <td align="center" rowspan="1" colspan="1">−20, 6, 4</td>
                      <td align="center" rowspan="1" colspan="1">3.90</td>
                      <td align="center" rowspan="1" colspan="1">—</td>
                      <td align="center" rowspan="1" colspan="1">13</td>
                    </tr>
                  </tbody>
                </table>
                <table-wrap-foot>
                  <fn>
                    <p>All brain regions (peaks) in which a classifier, trained on discriminating dot motion direction, decoded gaze direction at a threshold of <italic>P</italic> &lt; 0.001, uncorrected for multiple comparisons, better in the eyes open than in the eyes covered condition. All listed regions also decoded gaze direction in the eyes open condition significantly (<italic>P</italic> &lt; 0.05, uncorrected) better than chance (50%). FWE rate-corrected (corr) <italic>P</italic> values are reported for regions that survived the correction for multiple comparisons in our predefined ROIs (small-volume correction), consisting of the activation cluster from the MT+ visual motion localizer (<ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2003110117/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S2 and Table S1</ext-link>), or 10-mm-radius spheres around the TPJ activation peaks in a previous fMRI study on theory of mind (<xref rid="r11" ref-type="bibr">11</xref>). The right TPJ peak in the posterior STS also survived correction for multiple comparisons using the whole brain as search space (<italic>P</italic> = 0.040). L., left; R., right.</p>
                  </fn>
                </table-wrap-foot>
              </table-wrap>
            </sec>
            <sec sec-type="discussion" id="s2">
              <title>Discussion</title>
              <p content-type="flushleft">These results strongly suggest that, when people view a face looking at an object, the brain treats that gaze as though a movement were present, passing from the face to the object. That movement encoding was observed in area MT+, known to be involved in visual motion processing, and in the TPJ, known to be involved in social cognition. Gaze is arguably the most relevant cue to the state of someone else’s visual attention, and having an efficient neural machinery for keeping track of gaze is thus essential for reading and predicting other people’s minds and behavior (<xref rid="r14" ref-type="bibr">14</xref>, <xref rid="r26" ref-type="bibr">26</xref>, <xref rid="r27" ref-type="bibr">27</xref>). The present findings demonstrate that this process involves more than simple registration of low-level visual cues about other people’s eyes. Specific regions of the human brain appear to encode other people’s gaze as an active motion streaming through the empty space from the agent to the gazed-upon object. These findings are consistent with previous behavioral work showing that people implicitly treat other people’s eyes as though they emanated a weak force, gently “pushing” on objects in the external world (<xref rid="r5" ref-type="bibr">5</xref>, <xref rid="r6" ref-type="bibr">6</xref>). We propose that this implicit, fluid-flow model of other people’s gaze may help keep track of visual attention in a complex social environment. The model may be a part of theory of mind, modeling another mind actively focusing on content, a precondition for reconstructing that mind’s intentions, beliefs, emotions, and so on (<xref rid="r4" ref-type="bibr">4</xref>, <xref rid="r11" ref-type="bibr">11</xref>, <xref rid="r26" ref-type="bibr">26</xref>, <xref rid="r28" ref-type="bibr">28</xref>, <xref rid="r29" ref-type="bibr">29</xref>). It is well-known that, during the course of evolution, it is not uncommon that ancient biological mechanisms are reused in a different role, a phenomenon called “exaptation” (<xref rid="r30" ref-type="bibr">30</xref>). We speculate that the visual motion system may have been used during the evolution of social brain mechanisms for tracking the attention of others. It may have simply proved adaptive to coopt the brain’s motion system to keep track of sources and targets of visual attention. It is as if the brain draws a quick visual sketch with moving arrows to help keep track of who is attending to what.</p>
              <p>The MT+ and TPJ are well-situated for constructing a simplified, fluid-flow model of other people’s gaze. The MT+ is highly specialized in visual motion perception (<xref rid="r31" ref-type="bibr">31</xref>) and is activated when people view static images featuring conventional implied motion stimuli (e.g., a running animal) (<xref rid="r32" ref-type="bibr">32</xref>). The TPJ has been implicated in a range of social cognition tasks (<xref rid="r33" ref-type="bibr">33</xref>, <xref rid="r34" ref-type="bibr">34</xref>), is a key node in the theory of mind network (<xref rid="r11" ref-type="bibr">11</xref>, <xref rid="r12" ref-type="bibr">12</xref>), and has been implicated in processing the awareness or attention states of others (<xref rid="r7" ref-type="bibr">7</xref>, <xref rid="r13" ref-type="bibr">13</xref>). Our finding that a classifier trained to discriminate the direction of actual visual motion successfully generalizes to decode gaze direction in independent left-out data, and that this decoding performance is found in the hypothesized brain regions MT+ and TPJ, supports our proposal. Although we cannot exclude the possibility that the observed MT+ and TPJ decoding reflects the participants anticipating actions of the agent, such as reaching, rather than encoding the agent’s gaze, we consider this alternative explanation less likely. First, the decoding is significantly stronger in the sighted than in the blind agent. Second, as a target of gaze, we chose a tree rather than a more commonly grasped item such as a cup. Third, prior behavioral experiments using similar stimuli suggested that there is a flow-field effect of the eyes linked specifically to participants reconstructing an agent’s attention (<xref rid="r5" ref-type="bibr">5</xref>, <xref rid="r6" ref-type="bibr">6</xref>). The results therefore provide evidence that the visual motion system is used to facilitate social brain mechanism for tracking the gaze of others.</p>
              <p>The recruitment of motion systems in social cognition, representing gaze as implied motion emanating from the eyes, might help explain several culturally universal folk beliefs. These beliefs include the extramission myth that vision involves energy flowing out of the eyes (<xref rid="r5" ref-type="bibr">5</xref>, <xref rid="r35" ref-type="bibr">35</xref><xref rid="r36" ref-type="bibr"/><xref rid="r37" ref-type="bibr"/><xref rid="r38" ref-type="bibr"/>–<xref rid="r39" ref-type="bibr">39</xref>) and beliefs in the mind as a kind of energy that can flow out of the body and affect external objects. It is possible that basic theory-of-mind mechanisms have provided people with highly inaccurate intuitions and biases about the properties of the mind, leading to common myths and folk beliefs that have been intuitively compelling to humans across cultures and time periods.</p>
            </sec>
            <sec sec-type="materials|methods" id="s3">
              <title>Materials and Methods</title>
              <sec id="s4">
                <title>Participants.</title>
                <p>Thirty-two human volunteers (18 females, 28 righthanded), aged 18 to 52 y (mean age, 26 y; SD = 8) participated in the study. Subjects were recruited either from a paid subject pool, receiving 50 USD for participation, or among Princeton undergraduate students, who received course credits as compensation. All subjects provided informed consent, and all procedures were approved by the Princeton Institutional Review Board.</p>
              </sec>
              <sec id="s5">
                <title>Experimental Setup.</title>
                <p>Before commencing the scanning session, subjects were shown six sample trials on a laptop computer screen and given the instructions related to fixation and button press responses. During scanning, the subjects lay comfortably in a supine position on the MRI bed. Through an angled mirror mounted on top of the head coil, they viewed a translucent screen (56 × 30 cm) positioned ∼80 cm from the eyes (viewable area: 39° × 21°), on which visual stimuli were projected from a Hyperion MRI Digital Projection System (Psychology Software Tools, Sharpsburg, PA) with a resolution of 1,920 × 1,080 pixels. A computer running MATLAB (MathWorks, Natick, MA) and the Psychophysics Toolbox (<xref rid="r40" ref-type="bibr">40</xref>) was used to present the visual stimuli. A right hand five-button response unit (Psychology Software Tools Celeritas, Sharpsburg, PA) was strapped onto the subjects’ right wrist, and they used the right index finger button to indicate responses during the catch trials.</p>
              </sec>
              <sec id="s6">
                <title>Experimental Conditions and Visual Stimuli.</title>
                <p>The experiment comprised seven conditions, featuring either random dot motion or static images of a face and a neutral object (a tree). The motion stimulus consisted of a 5°-wide × 5°-high field of randomly distributed, short-lived, moving black dots. Each dot had a diameter of 0.05°, a velocity of 2°/s, and a lifetime of 200 ms. The density was 50 dots per square visual degree. One hundred percent of the dots moved coherently in the same direction, creating an unambiguous sense of motion to the left (dot motion left condition) or right (dot motion right condition). The stimulus was presented for 1,500 ms on each trial.</p>
                <p>In the trials featuring static images, participants saw a cartoon face in profile on one side of the display, facing a tree on the other side. In the eyes open conditions, the eyes on the face were open, implying that the head was gazing at the tree. The face was 5.2° wide × 5.7° high, and the tree was 4.5° wide × 5.7° high. The face and the tree appeared on opposite sides of the fixation point, and the edge of each image was distanced 2.5° from the midline (thus, the total distance between the tree and the face was 5°). The face appeared either on the right side looking left (eyes open left condition) or on the left side looking right (eyes open right condition). In the two control conditions, eyes covered left and eyes covered right, the eyes of the cartoon face were covered with a blindfold, while all other visual features were kept identical. Again, the stimulus was presented for 1,500 ms on each trial.</p>
                <p>The purpose of the catch trials was to ensure that the subjects paid attention to the visual stimuli. On each catch trial, some part of the visual stimulus was colored bright green, and subjects were instructed to press a button as soon as they discovered that something was green. The catch trials consisted of dot motion featuring bright green dots (instead of black ones), or the static face-and-tree image where either the face (eyes open or blindfolded) or the tree was colored green. Within each run, there was exactly one trial of each of the three (dots, face, object) catch trial types.</p>
                <p>Throughout each run—in all seven conditions, as well as during the intertrial intervals—a light gray fixation point (0.5° diameter) of a shade (red [R], 198; green [G], 198; blue [B], 198) just slightly darker than the gray background (R, 210; G, 210; B, 210) was positioned slightly below the center of the screen (note that the fixation point shown in <xref ref-type="fig" rid="fig01">Fig. 1</xref> is darker than the one used in the stimulus, for visualization purposes). The purpose of using a subtle gray color and placing the fixation below the line of sight of the cartoon face was to avoid the visual impression that the face was gazing at the fixation point instead of at the tree. Subjects were instructed to maintain fixation as continuously as possible throughout the run.</p>
                <p>The experiment consisted of 20 runs. In each run, the seven conditions were repeated three times, yielding a total of 21 trials per run. The trial order was fully randomized, with the limitation that two consecutive trials could not belong to the same condition. We included 10 s of baseline before the onset of the first trial, and 16 s of baseline after the offset of the last trial, in each run. The run-average intertrial interval (ITI) was 8.0 s (individual ITIs were jittered between 6.0 and 10.0 s), yielding a total run duration of 3 min 38 s.</p>
              </sec>
              <sec id="s7">
                <title>fMRI Acquisition.</title>
                <p>Functional imaging data were collected using a Siemens Prisma 3T scanner equipped with a 64-channel head coil. Gradient-echo T2*-weighted echo-planar images (EPIs) with BOLD contrast were used as an index of brain activity (<xref rid="r41" ref-type="bibr">41</xref>). Functional image volumes were composed of 54 near-axial slices with a thickness of 2.5 mm (with no interslice gap), which ensured that the entire brain, excluding cerebellum, was within the field-of-view in all subjects (54 × 78 matrix, 2.5 mm × 2.5 mm in-plane resolution, echo time [TE] = 30 ms, flip angle = 80<bold>°</bold>). Simultaneous multislice (SMS) imaging was used (SMS factor = 2). One complete volume was collected every 2 s (repetition time [TR] = 2,000 ms). A total of 2,180 functional volumes were collected for each participant in the main experiment, divided into 20 runs (109 volumes per run). In the subsequent area MT+ functional localizer experiment, 316 functional volumes were collected (158 volumes × two runs). The first three volumes of each run were discarded to account for non–steady-state magnetization. A high-resolution structural image was acquired for each participant at the end of the experiment (three dimensional magnetization prepared rapid gradient echo sequence, voxel size = 1 mm isotropic, field of view = 256 mm, 176 slices, TR = 2,300 ms, TE = 2.96 ms, inversion time = 1,000 ms, flip angle = 9°, integrated parallel acquisition technique generalized autocalibrating partially parallel acquisition = 2). At the end of each scanning session, matching spin echo EPI pairs (anterior-to-posterior and posterior-to-anterior) were acquired for blip-up/blip-down field map correction.</p>
              </sec>
              <sec id="s8">
                <title>fMRI Preprocessing.</title>
                <p>Preprocessing was carried out using the FMRIPREP version v1.2.3 (<xref rid="r42" ref-type="bibr">42</xref>) pipeline. See <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2003110117/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, <italic>Supplementary Text</italic></ext-link> for details.</p>
              </sec>
              <sec id="s9">
                <title>fMRI Analysis.</title>
                <p>The fMRI data from all participants were analyzed with the Statistical Parametric Mapping software (SPM12) (Wellcome Department of Cognitive Neurology, London, UK) (<xref rid="r43" ref-type="bibr">43</xref>). We first used a conventional GLM to estimate regression (beta) coefficients for the six main conditions in each run: dot motion left, dot motion right, eyes open (facing) left, eyes open right, eyes covered left, and eyes covered right. One regressor of no interest was created to model all of the catch trials. Each condition was modeled with a boxcar function of duration 1.5 s and convolved with the standard SPM12 hemodynamic response function. The runwise beta coefficients for the six main conditions were then submitted to subsequent multivariate analyses (<xref rid="r21" ref-type="bibr">21</xref>, <xref rid="r22" ref-type="bibr">22</xref>).</p>
                <p>Within each participant, we used locally multivariate mapping (the Searchlight approach) (<xref rid="r20" ref-type="bibr">20</xref>) to identify multivoxel patterns. The analysis was carried out in The Decoding Toolbox (TDT) version 3.997 (<xref rid="r44" ref-type="bibr">44</xref>) for SPM. The brain was partitioned into overlapping voxel clusters, each of which was approximately spherical in shape with a radius of 12 mm (default value in TDT). In each of these clusters, we used linear SVMs (with the fixed regularization parameter of C = 1) to compute decoding accuracies. We used a cross-classification approach where an SVM was trained to discriminate dot motion left versus dot motion right and then tested on either the eyes open (left versus right) or eyes covered (left versus right). To ensure independent training and testing datasets, we used a 20-fold leave-one-run-out cross-validation approach. This process resulted in two decoding accuracy maps (eyes open<sub>left</sub>
<sub>vs.</sub>
<sub>right</sub> and eyes covered<sub>left</sub>
<sub>vs.</sub>
<sub>right</sub>) for each subject, in which the value of each voxel represents the average proportion of correctly classified runs. Because the goal of the analysis was to identify areas in which the decoding accuracy was higher in eyes open<sub>left</sub>
<sub>vs.</sub>
<sub>right</sub> than in eyes covered<sub>left</sub>
<sub>vs.</sub>
<sub>right</sub>, we calculated the voxel-wise decoding accuracy difference (eyes open<sub>left</sub>
<sub>vs.</sub>
<sub>right</sub> minus eyes covered<sub>left</sub>
<sub>vs.</sub>
<sub>right</sub>). The resulting decoding maps were spatially normalized to the standard Montreal Neurological Institute (MNI) space and smoothed using a 3-mm full width at half maximum (FWHM) Gaussian kernel, and then entered into a second-level analysis using SPM12.</p>
                <p>At the second-level, we employed a voxel-wise whole-brain approach. The whole-brain decoding difference map (eyes open<sub>left</sub>
<sub>vs.</sub>
<sub>right</sub> minus eyes covered<sub>left</sub>
<sub>vs.</sub>
<sub>right</sub>) was thresholded at <italic>P</italic> &lt; 0.001 (uncorrected for multiple comparisons), using the decoding map representing eyes open<sub>left</sub>
<sub>vs.</sub>
<sub>right</sub> versus chance level (50%) as an inclusive mask (thresholded at <italic>P</italic> &lt; 0.05, uncorrected), and projected onto orthogonal sections of the average structural scan generated from the 32 subjects. For the statistical inference, we applied corrections for multiple comparisons within our ROIs using the family-wise error (FWE) rate correction implemented in SPM12 (“small-volume correction”). Our a priori defined ROIs consisted of the functionally localized left and right area MT+ (see <xref ref-type="sec" rid="s10">MT Complex Visual Motion Localizer</xref> below), and the left and right TPJ defined as 10-mm-radius spheres centered on the peak MNI coordinates (left TPJ, −54, −60, 21; and right TPJ, 51, −54, 27) in a previous landmark study of theory of mind (<xref rid="r11" ref-type="bibr">11</xref>). For areas outside the hypothesized regions, we corrected for multiple comparisons using the whole brain as search space. For the whole-brain analysis, we used the permutation testing approach implemented in the SnPM13 toolbox (<xref rid="r45" ref-type="bibr">45</xref>), in which whole-brain significant voxels are identified as voxels whose <italic>t</italic> values are greater than 95% of the whole-brain maximum <italic>t</italic> values of a distribution of group-level statistical maps with permuted condition labels (10,000 iterations). Only one such whole-brain significant voxel was identified, and it was located within our a priori defined right TPJ ROI. In a purely descriptive manner, we also report the locations and <italic>t</italic> values of strong activations (<italic>P</italic> &lt; 0.001, uncorrected) to illustrate the specificity of the significant activations (<xref rid="t01" ref-type="table">Table 1</xref>). In the figures, the activation maps had a significance threshold of <italic>P</italic> &lt; 0.001 (uncorrected) (<xref ref-type="fig" rid="fig02">Fig. 2</xref> and <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2003110117/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S3</ext-link>). For all activations, the coordinates of the peak voxel are given in the MNI standard space (<italic>x</italic>, <italic>y</italic>, <italic>z</italic>).</p>
              </sec>
              <sec id="s10">
                <title>MT Complex Visual Motion Localizer.</title>
                <p>To facilitate the localization of BOLD responses within the MT complex (MT+), we exposed subjects to a functional localizer of MT+ (<xref rid="r23" ref-type="bibr">23</xref><xref rid="r24" ref-type="bibr"/>–<xref rid="r25" ref-type="bibr">25</xref>) after completing the 20 runs of the main experiment. The procedures and stimuli were identical to those described for the “Full-field hMT+ visual motion localizer” condition in Jiang et al. (<xref rid="r23" ref-type="bibr">23</xref>). Specifically, the localizer stimulus consisted of blocks of moving dots, static dots, and a fixation condition containing no dots. The dots were white on a black background and had a limited lifetime of 200 ms. In the moving condition, all of the dots moved coherently in one of eight directions (spaced evenly between 0° and 360°) with a speed of 8°/s. The direction of motion changed once per second (the same direction was prevented from appearing twice in a row). In the static condition, dots were presented without motion, and the positions of the dots were reset once per second. In the fixation condition, subjects were presented with only the fixation cross but no dots. Dots were presented within a circular aperture (radius 8°) with a central fixation cross-surrounded by a gap (radius 1.5°, to minimize motion-induced eye movements) in the dot field. The diameter of each dot was 0.3°, and dot density was one per square degree.</p>
                <p>Participants were asked to fixate throughout the scan and performed no task. Each block lasted 10 s, during which one of the three visual stimulation conditions (motion, static, and fixation) was presented. The three conditions were cycled in a fixed order (motion, static, and fixation). Every participant performed two runs, each lasting ∼5 min, and included 30 10-s blocks.</p>
                <p>After preprocessing using FMRIPREP as described above, we smoothed the spatially normalized functional volumes using a 6-mm FWHM Gaussian kernel. The smoothed data were then entered into a GLM. In the first-level analysis, we defined separate regressors for the moving and static conditions, modeling the 10-s epochs with a boxcar function convolved with the standard SPM12 hemodynamic response function. We defined a linear contrast (moving-static) in the GLM, and the contrast images from all subjects were entered into a random effects group analysis (second-level analysis). The resulting group-level activation map was thresholded at <italic>P</italic> &lt; 0.05 (FWE-corrected using the entire brain as search space), and the distinct clusters of activation in the temporo-parieto-occipital junction on the left and right side defined our MT+ ROIs (<ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2003110117/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S2</ext-link>).</p>
              </sec>
              <sec id="s11">
                <title>Eye Tracking.</title>
                <p>Eye movements were recorded via an MRI-compatible infrared eye tracker (SR Research EyeLink 1000 Plus), mounted just below the projector screen, sampling at 1,000 Hz. Before each scanning session, a calibration routine on five screen locations was used and repeated until the maximum error for any point was less than 1°. The obtained eye position data were cleaned of artifacts related to blink events and smoothed using a 20-ms moving average. It was then analyzed in two ways. First, we calculated the proportion of time subjects stayed on fixation (defined as an eye position within the display area 2.5° surrounding the fixation point), to estimate how well subjects followed the instructions. Second, we built an SVM decoding model analog to the cross-classification approach used for the fMRI data, but here based purely on eye-tracking data, to test whether eye movement dynamics alone are sufficient to decode the conditions of interest. In keeping with a previous study (<xref rid="r46" ref-type="bibr">46</xref>), we organized the data in the following way. The part of the display within which the stimuli appear was divided into a 3 × 9 grid of 27 equally sized (1.6°) squares. For each trial, the proportion of time that the subject fixated within each square (27 features) and the saccades between those regions (27 × 27 = 729 features) were calculated. These 756 features, representing information about both where people were looking as well as saccade dynamics, were then averaged across repetitions for each of the six conditions within each of the 20 runs, yielding one eye movement feature vector per condition per run (per subject). The feature vectors were submitted to an SVM classifier (C = 1). Using a 20-fold leave-one-run-out approach, the SVM model was trained on dot motion left versus dot motion right and tested on eyes open (left versus right) or eyes covered (left versus right) in the left-out run. At the group level, the decoding accuracies were tested against chance level using <italic>t</italic> tests. The results showed that gaze direction could not be decoded significantly (<italic>P</italic> &gt; 0.05) better than chance in either the eyes open or the eyes covered conditions (see the <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2003110117/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S1</ext-link> legend for statistical details).</p>
              </sec>
              <sec id="s12">
                <title>Postscan Questionnaire.</title>
                <p>After the scanning session was completed, subjects were given a questionnaire asking what they thought the purpose of the experiment might be. Though subjects offered guesses about the purpose of the experiment, none indicated anything close to a correct understanding.</p>
              </sec>
              <sec id="s13">
                <title>Data Availability.</title>
                <p>The data that support the findings of this study are available at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/Other_People_s_Gaze_Encoded_as_Implied_Motion_in_the_Human_Brain/12184848/1">https://figshare.com/articles/Other_People_s_Gaze_Encoded_as_Implied_Motion_in_the_Human_Brain/12184848/1</ext-link> (<xref rid="bib47" ref-type="bibr">47</xref>).</p>
              </sec>
            </sec>
            <sec sec-type="supplementary-material">
              <title>Supplementary Material</title>
              <supplementary-material content-type="local-data">
                <label>Supplementary File</label>
                <media xlink:href="pnas.2003110117.sapp.pdf"/>
              </supplementary-material>
            </sec>
          </body>
          <back>
            <ack>
              <p>This work was supported by the Princeton Neuroscience Institute Innovation Fund. A.G. was supported by the Wenner–Gren Foundation, the Swedish Society of Medicine, and the Foundation Blanceflor. We thank Sam Nastase for valuable input regarding the multivoxel pattern analysis.</p>
            </ack>
            <fn-group>
              <fn fn-type="COI-statement">
                <p>The authors declare no competing interest.</p>
              </fn>
              <fn fn-type="other">
                <p>This article is a PNAS Direct Submission.</p>
              </fn>
              <fn fn-type="other">
                <p>Data deposition: The data that support the findings of this study are available at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/Other_People_s_Gaze_Encoded_as_Implied_Motion_in_the_Human_Brain/12184848/1" xlink:show="new">https://figshare.com/articles/Other_People_s_Gaze_Encoded_as_Implied_Motion_in_the_Human_Brain/12184848/1</ext-link>.</p>
              </fn>
              <fn fn-type="supplementary-material">
                <p>This article contains supporting information online at <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2003110117/-/DCSupplemental" xlink:show="new">https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2003110117/-/DCSupplemental</ext-link>.</p>
              </fn>
            </fn-group>
            <ref-list>
              <ref id="r1">
                <label>1</label>
                <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Yarbus</surname><given-names>A. L.</given-names></name>, </person-group><chapter-title>“Eye movements during perception of complex objects”</chapter-title> in <source>Eye Movements and Vision</source>, <person-group person-group-type="editor"><name name-style="western"><surname>Yarbus</surname><given-names>A. L.</given-names></name></person-group>, Ed. (<publisher-loc>Springer</publisher-loc>, <year>1967</year>), pp. <fpage>171</fpage>–<lpage>211</lpage>.</mixed-citation>
              </ref>
              <ref id="r2">
                <label>2</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Symons</surname><given-names>L. A.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Cedrone</surname><given-names>C. C.</given-names></name>, <name name-style="western"><surname>Nishimura</surname><given-names>M.</given-names></name></person-group>, <article-title>What are you looking at? Acuity for triadic eye gaze</article-title>. <source>J. Gen. Psychol.</source><volume>131</volume>, <fpage>451</fpage>–<lpage>469</lpage> (<year>2004</year>).<pub-id pub-id-type="pmid">15523825</pub-id></mixed-citation>
              </ref>
              <ref id="r3">
                <label>3</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kobayashi</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Kohshima</surname><given-names>S.</given-names></name></person-group>, <article-title>Unique morphology of the human eye</article-title>. <source>Nature</source><volume>387</volume>, <fpage>767</fpage>–<lpage>768</lpage> (<year>1997</year>).</mixed-citation>
              </ref>
              <ref id="r4">
                <label>4</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Graziano</surname><given-names>M. S.</given-names></name>, <name name-style="western"><surname>Kastner</surname><given-names>S.</given-names></name></person-group>, <article-title>Human consciousness and its relationship to social neuroscience: A novel hypothesis</article-title>. <source>Cogn. Neurosci.</source><volume>2</volume>, <fpage>98</fpage>–<lpage>113</lpage> (<year>2011</year>).<pub-id pub-id-type="pmid">22121395</pub-id></mixed-citation>
              </ref>
              <ref id="r5">
                <label>5</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guterstam</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Kean</surname><given-names>H. H.</given-names></name>, <name name-style="western"><surname>Webb</surname><given-names>T. W.</given-names></name>, <name name-style="western"><surname>Kean</surname><given-names>F. S.</given-names></name>, <name name-style="western"><surname>Graziano</surname><given-names>M. S. A.</given-names></name></person-group>, <article-title>Implicit model of other people’s visual attention as an invisible, force-carrying beam projecting from the eyes</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><volume>116</volume>, <fpage>328</fpage>–<lpage>333</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">30559179</pub-id></mixed-citation>
              </ref>
              <ref id="r6">
                <label>6</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guterstam</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Graziano</surname><given-names>M. S. A.</given-names></name></person-group>, <article-title>Implied motion as a possible mechanism for encoding other people’s attention</article-title>. <source>Prog. Neurobiol.</source>, <fpage>10.1016/j.pneurobio.2020.101797</fpage> (<year>2020</year>).</mixed-citation>
              </ref>
              <ref id="r7">
                <label>7</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kelly</surname><given-names>Y. T.</given-names></name>, <name name-style="western"><surname>Webb</surname><given-names>T. W.</given-names></name>, <name name-style="western"><surname>Meier</surname><given-names>J. D.</given-names></name>, <name name-style="western"><surname>Arcaro</surname><given-names>M. J.</given-names></name>, <name name-style="western"><surname>Graziano</surname><given-names>M. S.</given-names></name></person-group>, <article-title>Attributing awareness to oneself and to others</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><volume>111</volume>, <fpage>5012</fpage>–<lpage>5017</lpage> (<year>2014</year>).<pub-id pub-id-type="pmid">24639542</pub-id></mixed-citation>
              </ref>
              <ref id="r8">
                <label>8</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Graziano</surname><given-names>M. S. A.</given-names></name>, <name name-style="western"><surname>Guterstam</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Bio</surname><given-names>B. J.</given-names></name>, <name name-style="western"><surname>Wilterson</surname><given-names>A. I.</given-names></name></person-group>, <article-title>Toward a standard model of consciousness: Reconciling the attention schema, global workspace, higher-order thought, and illusionist theories</article-title>. <source>Cogn. Neuropsychol.</source>, <fpage>1</fpage>–<lpage>18</lpage> (<year>2019</year>).</mixed-citation>
              </ref>
              <ref id="r9">
                <label>9</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Newsome</surname><given-names>W. T.</given-names></name>, <name name-style="western"><surname>Paré</surname><given-names>E. B.</given-names></name></person-group>, <article-title>A selective impairment of motion perception following lesions of the middle temporal visual area (MT)</article-title>. <source>J. Neurosci.</source><volume>8</volume>, <fpage>2201</fpage>–<lpage>2211</lpage> (<year>1988</year>).<pub-id pub-id-type="pmid">3385495</pub-id></mixed-citation>
              </ref>
              <ref id="r10">
                <label>10</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tootell</surname><given-names>R. B.</given-names></name><etal/></person-group>, <article-title>Visual motion aftereffect in human cortical area MT revealed by functional magnetic resonance imaging</article-title>. <source>Nature</source><volume>375</volume>, <fpage>139</fpage>–<lpage>141</lpage> (<year>1995</year>).<pub-id pub-id-type="pmid">7753168</pub-id></mixed-citation>
              </ref>
              <ref id="r11">
                <label>11</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saxe</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Kanwisher</surname><given-names>N.</given-names></name></person-group>, <article-title>People thinking about thinking people. The role of the temporo-parietal junction in “theory of mind”</article-title>. <source>Neuroimage</source><volume>19</volume>, <fpage>1835</fpage>–<lpage>1842</lpage> (<year>2003</year>).<pub-id pub-id-type="pmid">12948738</pub-id></mixed-citation>
              </ref>
              <ref id="r12">
                <label>12</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gallagher</surname><given-names>H. L.</given-names></name><etal/></person-group>, <article-title>Reading the mind in cartoons and stories: An fMRI study of “theory of mind” in verbal and nonverbal tasks</article-title>. <source>Neuropsychologia</source><volume>38</volume>, <fpage>11</fpage>–<lpage>21</lpage> (<year>2000</year>).<pub-id pub-id-type="pmid">10617288</pub-id></mixed-citation>
              </ref>
              <ref id="r13">
                <label>13</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Igelström</surname><given-names>K. M.</given-names></name>, <name name-style="western"><surname>Webb</surname><given-names>T. W.</given-names></name>, <name name-style="western"><surname>Kelly</surname><given-names>Y. T.</given-names></name>, <name name-style="western"><surname>Graziano</surname><given-names>M. S. A.</given-names></name></person-group>, <article-title>Topographical organization of attentional, social, and memory processes in the human temporoparietal cortex</article-title>. <source>eNeuro</source><volume>3</volume>, <fpage>ENEURO.0060</fpage>-<lpage>16.2016</lpage> (<year>2016</year>).</mixed-citation>
              </ref>
              <ref id="r14">
                <label>14</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Calder</surname><given-names>A. J.</given-names></name><etal/></person-group>, <article-title>Reading the mind from eye gaze</article-title>. <source>Neuropsychologia</source><volume>40</volume>, <fpage>1129</fpage>–<lpage>1138</lpage> (<year>2002</year>).<pub-id pub-id-type="pmid">11931917</pub-id></mixed-citation>
              </ref>
              <ref id="r15">
                <label>15</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Puce</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Allison</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Bentin</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Gore</surname><given-names>J. C.</given-names></name>, <name name-style="western"><surname>McCarthy</surname><given-names>G.</given-names></name></person-group>, <article-title>Temporal cortex activation in humans viewing eye and mouth movements</article-title>. <source>J. Neurosci.</source><volume>18</volume>, <fpage>2188</fpage>–<lpage>2199</lpage> (<year>1998</year>).<pub-id pub-id-type="pmid">9482803</pub-id></mixed-citation>
              </ref>
              <ref id="r16">
                <label>16</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Perrett</surname><given-names>D. I.</given-names></name><etal/></person-group>, <article-title>Visual cells in the temporal cortex sensitive to face view and gaze direction</article-title>. <source>Proc. R. Soc. Lond. B Biol. Sci.</source><volume>223</volume>, <fpage>293</fpage>–<lpage>317</lpage> (<year>1985</year>).<pub-id pub-id-type="pmid">2858100</pub-id></mixed-citation>
              </ref>
              <ref id="r17">
                <label>17</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hoffman</surname><given-names>E. A.</given-names></name>, <name name-style="western"><surname>Haxby</surname><given-names>J. V.</given-names></name></person-group>, <article-title>Distinct representations of eye gaze and identity in the distributed human neural system for face perception</article-title>. <source>Nat. Neurosci.</source><volume>3</volume>, <fpage>80</fpage>–<lpage>84</lpage> (<year>2000</year>).<pub-id pub-id-type="pmid">10607399</pub-id></mixed-citation>
              </ref>
              <ref id="r18">
                <label>18</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wicker</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Michel</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Henaff</surname><given-names>M.-A.</given-names></name>, <name name-style="western"><surname>Decety</surname><given-names>J.</given-names></name></person-group>, <article-title>Brain regions involved in the perception of gaze: A PET study</article-title>. <source>Neuroimage</source><volume>8</volume>, <fpage>221</fpage>–<lpage>227</lpage> (<year>1998</year>).<pub-id pub-id-type="pmid">9740764</pub-id></mixed-citation>
              </ref>
              <ref id="r19">
                <label>19</label>
                <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Julesz</surname><given-names>B.</given-names></name></person-group>, <source>Foundations of Cyclopean Perception</source>, (<publisher-name>University of Chicago Press</publisher-name>, <year>1971</year>).</mixed-citation>
              </ref>
              <ref id="r20">
                <label>20</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kriegeskorte</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Goebel</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Bandettini</surname><given-names>P.</given-names></name></person-group>, <article-title>Information-based functional brain mapping</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><volume>103</volume>, <fpage>3863</fpage>–<lpage>3868</lpage> (<year>2006</year>).<pub-id pub-id-type="pmid">16537458</pub-id></mixed-citation>
              </ref>
              <ref id="r21">
                <label>21</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Haxby</surname><given-names>J. V.</given-names></name><etal/></person-group>, <article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title>. <source>Science</source><volume>293</volume>, <fpage>2425</fpage>–<lpage>2430</lpage> (<year>2001</year>).<pub-id pub-id-type="pmid">11577229</pub-id></mixed-citation>
              </ref>
              <ref id="r22">
                <label>22</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Misaki</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Bandettini</surname><given-names>P. A.</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname><given-names>N.</given-names></name></person-group>, <article-title>Comparison of multivariate classifiers and response normalizations for pattern-information fMRI</article-title>. <source>Neuroimage</source><volume>53</volume>, <fpage>103</fpage>–<lpage>118</lpage> (<year>2010</year>).<pub-id pub-id-type="pmid">20580933</pub-id></mixed-citation>
              </ref>
              <ref id="r23">
                <label>23</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Beauchamp</surname><given-names>M. S.</given-names></name>, <name name-style="western"><surname>Fine</surname><given-names>I.</given-names></name></person-group>, <article-title>Re-examining overlap between tactile and visual motion responses within hMT+ and STS</article-title>. <source>Neuroimage</source><volume>119</volume>, <fpage>187</fpage>–<lpage>196</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">26123373</pub-id></mixed-citation>
              </ref>
              <ref id="r24">
                <label>24</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huk</surname><given-names>A. C.</given-names></name>, <name name-style="western"><surname>Dougherty</surname><given-names>R. F.</given-names></name>, <name name-style="western"><surname>Heeger</surname><given-names>D. J.</given-names></name></person-group>, <article-title>Retinotopy and functional subdivision of human areas MT and MST</article-title>. <source>J. Neurosci.</source><volume>22</volume>, <fpage>7195</fpage>–<lpage>7205</lpage> (<year>2002</year>).<pub-id pub-id-type="pmid">12177214</pub-id></mixed-citation>
              </ref>
              <ref id="r25">
                <label>25</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Beauchamp</surname><given-names>M. S.</given-names></name>, <name name-style="western"><surname>Yasar</surname><given-names>N. E.</given-names></name>, <name name-style="western"><surname>Kishan</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Ro</surname><given-names>T.</given-names></name></person-group>, <article-title>Human MST but not MT responds to tactile stimulation</article-title>. <source>J. Neurosci.</source><volume>27</volume>, <fpage>8261</fpage>–<lpage>8267</lpage> (<year>2007</year>).<pub-id pub-id-type="pmid">17670972</pub-id></mixed-citation>
              </ref>
              <ref id="r26">
                <label>26</label>
                <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Baron-Cohen</surname><given-names>S.</given-names></name></person-group>, <source>Mindblindness: An Essay on Autism and Theory of Mind</source>, (<publisher-name>MIT Press</publisher-name>, <year>1997</year>).</mixed-citation>
              </ref>
              <ref id="r27">
                <label>27</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Frischen</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Bayliss</surname><given-names>A. P.</given-names></name>, <name name-style="western"><surname>Tipper</surname><given-names>S. P.</given-names></name></person-group>, <article-title>Gaze cueing of attention: Visual attention, social cognition, and individual differences</article-title>. <source>Psychol. Bull.</source><volume>133</volume>, <fpage>694</fpage>–<lpage>724</lpage> (<year>2007</year>).<pub-id pub-id-type="pmid">17592962</pub-id></mixed-citation>
              </ref>
              <ref id="r28">
                <label>28</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saxe</surname><given-names>R.</given-names></name></person-group>, <article-title>Uniquely human social cognition</article-title>. <source>Curr. Opin. Neurobiol.</source><volume>16</volume>, <fpage>235</fpage>–<lpage>239</lpage> (<year>2006</year>).<pub-id pub-id-type="pmid">16546372</pub-id></mixed-citation>
              </ref>
              <ref id="r29">
                <label>29</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baron-Cohen</surname><given-names>S.</given-names></name></person-group>, <article-title>Precursors to a theory of mind: Understanding attention in others</article-title>. <source>Nat. Theor. Mind Evol. Dev. Simul. Everyday Mindreading</source><volume>1</volume>, <fpage>233</fpage>–<lpage>251</lpage> (<year>1991</year>).</mixed-citation>
              </ref>
              <ref id="r30">
                <label>30</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gould</surname><given-names>S. J.</given-names></name>, <name name-style="western"><surname>Vrba</surname><given-names>E. S.</given-names></name></person-group>, <article-title>Exaptation—A missing term in the science of form</article-title>. <source>Paleobiology</source><volume>8</volume>, <fpage>4</fpage>–<lpage>15</lpage> (<year>1982</year>).</mixed-citation>
              </ref>
              <ref id="r31">
                <label>31</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zeki</surname><given-names>S.</given-names></name><etal/></person-group>, <article-title>A direct demonstration of functional specialization in human visual cortex</article-title>. <source>J. Neurosci.</source><volume>11</volume>, <fpage>641</fpage>–<lpage>649</lpage> (<year>1991</year>).<pub-id pub-id-type="pmid">2002358</pub-id></mixed-citation>
              </ref>
              <ref id="r32">
                <label>32</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kourtzi</surname><given-names>Z.</given-names></name>, <name name-style="western"><surname>Kanwisher</surname><given-names>N.</given-names></name></person-group>, <article-title>Activation in human MT/MST by static images with implied motion</article-title>. <source>J. Cogn. Neurosci.</source><volume>12</volume>, <fpage>48</fpage>–<lpage>55</lpage> (<year>2000</year>).<pub-id pub-id-type="pmid">10769305</pub-id></mixed-citation>
              </ref>
              <ref id="r33">
                <label>33</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Allison</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Puce</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>McCarthy</surname><given-names>G.</given-names></name></person-group>, <article-title>Social perception from visual cues: Role of the STS region</article-title>. <source>Trends Cogn. Sci.</source><volume>4</volume>, <fpage>267</fpage>–<lpage>278</lpage> (<year>2000</year>).<pub-id pub-id-type="pmid">10859571</pub-id></mixed-citation>
              </ref>
              <ref id="r34">
                <label>34</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Greene</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Haidt</surname><given-names>J.</given-names></name></person-group>, <article-title>How (and where) does moral judgment work?</article-title><source>Trends Cogn. Sci.</source><volume>6</volume>, <fpage>517</fpage>–<lpage>523</lpage> (<year>2002</year>).<pub-id pub-id-type="pmid">12475712</pub-id></mixed-citation>
              </ref>
              <ref id="r35">
                <label>35</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Benassi</surname><given-names>V. A.</given-names></name>, <name name-style="western"><surname>Sweeney</surname><given-names>P. D.</given-names></name>, <name name-style="western"><surname>Drevno</surname><given-names>G. E.</given-names></name></person-group>, <article-title>Mind over matter: Perceived success at psychokinesis</article-title>. <source>J. Pers. Soc. Psychol.</source><volume>37</volume>, <fpage>1377</fpage>–<lpage>1386</lpage> (<year>1979</year>).</mixed-citation>
              </ref>
              <ref id="r36">
                <label>36</label>
                <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Dundes</surname><given-names>A.</given-names></name></person-group>, <source>The Evil Eye: A Casebook</source>, (<publisher-name>University of Wisconsin Press</publisher-name>, <year>1992</year>).</mixed-citation>
              </ref>
              <ref id="r37">
                <label>37</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gross</surname><given-names>C. G.</given-names></name></person-group>, <article-title>The fire that comes from the eye</article-title>. <source>Neuroscientist</source><volume>5</volume>, <fpage>58</fpage>–<lpage>64</lpage> (<year>1999</year>).</mixed-citation>
              </ref>
              <ref id="r38">
                <label>38</label>
                <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Sidky</surname><given-names>H.</given-names></name></person-group>, <source>The Origins of Shamanism, Spirit Beliefs, and Religiosity: A Cognitive Anthropological Perspective</source>, (<publisher-name>Lexington Books</publisher-name>, <year>2017</year>).</mixed-citation>
              </ref>
              <ref id="r39">
                <label>39</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Winer</surname><given-names>G. A.</given-names></name>, <name name-style="western"><surname>Cottrell</surname><given-names>J. E.</given-names></name>, <name name-style="western"><surname>Gregg</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Fournier</surname><given-names>J. S.</given-names></name>, <name name-style="western"><surname>Bica</surname><given-names>L. A.</given-names></name></person-group>, <article-title>Fundamentally misunderstanding visual perception. Adults’ belief in visual emissions</article-title>. <source>Am. Psychol.</source><volume>57</volume>, <fpage>417</fpage>–<lpage>424</lpage> (<year>2002</year>).<pub-id pub-id-type="pmid">12094435</pub-id></mixed-citation>
              </ref>
              <ref id="r40">
                <label>40</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brainard</surname><given-names>D. H.</given-names></name></person-group>, <article-title>The psychophysics toolbox</article-title>. <source>Spat. Vis.</source><volume>10</volume>, <fpage>433</fpage>–<lpage>436</lpage> (<year>1997</year>).<pub-id pub-id-type="pmid">9176952</pub-id></mixed-citation>
              </ref>
              <ref id="r41">
                <label>41</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Logothetis</surname><given-names>N. K.</given-names></name>, <name name-style="western"><surname>Pauls</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Augath</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Trinath</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Oeltermann</surname><given-names>A.</given-names></name></person-group>, <article-title>Neurophysiological investigation of the basis of the fMRI signal</article-title>. <source>Nature</source><volume>412</volume>, <fpage>150</fpage>–<lpage>157</lpage> (<year>2001</year>).<pub-id pub-id-type="pmid">11449264</pub-id></mixed-citation>
              </ref>
              <ref id="r42">
                <label>42</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Esteban</surname><given-names>O.</given-names></name><etal/></person-group>, <article-title>fMRIPrep: A robust preprocessing pipeline for functional MRI</article-title>. <source>Nat. Methods</source><volume>16</volume>, <fpage>111</fpage>–<lpage>116</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">30532080</pub-id></mixed-citation>
              </ref>
              <ref id="r43">
                <label>43</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K. J.</given-names></name><etal/></person-group>, <article-title>Statistical parametric maps in functional imaging: A general linear approach</article-title>. <source>Hum. Brain Mapp.</source><volume>2</volume>, <fpage>189</fpage>–<lpage>210</lpage> (<year>1994</year>).</mixed-citation>
              </ref>
              <ref id="r44">
                <label>44</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hebart</surname><given-names>M. N.</given-names></name>, <name name-style="western"><surname>Görgen</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Haynes</surname><given-names>J.-D.</given-names></name></person-group>, <article-title>The Decoding Toolbox (TDT): A versatile software package for multivariate analyses of functional imaging data</article-title>. <source>Front. Neuroinformatics</source><volume>8</volume>, <fpage>88</fpage> (<year>2015</year>).</mixed-citation>
              </ref>
              <ref id="r45">
                <label>45</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nichols</surname><given-names>T. E.</given-names></name>, <name name-style="western"><surname>Holmes</surname><given-names>A. P.</given-names></name></person-group>, <article-title>Nonparametric permutation tests for functional neuroimaging: A primer with examples</article-title>. <source>Hum. Brain Mapp.</source><volume>15</volume>, <fpage>1</fpage>–<lpage>25</lpage> (<year>2002</year>).<pub-id pub-id-type="pmid">11747097</pub-id></mixed-citation>
              </ref>
              <ref id="r46">
                <label>46</label>
                <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Schneider</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Pao</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Pea</surname><given-names>R. D.</given-names></name>, </person-group><chapter-title>“Predicting students’ learning outcomes using eye-tracking data”</chapter-title> in <source>LAK ’13: Proceedings of the Third International Conference on Learning Analytics and Knowledge</source>, <person-group person-group-type="editor"><name name-style="western"><surname>Suthers</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Verbert</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Duval</surname><given-names>E.</given-names></name></person-group>, Eds. (<publisher-name>Association for Computing Machinery</publisher-name>, <publisher-loc>New York</publisher-loc>, <year>2013</year>).</mixed-citation>
              </ref>
              <ref id="bib47">
                <label>47</label>
                <mixed-citation publication-type="web"><person-group person-group-type="author"><name name-style="western"><surname>Guterstam</surname><given-names>A.</given-names></name></person-group>, <article-title>Other People’s Gaze Encoded as Implied Motion in the Human Brain</article-title> (<year>2020</year>). <pub-id pub-id-type="doi">10.6084/m9.figshare.12184848.v1</pub-id> (23 April 2020).</mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
