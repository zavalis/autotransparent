<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2023-02-07T13:03:16Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:7446801" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:7446801</identifier>
        <datestamp>2020-08-26</datestamp>
        <setSpec>ploscomp</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS Comput. Biol</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">ploscomp</journal-id>
              <journal-title-group>
                <journal-title>PLoS Computational Biology</journal-title>
              </journal-title-group>
              <issn pub-type="ppub">1553-734X</issn>
              <issn pub-type="epub">1553-7358</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, CA USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC7446801</article-id>
              <article-id pub-id-type="pmcid">PMC7446801</article-id>
              <article-id pub-id-type="pmc-uid">7446801</article-id>
              <article-id pub-id-type="pmid">32764745</article-id>
              <article-id pub-id-type="pmid">32764745</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pcbi.1007659</article-id>
              <article-id pub-id-type="publisher-id">PCOMPBIOL-D-20-00042</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cellular Neuroscience</subject>
                      <subj-group>
                        <subject>Synaptic Plasticity</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Developmental Neuroscience</subject>
                      <subj-group>
                        <subject>Synaptic Plasticity</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Cell Biology</subject>
                    <subj-group>
                      <subject>Cellular Types</subject>
                      <subj-group>
                        <subject>Animal Cells</subject>
                        <subj-group>
                          <subject>Neurons</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cellular Neuroscience</subject>
                      <subj-group>
                        <subject>Neurons</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Anatomy</subject>
                    <subj-group>
                      <subject>Nervous System</subject>
                      <subj-group>
                        <subject>Synapses</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Medicine and Health Sciences</subject>
                  <subj-group>
                    <subject>Anatomy</subject>
                    <subj-group>
                      <subject>Nervous System</subject>
                      <subj-group>
                        <subject>Synapses</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Physiology</subject>
                    <subj-group>
                      <subject>Electrophysiology</subject>
                      <subj-group>
                        <subject>Neurophysiology</subject>
                        <subj-group>
                          <subject>Synapses</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Neurophysiology</subject>
                      <subj-group>
                        <subject>Synapses</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Computer and Information Sciences</subject>
                  <subj-group>
                    <subject>Neural Networks</subject>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Neural Networks</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cellular Neuroscience</subject>
                      <subj-group>
                        <subject>Neuronal Plasticity</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Computer and Information Sciences</subject>
                  <subj-group>
                    <subject>Systems Science</subject>
                    <subj-group>
                      <subject>Dynamical Systems</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Physical Sciences</subject>
                  <subj-group>
                    <subject>Mathematics</subject>
                    <subj-group>
                      <subject>Systems Science</subject>
                      <subj-group>
                        <subject>Dynamical Systems</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Anatomy</subject>
                    <subj-group>
                      <subject>Nervous System</subject>
                      <subj-group>
                        <subject>Neuroanatomy</subject>
                        <subj-group>
                          <subject>Neural Pathways</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Medicine and Health Sciences</subject>
                  <subj-group>
                    <subject>Anatomy</subject>
                    <subj-group>
                      <subject>Nervous System</subject>
                      <subj-group>
                        <subject>Neuroanatomy</subject>
                        <subj-group>
                          <subject>Neural Pathways</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Neuroanatomy</subject>
                      <subj-group>
                        <subject>Neural Pathways</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Physical Sciences</subject>
                  <subj-group>
                    <subject>Mathematics</subject>
                    <subj-group>
                      <subject>Algebra</subject>
                      <subj-group>
                        <subject>Linear Algebra</subject>
                        <subj-group>
                          <subject>Eigenvalues</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Achieving stable dynamics in neural circuits</article-title>
                <alt-title alt-title-type="running-head">Achieving stable neural dynamics</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4330-1201</contrib-id>
                  <name>
                    <surname>Kozachkov</surname>
                    <given-names>Leo</given-names>
                  </name>
                  <role content-type="https://casrai.org/credit/">Conceptualization</role>
                  <role content-type="https://casrai.org/credit/">Formal analysis</role>
                  <role content-type="https://casrai.org/credit/">Software</role>
                  <role content-type="https://casrai.org/credit/">Writing – original draft</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff002">
                    <sup>2</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff003">
                    <sup>3</sup>
                  </xref>
                  <xref ref-type="author-notes" rid="econtrib001">
                    <sup>‡</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1011-5234</contrib-id>
                  <name>
                    <surname>Lundqvist</surname>
                    <given-names>Mikael</given-names>
                  </name>
                  <role content-type="https://casrai.org/credit/">Conceptualization</role>
                  <role content-type="https://casrai.org/credit/">Formal analysis</role>
                  <role content-type="https://casrai.org/credit/">Software</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff002">
                    <sup>2</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff004">
                    <sup>4</sup>
                  </xref>
                  <xref ref-type="author-notes" rid="econtrib001">
                    <sup>‡</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Slotine</surname>
                    <given-names>Jean-Jacques</given-names>
                  </name>
                  <role content-type="https://casrai.org/credit/">Conceptualization</role>
                  <role content-type="https://casrai.org/credit/">Supervision</role>
                  <role content-type="https://casrai.org/credit/">Validation</role>
                  <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff003">
                    <sup>3</sup>
                  </xref>
                  <xref ref-type="author-notes" rid="econtrib001">
                    <sup>‡</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0582-6958</contrib-id>
                  <name>
                    <surname>Miller</surname>
                    <given-names>Earl K.</given-names>
                  </name>
                  <role content-type="https://casrai.org/credit/">Conceptualization</role>
                  <role content-type="https://casrai.org/credit/">Project administration</role>
                  <role content-type="https://casrai.org/credit/">Supervision</role>
                  <role content-type="https://casrai.org/credit/">Validation</role>
                  <role content-type="https://casrai.org/credit/">Writing – original draft</role>
                  <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff002">
                    <sup>2</sup>
                  </xref>
                  <xref ref-type="author-notes" rid="econtrib001">
                    <sup>‡</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor001">*</xref>
                </contrib>
              </contrib-group>
              <aff id="aff001">
                <label>1</label>
                <addr-line>The Picower Institute for Learning &amp; Memory, Massachusetts Institute of Technology (MIT), Cambridge, Massachusetts, United States of America</addr-line>
              </aff>
              <aff id="aff002">
                <label>2</label>
                <addr-line>Department of Brain &amp; Cognitive Sciences, Massachusetts Institute of Technology (MIT), Cambridge, Massachusetts, United States of America</addr-line>
              </aff>
              <aff id="aff003">
                <label>3</label>
                <addr-line>Nonlinear Systems Laboratory, Massachusetts Institute of Technology (MIT), Cambridge, Massachusetts, United States of America</addr-line>
              </aff>
              <aff id="aff004">
                <label>4</label>
                <addr-line>Department of Psychology, Stockholm University, Stockholm, Sweden</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>Haith</surname>
                    <given-names>Adrian M</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>Johns Hopkins University, UNITED STATES</addr-line>
              </aff>
              <author-notes>
                <fn fn-type="COI-statement" id="coi001">
                  <p>The authors have declared that no competing interests exist.</p>
                </fn>
                <fn fn-type="other" id="econtrib001">
                  <p>‡ LK and ML share first authorship on this work. JJS and EKM are joint senior principal investigators on this work.</p>
                </fn>
                <corresp id="cor001">* E-mail: <email>ekmiller@mit.edu</email></corresp>
              </author-notes>
              <pub-date pub-type="epub">
                <day>7</day>
                <month>8</month>
                <year>2020</year>
              </pub-date>
              <pub-date pub-type="collection">
                <month>8</month>
                <year>2020</year>
              </pub-date>
              <volume>16</volume>
              <issue>8</issue>
              <elocation-id>e1007659</elocation-id>
              <history>
                <date date-type="received">
                  <day>14</day>
                  <month>1</month>
                  <year>2020</year>
                </date>
                <date date-type="accepted">
                  <day>27</day>
                  <month>6</month>
                  <year>2020</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2020 Kozachkov et al</copyright-statement>
                <copyright-year>2020</copyright-year>
                <copyright-holder>Kozachkov et al</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
                </license>
              </permissions>
              <self-uri content-type="pdf" xlink:href="pcbi.1007659.pdf"/>
              <abstract>
                <p>The brain consists of many interconnected networks with time-varying, partially autonomous activity. There are multiple sources of noise and variation yet activity has to eventually converge to a stable, reproducible state (or sequence of states) for its computations to make sense. We approached this problem from a control-theory perspective by applying contraction analysis to recurrent neural networks. This allowed us to find mechanisms for achieving stability in multiple connected networks with biologically realistic dynamics, including synaptic plasticity and time-varying inputs. These mechanisms included inhibitory Hebbian plasticity, excitatory anti-Hebbian plasticity, synaptic sparsity and excitatory-inhibitory balance. Our findings shed light on how stable computations might be achieved despite biological complexity. Crucially, our analysis is not limited to analyzing the stability of fixed geometric objects in state space (e.g points, lines, planes), but rather the stability of state trajectories which may be complex and time-varying.</p>
              </abstract>
              <abstract abstract-type="summary">
                <title>Author summary</title>
                <p>Stability is essential for any complex system including, and perhaps especially, the brain. The brain’s neural networks are highly dynamic and noisy. Activity fluctuates from moment to moment and can be highly variable. Yet it is critical that these networks reach a consistent state (or sequence of states) for their computations to make sense. Failures in stability have consequences ranging from mild (e.g incorrect decisions) to severe (disease states). In this paper we use tools from control theory and dynamical systems theory to find mechanisms which produce stability in recurrent neural networks (RNNs). We show that a kind of “unlearning” (inhibitory Hebbian and excitatory anti-Hebbian plasticity), balance of excitation and inhibition, and sparse anatomical connectivity all lead to stability. Crucially, we focus on the stability of neural <italic>trajectories</italic>. This is different from traditional studies of stability of fixed points or planes. We do not assess <italic>what</italic> trajectories our networks will follow but, rather, when these trajectories will all converge towards each other to achieve stability.</p>
              </abstract>
              <funding-group>
                <award-group id="award001">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000025</institution-id>
                      <institution>National Institute of Mental Health</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>R37MH087027</award-id>
                  <principal-award-recipient>
                    <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0582-6958</contrib-id>
                    <name>
                      <surname>Miller</surname>
                      <given-names>Earl K.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <award-group id="award002">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000006</institution-id>
                      <institution>Office of Naval Research</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>MURI N00014-16-1-2832</award-id>
                  <principal-award-recipient>
                    <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0582-6958</contrib-id>
                    <name>
                      <surname>Miller</surname>
                      <given-names>Earl K.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <award-group id="award003">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
                      <institution>National Science Foundation</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>1809314</award-id>
                  <principal-award-recipient>
                    <name>
                      <surname>Slotine</surname>
                      <given-names>Jean-Jacques</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <award-group id="award004">
                  <funding-source>
                    <institution>The Picower Institute Faculty Innovation Fund</institution>
                  </funding-source>
                  <principal-award-recipient>
                    <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0582-6958</contrib-id>
                    <name>
                      <surname>Miller</surname>
                      <given-names>Earl K.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <funding-statement>This work was supported by NIMH R37MH087027, The MIT Picower Institute Innovation Fund, ONR MURI N00014-16-1-2832, and Swedish Research Council Starting Grant 2018-04197. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <fig-count count="4"/>
                <table-count count="1"/>
                <page-count count="15"/>
              </counts>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>PLOS Publication Stage</meta-name>
                  <meta-value>vor-update-to-uncorrected-proof</meta-value>
                </custom-meta>
                <custom-meta>
                  <meta-name>Publication Update</meta-name>
                  <meta-value>2020-08-19</meta-value>
                </custom-meta>
                <custom-meta id="data-availability">
                  <meta-name>Data Availability</meta-name>
                  <meta-value>All detailed proofs of main results are found in the appendix. Simulations (Figs <xref ref-type="fig" rid="pcbi.1007659.g002">2</xref> and <xref ref-type="fig" rid="pcbi.1007659.g003">3</xref>) were performed in Python. Code to reproduce the figures is available at [<ext-link ext-link-type="uri" xlink:href="https://github.com/kozleo/stable_dynamics">https://github.com/kozleo/stable_dynamics</ext-link>]. Numerical integration was performed using sdeint, an open-source collection of numerical algorithms for performing integrations of stochastic ordinary differential equations.</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
            <notes>
              <title>Data Availability</title>
              <p>All detailed proofs of main results are found in the appendix. Simulations (Figs <xref ref-type="fig" rid="pcbi.1007659.g002">2</xref> and <xref ref-type="fig" rid="pcbi.1007659.g003">3</xref>) were performed in Python. Code to reproduce the figures is available at [<ext-link ext-link-type="uri" xlink:href="https://github.com/kozleo/stable_dynamics">https://github.com/kozleo/stable_dynamics</ext-link>]. Numerical integration was performed using sdeint, an open-source collection of numerical algorithms for performing integrations of stochastic ordinary differential equations.</p>
            </notes>
          </front>
          <body>
            <sec sec-type="intro" id="sec001">
              <title>Introduction</title>
              <p>Behavior emerges from complex neural dynamics unfolding over time in multi-area brain networks. Even in tightly controlled experimental settings, these neural dynamics often vary between identical trials [<xref rid="pcbi.1007659.ref001" ref-type="bibr">1</xref>,<xref rid="pcbi.1007659.ref002" ref-type="bibr">2</xref>]. This can be due to a variety of factors including variability in membrane potentials, inputs, plastic changes due to recent experience and so on. Yet, in spite of these fluctuations, brain networks must achieve computational stability: despite being “knocked around” by plasticity and noise, the behavioral output of the brain on two experimentally identical trials needs to be similar. How is this stability achieved?</p>
              <p>Stability has played a central role in computational neuroscience since the 1980’s, with the advent of models of associative memory that stored neural activation patterns as stable point attractors [<xref rid="pcbi.1007659.ref003" ref-type="bibr">3</xref>–<xref rid="pcbi.1007659.ref007" ref-type="bibr">7</xref>], although researchers were thinking about the brain’s stability since as early as the 1950’s [<xref rid="pcbi.1007659.ref008" ref-type="bibr">8</xref>]. The vast majority of this work is concerned with the stability of activity around points, lines, or planes in neural state space [<xref rid="pcbi.1007659.ref009" ref-type="bibr">9</xref>,<xref rid="pcbi.1007659.ref010" ref-type="bibr">10</xref>]. However, recent neurophysiological studies have revealed that in many cases, single-trial neural activity is highly dynamic, and therefore potentially inconsistent with a static attractor viewpoint [<xref rid="pcbi.1007659.ref001" ref-type="bibr">1</xref>,<xref rid="pcbi.1007659.ref011" ref-type="bibr">11</xref>]. Consequently, there has been a number of recent studies—both computational and experimental—which focus more broadly on the stability of neural <italic>trajectories</italic> [<xref rid="pcbi.1007659.ref012" ref-type="bibr">12</xref>,<xref rid="pcbi.1007659.ref013" ref-type="bibr">13</xref>], which may be complex and time-varying.</p>
              <p>While these studies provide important empirical results and intuitions, they do not offer analytical insight into mechanisms for achieving stable trajectories in recurrent neural networks. Nor do they offer insights into achieving such stability in plastic (or multi-modal) networks. Here we focus on finding conditions that guarantee stable trajectories in recurrent neural networks and thus shed light onto how stable trajectories might be achieved <italic>in vivo</italic>.</p>
              <p>To do so, we used contraction analysis, a concept developed in control theory [<xref rid="pcbi.1007659.ref014" ref-type="bibr">14</xref>]. Unlike a chaotic system where perturbations and distortions can be amplified over time, the population activity of a contracting network will converge towards the same trajectory, thus achieving stable dynamics (<xref ref-type="fig" rid="pcbi.1007659.g001">Fig 1</xref>). One way to understand contraction is to represent the state of a network at a given time as a point in the network’s ‘state-space’, for instance the space spanned by the possible firing rates of all the networks’ neurons. This state-space has the same number of dimensions as the number of units <italic>n</italic> in the network. A particular pattern of neural firing rates corresponds to a point in this state-space. This point moves in the <italic>n</italic> dimensions as the firing rates change and traces out a trajectory over time.</p>
              <fig id="pcbi.1007659.g001" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pcbi.1007659.g001</object-id>
                <label>Fig 1</label>
                <caption>
                  <title>Cartoon demonstrating the contraction property.</title>
                  <p>In a network with <italic>N</italic> neural units and <italic>S</italic> dynamic synaptic weights, the network activity can be described a trajectory over time in an (<italic>N</italic> + <italic>S</italic>)-dimensional space. In a contracting system all such trajectories will converge exponentially <italic>in some metric</italic> towards each other over time, regardless of initial conditions. In other words, the distance between any two trajectories shrinks to zero—potentially after transient divergence (as shown).</p>
                </caption>
                <graphic xlink:href="pcbi.1007659.g001"/>
              </fig>
              <p>In a contracting network, all such trajectories converge. These contracting dynamics have previously been used in several applications, including neural networks with winner take all dynamics [<xref rid="pcbi.1007659.ref015" ref-type="bibr">15</xref>,<xref rid="pcbi.1007659.ref016" ref-type="bibr">16</xref>], in a model of action-selection in the basal ganglia [<xref rid="pcbi.1007659.ref017" ref-type="bibr">17</xref>], and to explain how neural synchronization can protect from noise [<xref rid="pcbi.1007659.ref018" ref-type="bibr">18</xref>]. Here, we instead explore how contraction can be achieved generally in more complex recurrent neural networks (RNNs) including those with plastic weights. We used RNNs that received arbitrary time-varying inputs and had synapses that changed on biologically relevant timescales [<xref rid="pcbi.1007659.ref019" ref-type="bibr">19</xref>–<xref rid="pcbi.1007659.ref021" ref-type="bibr">21</xref>]. Our analysis reveals several novel classes of mechanisms that produced contraction including inhibitory Hebbian plasticity, excitatory anti-Hebbian plasticity, excitatory-inhibitory balance, and sparse connectivity. For the first two parts of the Results section, we focus on contraction of <italic>both</italic> neural activity <italic>and</italic> components of the weight matrix (<xref ref-type="fig" rid="pcbi.1007659.g001">Fig 1</xref>). For the remaining parts of the Results section, we hold the weights fixed (i.e they become parameters, not variables) and focus on contraction of neural activity alone.</p>
            </sec>
            <sec sec-type="results" id="sec002">
              <title>Results</title>
              <p>The main tool we used to characterize contraction was the logarithmic norm (also known as a matrix measure). The formal definition of the logarithmic norm is as follows (from [<xref rid="pcbi.1007659.ref022" ref-type="bibr">22</xref>] section 2.2.2): let <bold>A</bold> be a matrix in <italic>C</italic><sup><italic>n</italic>×<italic>n</italic></sup> and ∥⋅∥<sub><italic>i</italic></sub> be an induced matrix norm on <italic>C</italic><sup><italic>n</italic>×<italic>n</italic></sup>. Then the corresponding logarithmic norm is the function <inline-formula id="pcbi.1007659.e001"><alternatives><graphic xlink:href="pcbi.1007659.e001.jpg" id="pcbi.1007659.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:math></alternatives></inline-formula> defined by
<disp-formula id="pcbi.1007659.e002"><alternatives><graphic xlink:href="pcbi.1007659.e002.jpg" id="pcbi.1007659.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mrow><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>→</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>ϵ</mml:mi></mml:mfrac></mml:mrow></mml:math></alternatives></disp-formula></p>
              <p>In the same way that different vector norms induce different matrix norms, different vector norms also induce different logarithmic norms. Two important logarithmic norms which we use throughout the paper are those induced by the vector 1-norm and the vector 2-norm:
<disp-formula id="pcbi.1007659.e003"><alternatives><graphic xlink:href="pcbi.1007659.e003.jpg" id="pcbi.1007659.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="4em"/></mml:mrow><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:math></alternatives></disp-formula>
Where <italic>λ</italic><sub><italic>max</italic></sub> denotes the largest eigenvalue. To study the contraction properties of RNNs, we applied the logarithmic norm to the RNN’s <italic>Jacobians</italic>. The Jacobian of a dynamical system is a matrix essentially describing the local ‘traffic laws’ of nearby trajectories of the system in its state space. More formally, it is the matrix of partial derivatives describing how a change in any system variable impacts the <italic>rate of change</italic> of every other variable in the system. It was shown in [<xref rid="pcbi.1007659.ref014" ref-type="bibr">14</xref>] that if the logarithmic norm of the Jacobian is negative then all nearby trajectories are funneled towards one another (see <xref ref-type="supplementary-material" rid="pcbi.1007659.s001">S1A Text</xref> Section 1.2 for technical review). This, in turn, implies that <italic>all</italic> trajectories are funneled towards one another at rate called the <italic>contraction rate</italic>. The contraction rate and the logarithmic norm are related as follows: the maximum value attained by the absolute value of logarithmic norm of the Jacobian along the network’s trajectory <italic>is</italic> the contraction rate. In other words, if the logarithmic norm of the Jacobian is upper bounded by some negative number −<italic>c</italic>, where <italic>c</italic> &gt; 0, then the contraction rate is simply <italic>c</italic>.</p>
              <p>Importantly, the above description can be generalized to different <italic>metrics</italic>. A metric is a symmetric, positive definite matrix which generalizes the notion of Euclidean distance. Every invertible coordinate transformation <bold>y</bold> = <bold>θx</bold> yields a metric <bold>M</bold> = <bold>θ</bold><sup>T</sup><bold>θ</bold>. To see this, consider the squared norm of ∥<bold>y</bold>∥<sup>2</sup> = <bold><italic>y</italic></bold><sup><bold><italic>T</italic></bold></sup><bold><italic>y</italic></bold> = <bold>x</bold><sup><bold><italic>T</italic></bold></sup><bold>θ</bold><sup><bold><italic>T</italic></bold></sup><bold>θx</bold> = <bold>x</bold><sup><bold><italic>T</italic></bold></sup><bold>Mx</bold>. Thus, the norm of <bold>y</bold> is related to the norm of <bold>x</bold> through the metric <bold>M</bold>. If one can find metric in which the network is contracting—in the sense that its Jacobian has negative logarithmic norm–this implies contraction for <italic>all</italic> coordinate systems. This makes contraction analysis useful for analyzing systems where exponential convergence of trajectories is preceded by transient divergence (<xref ref-type="fig" rid="pcbi.1007659.g001">Fig 1</xref>) as in recent models of motor cortex [<xref rid="pcbi.1007659.ref023" ref-type="bibr">23</xref>,<xref rid="pcbi.1007659.ref024" ref-type="bibr">24</xref>]. In this case, it is usually possible to find a coordinate system in which the convergence of trajectories is ‘pure’. For example, linear stable systems were recently used in the motor control literature to find initial conditions which produce the most energetic neural response [<xref rid="pcbi.1007659.ref023" ref-type="bibr">23</xref>] They are ‘purely’ contracting in a metric defined by the eigenvectors of the weight matrix (see Example 5.1 in [<xref rid="pcbi.1007659.ref014" ref-type="bibr">14</xref>]) but transiently diverging in the identity metric (i.e <bold>M</bold> = <bold>I</bold>). Note that the identity metric corresponds to <bold>θ</bold> = <bold>I</bold>, which is simply the original, untransformed coordinate system.</p>
              <sec id="sec003">
                <title>Inhibitory hebbian plasticity &amp; excitatory anti-Hebbian plasticity produce contraction</title>
                <p>It is known that certain forms of synaptic plasticity can quickly lead to extreme instabilities if left unchecked [<xref rid="pcbi.1007659.ref009" ref-type="bibr">9</xref>,<xref rid="pcbi.1007659.ref025" ref-type="bibr">25</xref>]. Thus, the same feature that can aid learning can also yield chaotic neural dynamics if not regulated. It is not known how the brain resolves this dilemma. A growing body of evidence—both experimental and computational—suggests that inhibitory plasticity (that is, the strengthening of inhibitory synapses) can stabilize neural dynamics while simultaneously allowing for learning/training in neural circuits [<xref rid="pcbi.1007659.ref026" ref-type="bibr">26</xref>–<xref rid="pcbi.1007659.ref028" ref-type="bibr">28</xref>]. By using the Jacobian analysis outlined above, we found that inhibitory Hebbian synaptic plasticity (as well as excitatory anti-Hebbian plasticity) indeed leads to stable dynamics in neural circuits. Specifically, we considered neural networks of the following common form:
<disp-formula id="pcbi.1007659.e004"><alternatives><graphic xlink:href="pcbi.1007659.e004.jpg" id="pcbi.1007659.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives><label>(1)</label></disp-formula>
where the term <italic>x</italic><sub><italic>i</italic></sub> denotes the ‘activation’ of neuron <italic>i</italic> as a function of time. Here we follow other authors [<xref rid="pcbi.1007659.ref023" ref-type="bibr">23</xref>] and interpret <italic>x</italic><sub><italic>i</italic></sub> as the <italic>deviation</italic> from the baseline firing rate of neuron <italic>i</italic>. Note that this interpretation assumes that the baseline firing rates are positive–thus allowing for x to be negative—and large enough so that <italic>baseline</italic> + <italic>x</italic> &gt; 0. The term <italic>W</italic><sub><italic>ij</italic></sub> denotes the weight between neurons <italic>i</italic> and <italic>j</italic> the term <italic>h</italic>(<italic>x</italic><sub><italic>i</italic></sub>) captures the dynamics neuron <italic>i</italic> would have in the absence of synaptic input, including self-feedback terms arising from the diagonal elements of the weight matrix—in other words, the dynamics neuron <italic>i</italic> would have if for all <italic>i</italic> and <italic>j</italic>, <italic>W</italic><sub>ij</sub> = 0. The term being summed represents the weighted contribution of all the neurons in the network on the activity of neuron <italic>i</italic>. Finally, the term <italic>u</italic><sub><italic>i</italic></sub>(<italic>t</italic>) represents external input into neuron <italic>i</italic>.</p>
                <p>We did not constrain the inputs into the RNN (except that they were not infinite) and we did not specify the particular form of <italic>h</italic>(<italic>x</italic><sub><italic>i</italic></sub>) except that it should be a leak term (i.e has a negative derivative for all <italic>x</italic>, see <xref ref-type="supplementary-material" rid="pcbi.1007659.s001">S1A Text</xref> Section 2.2.4, e.g <italic>h</italic>(<italic>x</italic><sub><italic>i</italic></sub>) = −<italic>x</italic><sub><italic>i</italic></sub>). Furthermore, we made no assumptions regarding the relative timescales of synaptic and neural activity. Synaptic dynamics were treated on an equal footing as neural dynamics. We considered synaptic plasticity of the following correlational form [<xref rid="pcbi.1007659.ref029" ref-type="bibr">29</xref>]:
<disp-formula id="pcbi.1007659.e005"><alternatives><graphic xlink:href="pcbi.1007659.e005.jpg" id="pcbi.1007659.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>γ</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives><label>(2)</label></disp-formula>
where the term <italic>k</italic><sub><italic>ij</italic></sub> &gt; 0 is the learning rate for each synapse and <italic>γ</italic>(<italic>t</italic>) &gt; 0 is a decay factor for each synapse. For technical reasons outlined in the appendix (<xref ref-type="supplementary-material" rid="pcbi.1007659.s001">S1A Text</xref> Section 3), we restricted <bold>K</bold>, the matrix containing the learning rates <italic>k</italic><sub><italic>ij</italic></sub>, to be positive semi-definite, symmetric, and have positive entries. A particular example of <bold>K</bold> satisfying these constraints is to have the learning rates of all synapses to be equal (i.e. <italic>k</italic><sub><italic>ij</italic></sub> = <italic>k</italic> &gt; 0).</p>
                <p>Before we show that (<xref ref-type="disp-formula" rid="pcbi.1007659.e005">2</xref>) leads to overall synaptic and neural contraction, it’s useful to spend some time interpreting this plasticity. Since <italic>W</italic><sub><italic>ij</italic></sub> can be positive or negative (corresponding to excitatory and inhibitory synapses, respectively), and <italic>x</italic><sub><italic>i</italic></sub><italic>x</italic><sub><italic>j</italic></sub> can be positive or negative (corresponding to correlated and anticorrelated neurons, respectively), there are four cases to consider. We summarize these cases in <xref rid="pcbi.1007659.t001" ref-type="table">Table 1</xref> and discuss them in details below. By Hebbian plasticity we refer to the increase of synaptic efficiency between correlated neurons [<xref rid="pcbi.1007659.ref030" ref-type="bibr">30</xref>]. In the context of simple neural networks with scalar weights, as we consider here, efficiency refers to the absolute value |<italic>w</italic>| of a weight. Thus, for excitatory synapses, (<xref ref-type="disp-formula" rid="pcbi.1007659.e005">2</xref>) in fact describes <italic>anti</italic>-Hebbian plasticity, because the positive synaptic weight becomes less positive (and thus less efficient) between correlated neurons and more positive (thus more efficient) for anticorrelated neurons. For inhibitory synapses, (<xref ref-type="disp-formula" rid="pcbi.1007659.e005">2</xref>) describes Hebbian plasticity because the direction of synaptic weight change is negative between correlated neurons, and thus the synapse becomes <italic>more</italic> efficient [<xref rid="pcbi.1007659.ref031" ref-type="bibr">31</xref>,<xref rid="pcbi.1007659.ref032" ref-type="bibr">32</xref>], while for anticorrelated neurons the direction of synaptic weight change is positive, and thus the synapse becomes less efficient. Plasticity of this form produced contracting neural and synaptic dynamics regardless of the initial values of the weights and neural activity (Figs <xref ref-type="fig" rid="pcbi.1007659.g002">2</xref> and <xref ref-type="fig" rid="pcbi.1007659.g003">3</xref>). The black trace of <xref ref-type="fig" rid="pcbi.1007659.g003">Fig 3A</xref> shows that this is not simply due to the weights decaying to 0. Thus, this plasticity is not only contraction preserving, it is contracting <italic>ensuring</italic>. Furthermore, we showed that the network is contracting in a non-identity metric (which we derive from the system parameters in <bold>K</bold>), opening up the possibility of transient divergent dynamics in the identity metric, as seen in the modelling of motor dynamics [<xref rid="pcbi.1007659.ref023" ref-type="bibr">23</xref>].</p>
                <table-wrap id="pcbi.1007659.t001" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pcbi.1007659.t001</object-id>
                  <label>Table 1</label>
                  <caption>
                    <title>Summary of the effect of the plasticity described in <xref ref-type="disp-formula" rid="pcbi.1007659.e005">Eq (2)</xref> on excitatory and inhibitory for correlated or anticorrelated pre and post synaptic neurons.</title>
                  </caption>
                  <alternatives>
                    <graphic id="pcbi.1007659.t001g" xlink:href="pcbi.1007659.t001"/>
                    <table frame="hsides" rules="groups">
                      <colgroup span="1">
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                      </colgroup>
                      <tbody>
                        <tr>
                          <td align="left" rowspan="2" colspan="1"/>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>Correlated Neurons</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>Anticorrelated Neurons</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1"><italic>x</italic><sub><italic>i</italic></sub><italic>x</italic><sub><italic>j</italic></sub> &gt; 0</td>
                          <td align="left" rowspan="1" colspan="1"><italic>x</italic><sub><italic>i</italic></sub><italic>x</italic><sub><italic>j</italic></sub> &lt; 0</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>Excitatory Synapse</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">Less Efficient</td>
                          <td align="left" rowspan="1" colspan="1">More Efficient</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1"><italic>w</italic> &gt; 0</td>
                          <td align="left" rowspan="1" colspan="1">Δ|<italic>w</italic>| &lt; 0</td>
                          <td align="left" rowspan="1" colspan="1">Δ|<italic>w</italic>| &gt; 0</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>Inhibitory Synapse</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">More Efficient</td>
                          <td align="left" rowspan="1" colspan="1">Less Efficient</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1"><italic>w</italic> &lt; 0</td>
                          <td align="left" rowspan="1" colspan="1">Δ|<italic>w</italic>| &gt; 0</td>
                          <td align="left" rowspan="1" colspan="1">Δ|<italic>w</italic>| &lt; 0</td>
                        </tr>
                      </tbody>
                    </table>
                  </alternatives>
                </table-wrap>
                <fig id="pcbi.1007659.g002" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pcbi.1007659.g002</object-id>
                  <label>Fig 2</label>
                  <caption>
                    <title>Contracting dynamics of neural and synaptic activity.</title>
                    <p>Euclidean distances between synaptic and neural trajectories demonstrate exponential shrinkage over time. The top row of panels shows the activation of a randomly selected neural unit (black) and synapse (blue) across two simulations (dotted and solid line). The bottom row shows the average Euclidean distance in state space for the whole population across simulations with distinct, randomized starting conditions. Leftmost Panel: Simulations of a contracting system where only starting conditions differ over simulations. Center Panel: the same as in Leftmost but with an additional random pulse perturbation in one of the two simulations indicated by a red background shading. Rightmost Panel: same as in Center Panel but with additional sustained noise, unique to each simulation.</p>
                  </caption>
                  <graphic xlink:href="pcbi.1007659.g002"/>
                </fig>
                <fig id="pcbi.1007659.g003" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pcbi.1007659.g003</object-id>
                  <label>Fig 3</label>
                  <caption>
                    <p>The anti-Hebbian plasticity pushes the weight matrix towards symmetry. (Left) Plotted are the spectral norms (largest singular value) of the overall weight matrix as well as the anti-symmetric part of that matrix. Since every square matrix can be uniquely decomposed as the sum of a symmetric and anti-symmetric component—0.5*(W+W’) and 0.5*(W-W‘), respectively—the teal curve decaying to zero implies that the matrix becomes symmetric. The black trace shows the spectral norm of the overall weight matrix. If this quantity does not decay to zero, it implies that not all the weights have decayed to zero. On the right, we plot the largest eigenvalue of the symmetric part of W. A prerequisite for overall contraction of the network is that this quantity be less than or equal to the ‘leak-rate’ of the individual neurons. The dotted line shows our theoretical upper bound for this quantity, and the solid line shows the actual value of taken from a simulation (see <xref ref-type="sec" rid="sec008">Methods</xref>).</p>
                  </caption>
                  <graphic xlink:href="pcbi.1007659.g003"/>
                </fig>
                <p>To explain how inhibitory Hebbian plasticity and excitatory anti-Hebbian plasticity work to produce contraction across a whole network, we needed to deal with the network in a holistic fashion, not by analyzing the dynamics of single neurons. To do so, we conceptualized RNNs with dynamic synapses as a single system formed by combining two subsystems, a neural subsystem and a synaptic subsystem. We showed that the above plasticity rule led the neural and synaptic subsystems to be independently contracting. Thus contraction analysis of the overall system then boiled down to examining the interactions between these subsystems [<xref rid="pcbi.1007659.ref033" ref-type="bibr">33</xref>].</p>
                <p>We found that this plasticity works like an interface between these systems. It produces two distinct effects that push networks toward contraction. First, it makes the synaptic weight matrix symmetric (<xref ref-type="fig" rid="pcbi.1007659.g003">Fig 3A</xref>, red trace). This means that the weight between neuron <italic>i</italic> to <italic>j</italic> is the same as <italic>j</italic> to <italic>i</italic>. We showed this by using the fact that every matrix can be written as the sum of a purely symmetric matrix and a purely anti-symmetric matrix. An anti-symmetric matrix is one where the <italic>ij</italic> element is the negative of the <italic>ji</italic> element (<italic>i</italic>.<italic>e</italic>. <italic>W</italic><sub><italic>ij</italic></sub> = −<italic>W</italic><sub><italic>ji</italic></sub>) and all the diagonal elements are zero. We then showed that anti-Hebbian plasticity shrinks the anti-symmetric part of the weight matrix to zero, implying that the weight matrix becomes symmetric. The symmetry of the weight matrix ‘cancels out’ off-diagonals in the Jacobian matrix (see <xref ref-type="supplementary-material" rid="pcbi.1007659.s001">S1A Text</xref> Section 3) of the overall neural-synaptic system. Loosely speaking, off-diagonal terms in the Jacobian represent potentially destabilizing cross-talk between the two subsystems. Furthermore, anti-Hebbian plasticity makes the weight matrix negative semi-definite. This means that all its eigenvalues are less than or equal to zero (<xref ref-type="fig" rid="pcbi.1007659.g003">Fig 3</xref>).</p>
              </sec>
              <sec id="sec004">
                <title>Sparse connectivity pushes networks toward contraction</title>
                <p>Synaptic connectivity in the brain is extraordinarily sparse. The adult human brain contains at least 10<sup>11</sup> neurons yet each neuron forms and receives on average only 10<sup>3</sup>−10<sup>4</sup> synaptic connections [<xref rid="pcbi.1007659.ref034" ref-type="bibr">34</xref>]. If the brain’s neurons were all-to-all connected this number would be on the order of 10<sup>11</sup> synaptic connections per neuron (<inline-formula id="pcbi.1007659.e006"><alternatives><graphic xlink:href="pcbi.1007659.e006.jpg" id="pcbi.1007659.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mtext>synaptic</mml:mtext><mml:mspace width="4pt"/><mml:mtext>connections</mml:mtext><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtext>neurons</mml:mtext><mml:mo>]</mml:mo></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>). Even in local patches of cortex, such as we model here, connectivity is far from all-to-all; cortical circuits are sparse [<xref rid="pcbi.1007659.ref035" ref-type="bibr">35</xref>]. Our analyses revealed that sparse connectivity helps produce global network contraction for many types of synaptic plasticity.</p>
                <p>To account for the possibility that some synapses may have much slower plasticity than others (and can thus be treated as synapses with fixed amplitude), we made a distinction between the total number of synapses and the total number of <italic>plastic</italic> synapses. These plastic synapses then changed on a similar time-scale as the neural firing rates. By neural dynamics, we mean the change in neural activity as a function of time. We analyzed RNNs with the structure:
<disp-formula id="pcbi.1007659.e007"><alternatives><graphic xlink:href="pcbi.1007659.e007.jpg" id="pcbi.1007659.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives><label>(3)</label></disp-formula>
Where <italic>h</italic><sub><italic>i</italic></sub>(<italic>x</italic><sub><italic>i</italic></sub>) is a nonlinear leak term (see <xref ref-type="supplementary-material" rid="pcbi.1007659.s001">S1A Text</xref> Section 2.2.4), and <italic>r</italic>(<italic>x</italic><sub><italic>j</italic></sub>) is a nonlinear activation function. The RNNs analyzed in this section are identical to those analyzed in the previous section, with the exception of the <italic>r</italic> terms, which we constrained to be linear. Under the assumption that the plastic synapses have a ‘forgetting term’, we show in the appendix (<xref ref-type="supplementary-material" rid="pcbi.1007659.s001">S1A Text</xref> Section 4) that if the following equation is satisfied for every neuron, then the overall network is contracting:
<disp-formula id="pcbi.1007659.e008"><alternatives><graphic xlink:href="pcbi.1007659.e008.jpg" id="pcbi.1007659.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></alternatives><label>(4)</label></disp-formula>
where <italic>p</italic><sub><italic>i</italic></sub> denotes the total number of afferent synapses into neuron <italic>i</italic> and <italic>α</italic><sub><italic>i</italic></sub> denotes the fraction of afferent <italic>plastic</italic> synapses into neuron <italic>i</italic>. The term <italic>w</italic><sub><italic>max</italic></sub> refers to the maximum possible absolute efficiency of any single synapse. That is, <italic>w</italic><sub><italic>max</italic></sub> = max<sub><italic>i</italic>,<italic>j</italic></sub> |<italic>w</italic><sub><italic>ij</italic></sub>|. Similarly, the term <italic>r</italic><sub><italic>max</italic></sub> refers to the maximum possible absolute value of <italic>r</italic>. That is, <italic>r</italic><sub><italic>max</italic></sub> = max<sub><italic>i</italic>,<italic>t</italic></sub> |<italic>r</italic><sub><italic>i</italic></sub>(<italic>t</italic>)|. The term <italic>β</italic><sub><italic>i</italic></sub> denotes the contraction rate of the <italic>i</italic><sup><italic>th</italic></sup> isolated neuron. That is, <inline-formula id="pcbi.1007659.e009"><alternatives><graphic xlink:href="pcbi.1007659.e009.jpg" id="pcbi.1007659.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>-</mml:mo><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Recall from the introduction that the contraction rate measures how quickly the trajectories of a contracting system reconvene after perturbation. Finally, <italic>g</italic><sub><italic>max</italic></sub> refers to the maximum gain of any neuron in the network. That is, <inline-formula id="pcbi.1007659.e010"><alternatives><graphic xlink:href="pcbi.1007659.e010.jpg" id="pcbi.1007659.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. Note that because <italic>β</italic><sub><italic>i</italic></sub> is a positive number by assumption, it is always possible to decrease <italic>p</italic><sub><italic>i</italic></sub> to the point where (<xref ref-type="disp-formula" rid="pcbi.1007659.e008">4</xref>) is satisfied. Of course, it is possible that the only value of <italic>p</italic><sub><italic>i</italic></sub> that satisfies (<xref ref-type="disp-formula" rid="pcbi.1007659.e008">4</xref>) is the trivial solution <italic>p</italic><sub><italic>i</italic></sub> = 0, which corresponds to removing all interconnections between neurons. Since these neurons are assumed to be contracting in isolation, the network is trivially contracting. However, if the term inside the parentheses of (<xref ref-type="disp-formula" rid="pcbi.1007659.e008">4</xref>) is small enough, or <italic>β</italic><sub><italic>i</italic></sub> is large enough, intermediate value of <italic>p</italic><sub><italic>i</italic></sub> can be found which satisfy the inequality. Because increasing the sparsity of a network corresponds to decreasing <italic>p</italic><sub><italic>i</italic></sub>, we may conclude that increasing the sparsity of connections pushes the system in the direction of contraction. Note that (<xref ref-type="disp-formula" rid="pcbi.1007659.e008">4</xref>) also implies that the faster the individual neurons are contracting (i.e. the larger <italic>β</italic><sub><italic>i</italic></sub> is), the denser you can connect them with other neurons while still preserving overall contraction.</p>
                <p>Up to now we have focused our analysis on the case where synaptic weights vary on a timescale comparable to neurons, and must therefore be factored into the stability analysis. For the next two sections, we’ll apply contraction analysis to neural network in the case where the weights may be regarded as <italic>fixed</italic> relative to the neural dynamics (i.e. there is a separation of timescales).</p>
              </sec>
              <sec id="sec005">
                <title>E-I balance leads to contraction in static RNNs</title>
                <p>Apart from making connections sparse, one way to ensure contraction is to make synaptic weights small. This can be seen for the case with static synapses by setting <italic>α</italic><sub><italic>i</italic></sub> = 0 in the section above, where <italic>W</italic><sub><italic>max</italic></sub> now has to be small to ensure contraction. Intuitively, this is because very small weights mean that neurons cannot exert much influence on one another. If the neurons are stable before interconnection, they will remain so. Since strong synaptic weights are commonly observed in the brain, we were more interested in studying when contraction can arise irrespective of weight amplitude. Negative and positive synaptic currents are approximately balanced in biology [<xref rid="pcbi.1007659.ref036" ref-type="bibr">36</xref>–<xref rid="pcbi.1007659.ref038" ref-type="bibr">38</xref>]. We reasoned that such balance might allow much larger weight amplitudes while still preserving contraction since most of the impact of such synapses cancel and the net effect small. This was indeed the case. To show this, we studied the same RNN as in the section above, while assuming additionally that the weights are static. In particular, we show in the appendix (<xref ref-type="supplementary-material" rid="pcbi.1007659.s001">S1A Text</xref> Section 5) that contraction can be assessed by studying the eigenvalues of the <italic>symmetric</italic> part of <bold>W</bold> (i.e. <inline-formula id="pcbi.1007659.e011"><alternatives><graphic xlink:href="pcbi.1007659.e011.jpg" id="pcbi.1007659.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>).</p>
                <p>Before we discuss the above result in detail, it is useful here to quickly review some facts about the stability of nonlinear systems as compared to the stability of linear systems. In particular, the fact that the eigenvalues of <bold>W</bold> are only informative for assessing contraction in regions where the dynamics may be regarded as linear. This is because in linear time-variant (LTI) systems (i.e. <inline-formula id="pcbi.1007659.e012"><alternatives><graphic xlink:href="pcbi.1007659.e012.jpg" id="pcbi.1007659.e012g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M12"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:math></alternatives></inline-formula>) stability is completely characterized in terms of the eigenvalues of <bold>A</bold>. However, this is <italic>not</italic> true for nonlinear systems, even those of the linear time-varying form <inline-formula id="pcbi.1007659.e013"><alternatives><graphic xlink:href="pcbi.1007659.e013.jpg" id="pcbi.1007659.e013g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M13"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>.</mml:mo></mml:math></alternatives></inline-formula> To see this, consider the following counter-example (from [<xref rid="pcbi.1007659.ref039" ref-type="bibr">39</xref>], section 4.2.2):
<disp-formula id="pcbi.1007659.e014"><alternatives><graphic xlink:href="pcbi.1007659.e014.jpg" id="pcbi.1007659.e014g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M14"><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives><label>(5)</label></disp-formula></p>
                <p>The eigenvalues of <bold>A</bold>(<italic>t</italic>) are (−1, 1) for all time, however one can verify by direct evalution that the solution of this system satisfies <italic>y</italic> = <italic>y</italic>(0)<italic>e</italic><sup>−<italic>t</italic></sup>, <inline-formula id="pcbi.1007659.e015"><alternatives><graphic xlink:href="pcbi.1007659.e015.jpg" id="pcbi.1007659.e015g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M15"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>y</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> which is unstable along <italic>x</italic>. However, it can be shown straightforwardly that if the eigenvalues of the <italic>symmetric</italic> part of <bold>A</bold>(<italic>t</italic>) are all negative, then the system is stable [<xref rid="pcbi.1007659.ref039" ref-type="bibr">39</xref>]. This fact underlies our analysis, and highlights the reason why the eigenvalues of the symmetric part of <bold>W</bold> are important for stability.</p>
                <p>Returning to our results, we show that if excitatory to inhibitory connections are of equal amplitude (and opposite sign) as inhibitory to excitatory connections, they will not interfere negatively with stability—regardless of amplitude (see <xref ref-type="supplementary-material" rid="pcbi.1007659.s001">S1A Text</xref> Section 5). This is because connections between inhibitory and excitatory units will be in the off-diagonal of the overall weight matrix and get cancelled out when computing the symmetric part. As an intuitive example, consider a two-neuron circuit made of one excitatory neuron and one inhibitory neuron connected recurrently (as in [<xref rid="pcbi.1007659.ref040" ref-type="bibr">40</xref>], <xref ref-type="fig" rid="pcbi.1007659.g001">Fig 1A</xref>). Assume that the overall weight matrix has the following structure:
<disp-formula id="pcbi.1007659.e016"><alternatives><graphic xlink:href="pcbi.1007659.e016.jpg" id="pcbi.1007659.e016g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M16"><mml:mi mathvariant="bold">W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>w</mml:mi></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mi>w</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>w</mml:mi></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mi>w</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
                <p>When taking the symmetric part of this matrix, the off-diagonal elements cancel out, leaving only the diagonal elements to consider. Since the eigenvalues of a diagonal matrix are simply its diagonal elements, we can conclude that if the excitatory and inhibitory subpopulations are independently contracting (<italic>w</italic> is less than the contraction rate of an isolated neuron), then overall contraction is guaranteed. It is straightforward to generalize this simple two-neuron example to circuits achieving E-I balance through interacting <italic>populations</italic> (see <xref ref-type="supplementary-material" rid="pcbi.1007659.s001">S1A Text</xref> Section 5). It is also straightforward to generalize to the case where E-I and I-E connections do not cancel out exactly neuron by neuron, but rather they cancel out in a statistical sense where the mean amplitudes are matched. Another way to view this E-I balance is in the framework of combinations of contracting systems (<xref ref-type="fig" rid="pcbi.1007659.g004">Fig 4</xref>). It is known that combining independently contracting systems in negative feedback preserves contraction [<xref rid="pcbi.1007659.ref014" ref-type="bibr">14</xref>]. We show that E-I balance actually translates to this negative feedback and thus can preserve contraction.</p>
                <fig id="pcbi.1007659.g004" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pcbi.1007659.g004</object-id>
                  <label>Fig 4</label>
                  <caption>
                    <title>Cartoon illustrating the combination properties of contracting systems.</title>
                    <p>A) Two isolated, contracting systems. The Jacobian of the overall system is block diagonal, with all zeros on the off-diagonal—corresponding to the fact that the systems are not connected. B) If one of the systems is connected to the other in a feedforward manner, the overall Jacobian is changed by the presence of non-zero terms on the bottom left block—corresponding to the connections going from the ‘top’ system to the ‘bottom’ system. This Jacobian may not be negative definite. However, it is known that a coordinate change exists which will make it negative definite. Thus, hierarchically connected contracting systems are contracting. C) If the systems are reciprocally connected, the system may lose its contracting properties (for example in the case of positive feedback). However, it is known that if the feedforward connections (blue) are ‘equal and opposite’ to the feedback connections (green) then the overall system is contracting. We use this property in the main text to prove that inhibitory Hebbian plasticity and excitatory anti-Hebbian plasticity lead to contracting neural circuits.</p>
                  </caption>
                  <graphic xlink:href="pcbi.1007659.g004"/>
                </fig>
              </sec>
              <sec id="sec006">
                <title>Relation to other models with fading memory</title>
                <p>As can be seen in <xref ref-type="fig" rid="pcbi.1007659.g002">Fig 2</xref>, contracting systems have ‘fading memories’. This means that past events will affect the current state, but that the impact of a transient perturbation gradually decays over time. Consider the transient input in <xref ref-type="fig" rid="pcbi.1007659.g002">Fig 2</xref> (red panel) presented on only one of the two trials to the network. Because the input is only present on one trial and not the other we call it a perturbation. When this perturbation occurs, the trajectories of the two trials become separated. However, after the disturbance is removed, the distance between the network’s trajectories starts shrinking back to zero again. Thus, the network does not hold onto the memory of the perturbation indefinitely—the memory fades away. A similar property has been used in Echo State Networks (ESNs) and liquid state machines (LSMs) to perform useful brain-inspired computations [<xref rid="pcbi.1007659.ref041" ref-type="bibr">41</xref>,<xref rid="pcbi.1007659.ref042" ref-type="bibr">42</xref>]. These networks are an alternative to classical attractor models in which neural computations are performed by entering stable states rather than by ‘fading memories’ of external inputs [<xref rid="pcbi.1007659.ref043" ref-type="bibr">43</xref>].</p>
                <p>While there are several distinctions between the networks described above and ESNs (e.g. ESNs are typically discrete time dynamical systems, rather than continuous), we show in the appendix (<xref ref-type="supplementary-material" rid="pcbi.1007659.s001">S1A Text</xref> Section 5.1) that they are a special case of the networks considered here. We show this for ESNs as opposed to LSMs because LSMs are typically implemented on integrate and fire neurons which, because of the spike reset, have a sharp discontinuity in their dynamics—making them unamenable to contraction analysis.</p>
                <p>By highlighting the link between contraction and ESNs, we demonstrate that the contracting neural networks considered here are in principle capable of performing useful and interesting neural computations. In other words, the strong stability properties of contracting neural networks do not automatically prohibit them from doing interesting computations. By working within the framework of contraction analysis we were able to study networks both with dynamic synapses and non-identity metrics—a much broader model space than allowed by the standard ESN framework.</p>
              </sec>
            </sec>
            <sec sec-type="conclusions" id="sec007">
              <title>Discussion</title>
              <p>We studied a fundamental question in neuroscience: How do neural circuits maintain stable dynamics in the presence of disturbance, noisy inputs and plastic change? We approached this problem from the perspective of dynamical systems theory, in light of the recent successes of understand neural circuits as dynamical systems [<xref rid="pcbi.1007659.ref044" ref-type="bibr">44</xref>]. We focused on <italic>contracting</italic> dynamical systems, which are yet largely unexplored in neuroscience, as a solution to the problem outlined above. We did so for three reasons:</p>
              <list list-type="order">
                <list-item>
                  <p>Contracting systems can be <italic>input-driven</italic>. This is important because neural circuits are typically bombarded with time-varying inputs either from the environment or from other brain areas. Previous stability analyses have focused primarily on the stability of RNNs without time-varying input. These analyses are most insightful in situations where the input into a circuit can be approximated as either absent or constant. However, naturalistic stimuli tend to be highly time-varying and complex [<xref rid="pcbi.1007659.ref045" ref-type="bibr">45</xref>].</p>
                </list-item>
                <list-item>
                  <p>Contracting systems are robust to noise and disturbances. Perturbations to a contracting system are forgotten at the rate of the contraction and noise therefore does not stack up over time. Importantly, the rate of forgetting (i.e the contraction rate) does not change with the size of the perturbation. Thus dynamic stability can co-exist with high trial-to-trial variability in contracting neural networks, as observed in biology.</p>
                </list-item>
                <list-item>
                  <p>Contracting systems can be combined with one another in ways that preserve contraction (<xref ref-type="fig" rid="pcbi.1007659.g004">Fig 4</xref>). This is not true of most dynamical systems which can easily ‘blow up’ when connected in feedback with one another [<xref rid="pcbi.1007659.ref008" ref-type="bibr">8</xref>]. This combination property is important as it is increasingly clear that cognitive functions such as working memory or attention are distributed in multiple cortical and sub-cortical regions [<xref rid="pcbi.1007659.ref046" ref-type="bibr">46</xref>,<xref rid="pcbi.1007659.ref047" ref-type="bibr">47</xref>]. In particular, prefrontal cortex has been suggested as a hub that can reconfigure the cortical effective network based on task demands [<xref rid="pcbi.1007659.ref048" ref-type="bibr">48</xref>]. Brain networks must therefore be able to effectively reconfigure themselves on a fast time-scale without loss of stability. Most attempts in modelling cognition, for instance working memory, tend to utilize single and often autonomous networks. Contracting networks display a combination of input-driven and autonomous dynamics, and thus have key features necessary for combining modules into flexible and distributed networks.</p>
                </list-item>
              </list>
              <p>To understand what mechanisms lead to contraction in neural circuits, we applied contraction analysis to RNNs. For RNNs with static weights, we found that the well- known Echo State Networks are a special case of a contracting network. Since realistic synapses are complex dynamical systems in their own right, we went one step further and asked when neural circuits with dynamic synapses would be contracting. We found that inhibitory Hebbian plasticity as well as excitatory anti-Hebbian plasticity and synaptic sparsity all lead to contraction in a broad class of RNNs.</p>
              <p>Inhibitory plasticity has recently been the focus of many experimental and computational studies due to its stabilizing nature as well as its capacity for facilitating nontrivial computations in neural circuits [<xref rid="pcbi.1007659.ref027" ref-type="bibr">27</xref>,<xref rid="pcbi.1007659.ref028" ref-type="bibr">28</xref>,<xref rid="pcbi.1007659.ref049" ref-type="bibr">49</xref>]. It is known to give rise to excitatory-inhibitory balance and has been implicated as the mechanism behind many experimental findings such as sparse firing rates in cortex [<xref rid="pcbi.1007659.ref028" ref-type="bibr">28</xref>]. Similarly, anti-Hebbian plasticity exists across many brain areas and species, such as salamander and rabbit retina [<xref rid="pcbi.1007659.ref031" ref-type="bibr">31</xref>], rat hippocampus [<xref rid="pcbi.1007659.ref050" ref-type="bibr">50</xref>,<xref rid="pcbi.1007659.ref051" ref-type="bibr">51</xref>], electric fish electrosensory lobe [<xref rid="pcbi.1007659.ref052" ref-type="bibr">52</xref>] and mouse prefrontal cortex [<xref rid="pcbi.1007659.ref053" ref-type="bibr">53</xref>]. Anti-Hebbian dynamics can give rise to sparse neural codes which decrease correlations between neural activity and increase overall stimulus representation in the network [<xref rid="pcbi.1007659.ref054" ref-type="bibr">54</xref>]. Because of this on-line decorrelation property, anti-Hebbian plasticity has also been implicated in predictive coding [<xref rid="pcbi.1007659.ref031" ref-type="bibr">31</xref>,<xref rid="pcbi.1007659.ref052" ref-type="bibr">52</xref>]. Our findings suggest that it also increase the stability of networks.</p>
              <p>For more general forms of synaptic dynamics, we showed that synaptic sparsity pushes RNNs towards being contracting. This aligns well with the experimental observation that synaptic connectivity is typically extremely sparse in the brain. Our results suggest that sparsity may be one factor pushing the brain towards dynamical stability. It is therefore interesting that synapses are regulated by homeostatic processes where synapses neighboring an upregulated synapse are immediately downregulated [<xref rid="pcbi.1007659.ref055" ref-type="bibr">55</xref>]. On the same note, we also observed that balancing the connections between excitatory and inhibitory populations leads to contraction. Balance between excitatory and inhibitory synaptic inputs are often observed in biology [<xref rid="pcbi.1007659.ref036" ref-type="bibr">36</xref>–<xref rid="pcbi.1007659.ref038" ref-type="bibr">38</xref>], and could thus serve contractive stability purposes. Related computational work on spiking networks has suggested that balanced synaptic currents leads to fast response properties, efficient coding, increased robustness of function and can support complex dynamics related to movements [<xref rid="pcbi.1007659.ref021" ref-type="bibr">21</xref>,<xref rid="pcbi.1007659.ref056" ref-type="bibr">56</xref>–<xref rid="pcbi.1007659.ref058" ref-type="bibr">58</xref>].</p>
              <p>A main advantage to our approach is that it provides provable certificates of global contractive stability for nonlinear, time-varying RNNs with synaptic plasticity. This distinguishes it from previous works where—while very interesting and useful—stability is experimentally observed, but not proven [<xref rid="pcbi.1007659.ref012" ref-type="bibr">12</xref>]. In some cases [<xref rid="pcbi.1007659.ref023" ref-type="bibr">23</xref>,<xref rid="pcbi.1007659.ref024" ref-type="bibr">24</xref>], linear stability around the origin is proven (which implies that there is a contraction region around the origin) but the size of this region is neither established nor sought after. Indeed, one future direction we are pursuing is the question of: <italic>given an RNN</italic>, <italic>can one provide a certificate of contractive stability in a region</italic>? An answer to this question would shed light on the stability properties of known RNN models in the literature (e.g. trained RNNs, biologically-detailed spiking models, etc.).</p>
              <p>Experimental neuroscience is moving in the direction of studying many interacting neural circuits simultaneously. This is fueled by the expanding capabilities of recording multiple areas simultaneously in vivo and study their interactions. This increases the need for multi-modal cognitive models. We therefore anticipate that the presented work can provide a useful foundation for how cognition in noisy and distributed computational networks can be understood.</p>
            </sec>
            <sec sec-type="materials|methods" id="sec008">
              <title>Materials and methods</title>
              <p>In the interested of space and cohesion, we’ve placed all the detailed proofs of main results into the appendix. The appendix was written to be self-contained, and thus also contains additional definitions of mathematical objects used throughout the text. Simulations (Figs <xref ref-type="fig" rid="pcbi.1007659.g002">2</xref> and <xref ref-type="fig" rid="pcbi.1007659.g003">3</xref>) were performed in Python. Code to reproduce the figures is available at [<ext-link ext-link-type="uri" xlink:href="https://github.com/kozleo/stable_dynamics">https://github.com/kozleo/stable_dynamics</ext-link>]. Numerical integrating was performed using sdeint, an open-source collection of numerical algorithms for integrating stochastic ordinary differential equations.</p>
              <p><xref ref-type="fig" rid="pcbi.1007659.g002">Fig 2</xref><bold>details</bold>:</p>
              <p>All parameters and time constants in Eqs (<xref ref-type="disp-formula" rid="pcbi.1007659.e004">1</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1007659.e005">2</xref>) were set to one. The integration step-size, <italic>dt</italic>, was set to 1e-2.</p>
              <p>Initial conditions for both neural and synaptic activation were drawn uniformly between -1 and 1. Inputs into the network were generated by drawing <italic>N</italic> frequencies uniformly between <italic>dt</italic> and 100<italic>dt</italic>, phases between 0 and 2<italic>π</italic>, amplitudes between 0 and 20 and generating an N x <italic>Time</italic> vector of sinusoids with the above parameters.</p>
              <p>The perturbations of the network was achieved by adding a vector of all 10s (i.e an additive vector input into the network, with each network of the element equal to 10) to the above input on one of the trials for 100 time steps in the middle of the simulation.</p>
              <p>The noise was generated by driving each neural unit with an independent Weiner process (sigma = .2).</p>
              <p><xref ref-type="fig" rid="pcbi.1007659.g003">Fig 3</xref><bold>details</bold>:</p>
              <p>The weight matrix used was the same as in <xref ref-type="fig" rid="pcbi.1007659.g002">Fig 2</xref>, leftmost panel (without perturbation, without noise).</p>
            </sec>
            <sec sec-type="supplementary-material" id="sec009">
              <title>Supporting information</title>
              <supplementary-material content-type="local-data" id="pcbi.1007659.s001">
                <label>S1 Text</label>
                <caption>
                  <title>The supplementary appendix file contains extensive mathematical proofs of the results stated above.</title>
                  <p>We kept the appendix self-contained by restating the basic results of contraction analysis and linear algebra which we used often in our proofs.</p>
                  <p>(PDF)</p>
                </caption>
                <media xlink:href="pcbi.1007659.s001.pdf">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
            </sec>
          </body>
          <back>
            <ack>
              <p>We thank Pawel Herman for comments on an earlier version of this manuscript. We thank Michael Happ and all members of the Miller Lab for helpful discussions and suggestions.</p>
            </ack>
            <ref-list>
              <title>References</title>
              <ref id="pcbi.1007659.ref001">
                <label>1</label>
                <mixed-citation publication-type="journal"><name><surname>Lundqvist</surname><given-names>M</given-names></name>, <name><surname>Rose</surname><given-names>J</given-names></name>, <name><surname>Herman</surname><given-names>P</given-names></name>, <name><surname>Brincat</surname><given-names>SL</given-names></name>, <name><surname>Buschman</surname><given-names>TJ</given-names></name>, <name><surname>Miller</surname><given-names>EK</given-names></name>. <article-title>Gamma and Beta Bursts Underlie Working Memory</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>90</volume>: <fpage>152</fpage>–<lpage>164</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.028</pub-id>
<?supplied-pmid 26996084?><pub-id pub-id-type="pmid">26996084</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref002">
                <label>2</label>
                <mixed-citation publication-type="journal"><name><surname>Churchland</surname><given-names>MM</given-names></name>, <name><surname>Yu</surname><given-names>BM</given-names></name>, <name><surname>Cunningham</surname><given-names>JP</given-names></name>, <name><surname>Sugrue</surname><given-names>LP</given-names></name>, <name><surname>Cohen</surname><given-names>MR</given-names></name>, <name><surname>Corrado</surname><given-names>GS</given-names></name>, <etal>et al</etal><article-title>Stimulus onset quenches neural variability: A widespread cortical phenomenon</article-title>. <source>Nat Neurosci</source>. <year>2010</year>;<volume>13</volume>: <fpage>369</fpage>–<lpage>378</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2501</pub-id>
<?supplied-pmid 20173745?><pub-id pub-id-type="pmid">20173745</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref003">
                <label>3</label>
                <mixed-citation publication-type="journal"><name><surname>Hopfield</surname><given-names>JJ</given-names></name>. <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proc Natl Acad Sci</source>. <year>1982</year>;<volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id>
<?supplied-pmid 6953413?><pub-id pub-id-type="pmid">6953413</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref004">
                <label>4</label>
                <mixed-citation publication-type="journal"><name><surname>Hirsch</surname><given-names>MW</given-names></name>. <article-title>Convergent activation dynamics in continuous time networks</article-title>. <source>Neural Networks</source>. <year>1989</year> pp. <fpage>331</fpage>–<lpage>349</lpage>. <pub-id pub-id-type="doi">10.1016/0893-6080(89)90018-X</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref005">
                <label>5</label>
                <mixed-citation publication-type="journal"><name><surname>Cohen</surname><given-names>MA</given-names></name>, <name><surname>Grossberg</surname><given-names>S</given-names></name>. <article-title>Absolute Stability of Global Pattern Formation and Parallel Memory Storage by Competitive Neural Networks</article-title>. <source>IEEE Trans Syst Man Cybern</source>. <year>1983</year>;<volume>SMC-13</volume>: <fpage>815</fpage>–<lpage>826</lpage>. <pub-id pub-id-type="doi">10.1109/TSMC.1983.6313075</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref006">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>Lundqvist</surname><given-names>M</given-names></name>, <name><surname>Herman</surname><given-names>P</given-names></name>, <name><surname>Lansner</surname><given-names>A</given-names></name>. <article-title>Theta and gamma power increases and alpha/beta power decreases with memory load in an attractor network model</article-title>. <source>J Cogn Neurosci</source>. <year>2011</year>;<volume>23</volume>: <fpage>3008</fpage>–<lpage>3020</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00029</pub-id>
<?supplied-pmid 21452933?><pub-id pub-id-type="pmid">21452933</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref007">
                <label>7</label>
                <mixed-citation publication-type="journal"><name><surname>Lansner</surname><given-names>A</given-names></name>, <name><surname>Ekeberg</surname><given-names>O</given-names></name>. <article-title>Reliability and Speed of Recall in an Associative Network</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>1985</year>;<volume>PAMI-7</volume>: <fpage>490</fpage>–<lpage>498</lpage>. <pub-id pub-id-type="doi">10.1109/tpami.1985.4767688</pub-id>
<?supplied-pmid 21869287?><pub-id pub-id-type="pmid">21869287</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref008">
                <label>8</label>
                <mixed-citation publication-type="book"><name><surname>Ashby</surname><given-names>W</given-names></name>. <source>Design for a brain: The origin of adaptive behaviour</source>. <publisher-name>Chapman &amp; Hall Ltd</publisher-name>; <year>1952</year>.</mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref009">
                <label>9</label>
                <mixed-citation publication-type="book"><name><surname>Dayan</surname><given-names>P</given-names></name>, <name><surname>Abbot</surname><given-names>LF</given-names></name>. <source>Theoretical Neuroscience Computational Neuroscience</source>. <publisher-name>The MIT press</publisher-name><year>2005</year><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.019</pub-id><?supplied-pmid 18995824?></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref010">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Liu</surname><given-names>D</given-names></name>. <article-title>A comprehensive review of stability analysis of continuous-time recurrent neural networks</article-title>. <source>IEEE Trans Neural Networks Learn Syst</source>. <year>2014</year>;<volume>25</volume>: <fpage>1229</fpage>–<lpage>1262</lpage>. <pub-id pub-id-type="doi">10.1109/TNNLS.2014.2317880</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref011">
                <label>11</label>
                <mixed-citation publication-type="journal"><name><surname>Spaak</surname><given-names>E</given-names></name>, <name><surname>Watanabe</surname><given-names>K</given-names></name>, <name><surname>Funahashi</surname><given-names>S</given-names></name>, <name><surname>Stokes</surname><given-names>MG</given-names></name>. <article-title>Stable and dynamic coding for working memory in primate prefrontal cortex</article-title>. <source>J Neurosci</source>. <year>2017</year>;<volume>37</volume>: <fpage>6503</fpage>–<lpage>6516</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3364-16.2017</pub-id>
<?supplied-pmid 28559375?><pub-id pub-id-type="pmid">28559375</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref012">
                <label>12</label>
                <mixed-citation publication-type="journal"><name><surname>Laje</surname><given-names>R</given-names></name>, <name><surname>Buonomano</surname><given-names>DV</given-names></name>. <article-title>Robust timing and motor patterns by taming chaos in recurrent neural networks</article-title>. <source>Nat Neurosci</source>. <year>2013</year>;<volume>16</volume>: <fpage>925</fpage>–<lpage>933</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3405</pub-id>
<?supplied-pmid 23708144?><pub-id pub-id-type="pmid">23708144</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref013">
                <label>13</label>
                <mixed-citation publication-type="journal"><name><surname>Chaisangmongkon</surname><given-names>W</given-names></name>, <name><surname>Swaminathan</surname><given-names>SK</given-names></name>, <name><surname>Freedman</surname><given-names>DJ</given-names></name>, <name><surname>Wang</surname><given-names>XJ</given-names></name>. <article-title>Computing by Robust Transience: How the Fronto-Parietal Network Performs Sequential</article-title>, <source>Category-Based Decisions. Neuron</source>. <year>2017</year>;<volume>93</volume>: <fpage>1504</fpage>–<lpage>1517.e4</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2017.03.002</pub-id>
<?supplied-pmid 28334612?><pub-id pub-id-type="pmid">28334612</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref014">
                <label>14</label>
                <mixed-citation publication-type="journal"><name><surname>Lohmiller</surname><given-names>W</given-names></name>, <name><surname>Slotine</surname><given-names>J-JE</given-names></name>. <article-title>On Contraction Analysis for Non-linear Systems</article-title>. <source>Automatica</source>. <year>1998</year>;<volume>34</volume>: <fpage>683</fpage>–<lpage>696</lpage>. <pub-id pub-id-type="doi">10.1016/S0005-1098(98)00019-3</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref015">
                <label>15</label>
                <mixed-citation publication-type="journal"><name><surname>Rutishauser</surname><given-names>U</given-names></name>, <name><surname>Douglas</surname><given-names>RJ</given-names></name>, <name><surname>Slotine</surname><given-names>J-J</given-names></name>. <source>Collective stability of networks of winner-take-all circuits*</source>. <year>2018</year> [cited 31 Oct 2019].</mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref016">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Rutishauser</surname><given-names>U</given-names></name>, <name><surname>Slotine</surname><given-names>J-J</given-names></name>, <name><surname>Douglas</surname><given-names>R</given-names></name>. <article-title>Computation in Dynamically Bounded Asymmetric Systems</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>: <fpage>1004039</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004039</pub-id><?supplied-pmid 25617645?><pub-id pub-id-type="pmid">25617645</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref017">
                <label>17</label>
                <mixed-citation publication-type="journal"><name><surname>Girard</surname><given-names>B</given-names></name>, <name><surname>Tabareau</surname><given-names>N</given-names></name>, <name><surname>Pham</surname><given-names>QC</given-names></name>, <name><surname>Berthoz</surname><given-names>A</given-names></name>, <name><surname>Slotine</surname><given-names>J-J</given-names></name>. <article-title>Where neuroscience and dynamic system theory meet autonomous robotics: A contracting basal ganglia model for action selection</article-title>. <source>Neural Networks</source>. <year>2008</year>;<volume>21</volume>: <fpage>628</fpage>–<lpage>641</lpage>. <pub-id pub-id-type="doi">10.1016/j.neunet.2008.03.009</pub-id>
<?supplied-pmid 18495422?><pub-id pub-id-type="pmid">18495422</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref018">
                <label>18</label>
                <mixed-citation publication-type="journal"><name><surname>Tabareau</surname><given-names>N</given-names></name>, <name><surname>Slotine</surname><given-names>JJ</given-names></name>, <name><surname>Pham</surname><given-names>QC</given-names></name>. <article-title>How synchronization protects from noise</article-title>. <source>PLoS Comput Biol</source>. <year>2010</year>;<volume>6</volume>: <fpage>1</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000637</pub-id>
<?supplied-pmid 20090826?><pub-id pub-id-type="pmid">20090826</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref019">
                <label>19</label>
                <mixed-citation publication-type="journal"><name><surname>Orhan</surname><given-names>AE</given-names></name>, <name><surname>Ma</surname><given-names>WJ</given-names></name>. <article-title>A diverse range of factors affect the nature of neural representations underlying short-term memory</article-title>. <source>Nat Neurosci</source>. <year>2019</year>;<volume>22</volume>: <fpage>275</fpage>–<lpage>283</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-018-0314-y</pub-id>
<?supplied-pmid 30664767?><pub-id pub-id-type="pmid">30664767</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref020">
                <label>20</label>
                <mixed-citation publication-type="journal"><name><surname>Mongillo</surname><given-names>G</given-names></name>, <name><surname>Barak</surname><given-names>O</given-names></name>, <name><surname>Tsodyks</surname><given-names>M</given-names></name>. <article-title>Synaptic Theory of Working Memory</article-title>. <source>Science (80-)</source>. <year>2008</year>;<volume>319</volume>: <fpage>1543</fpage><pub-id pub-id-type="doi">10.1126/science.1150769</pub-id><?supplied-pmid 18339943?><pub-id pub-id-type="pmid">18339943</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref021">
                <label>21</label>
                <mixed-citation publication-type="journal"><name><surname>Lundqvist</surname><given-names>M</given-names></name>, <name><surname>Compte</surname><given-names>A</given-names></name>, <name><surname>Lansner</surname><given-names>A</given-names></name>. <article-title>Bistable, Irregular Firing and Population Oscillations in a Modular Attractor Memory Network</article-title>. <name><surname>Morrison</surname><given-names>A</given-names></name>, editor. <source>PLoS Comput Biol</source>. <year>2010</year>;<volume>6</volume>: <fpage>e1000803</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000803</pub-id><?supplied-pmid 20532199?><pub-id pub-id-type="pmid">20532199</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref022">
                <label>22</label>
                <mixed-citation publication-type="journal"><name><surname>Vidyasagar</surname><given-names>M</given-names></name>. <source>Nonlinear systems analysis</source>. <year>2002</year>.</mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref023">
                <label>23</label>
                <mixed-citation publication-type="journal"><name><surname>Hennequin</surname><given-names>G</given-names></name>, <name><surname>Vogels</surname><given-names>TP</given-names></name>, <name><surname>Gerstner</surname><given-names>W</given-names></name>. <article-title>Optimal control of transient dynamics in balanced networks supports generation of complex movements</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>82</volume>: <fpage>1394</fpage>–<lpage>1406</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.045</pub-id>
<?supplied-pmid 24945778?><pub-id pub-id-type="pmid">24945778</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref024">
                <label>24</label>
                <mixed-citation publication-type="journal"><name><surname>Stroud</surname><given-names>JP</given-names></name>, <name><surname>Porter</surname><given-names>MA</given-names></name>, <name><surname>Hennequin</surname><given-names>G</given-names></name>, <name><surname>Vogels</surname><given-names>TP</given-names></name>. <article-title>Motor primitives in space and time via targeted gain modulation in cortical networks</article-title>. <source>Nat Neurosci</source>. <year>2018</year>;<volume>21</volume>: <fpage>1774</fpage>–<lpage>1783</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-018-0276-0</pub-id>
<?supplied-pmid 30482949?><pub-id pub-id-type="pmid">30482949</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref025">
                <label>25</label>
                <mixed-citation publication-type="journal"><name><surname>Zenke</surname><given-names>F</given-names></name>, <name><surname>Gerstner</surname><given-names>W</given-names></name>, <name><surname>Ganguli</surname><given-names>S</given-names></name>. <article-title>The temporal paradox of Hebbian learning and homeostatic plasticity</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2017</year> pp. <fpage>166</fpage>–<lpage>176</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2017.03.015</pub-id>
<?supplied-pmid 28431369?><pub-id pub-id-type="pmid">28431369</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref026">
                <label>26</label>
                <mixed-citation publication-type="journal"><name><surname>Vogelsy</surname><given-names>TP</given-names></name>, <name><surname>Froemkey</surname><given-names>RC</given-names></name>, <name><surname>Doyon</surname><given-names>N</given-names></name>, <name><surname>Gilson</surname><given-names>M</given-names></name>, <name><surname>Haas</surname><given-names>JS</given-names></name>, <name><surname>Liu</surname><given-names>R</given-names></name>, <etal>et al</etal><article-title>Inhibitory synaptic plasticity: Spike timing-dependence and putative network function</article-title>. <source>Frontiers in Neural Circuits</source>. <year>2013</year><pub-id pub-id-type="doi">10.3389/fncir.2013.00119</pub-id><?supplied-pmid 23882186?><pub-id pub-id-type="pmid">23882186</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref027">
                <label>27</label>
                <mixed-citation publication-type="journal"><name><surname>Hennequin</surname><given-names>G</given-names></name>, <name><surname>Agnes</surname><given-names>EJ</given-names></name>, <name><surname>Vogels</surname><given-names>TP</given-names></name>. <article-title>Inhibitory Plasticity: Balance, Control, and Codependence</article-title>. <source>Annu Rev Neurosci</source>. <year>2017</year>;<volume>40</volume>: <fpage>557</fpage>–<lpage>579</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-072116-031005</pub-id>
<?supplied-pmid 28598717?><pub-id pub-id-type="pmid">28598717</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref028">
                <label>28</label>
                <mixed-citation publication-type="journal"><name><surname>Vogels</surname><given-names>TP</given-names></name>, <name><surname>Sprekeler</surname><given-names>H</given-names></name>, <name><surname>Zenke</surname><given-names>F</given-names></name>, <name><surname>Clopath</surname><given-names>C</given-names></name>, <name><surname>Gerstner</surname><given-names>W</given-names></name>. <article-title>Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks</article-title>. <source>Science (80-)</source>. <year>2011</year>;<volume>334</volume>: <fpage>1569</fpage>–<lpage>1573</lpage>. <pub-id pub-id-type="doi">10.1126/science.1211095</pub-id>
<?supplied-pmid 22075724?><pub-id pub-id-type="pmid">22075724</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref029">
                <label>29</label>
                <mixed-citation publication-type="journal"><name><surname>Gerstner</surname><given-names>W</given-names></name>, <name><surname>Kistler</surname><given-names>WM</given-names></name>. <article-title>Mathematical formulations of Hebbian learning</article-title>. <source>Biol Cybern</source>. <year>2002</year>;<volume>87</volume>: <fpage>404</fpage>–<lpage>415</lpage>. <pub-id pub-id-type="doi">10.1007/s00422-002-0353-y</pub-id>
<?supplied-pmid 12461630?><pub-id pub-id-type="pmid">12461630</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref030">
                <label>30</label>
                <mixed-citation publication-type="journal"><name><surname>Gerstner</surname><given-names>W</given-names></name>, <name><surname>Kistler</surname><given-names>WM</given-names></name>. <article-title>Mathematical formulations of Hebbian learning</article-title>. <source>Biol Cybern</source>. <year>2002</year>;<volume>87</volume>: <fpage>404</fpage>–<lpage>415</lpage>. <pub-id pub-id-type="doi">10.1007/s00422-002-0353-y</pub-id>
<?supplied-pmid 12461630?><pub-id pub-id-type="pmid">12461630</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref031">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>Hosoya</surname><given-names>T</given-names></name>, <name><surname>Baccus</surname><given-names>SA</given-names></name>, <name><surname>Meister</surname><given-names>M</given-names></name>. <article-title>Dynamic predictive coding by the retina</article-title>. <source>Nature</source>. <year>2005</year>;<volume>436</volume>: <fpage>71</fpage> Available: <pub-id pub-id-type="doi">10.1038/nature03689</pub-id>
<?supplied-pmid 16001064?><pub-id pub-id-type="pmid">16001064</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref032">
                <label>32</label>
                <mixed-citation publication-type="journal"><name><surname>Gerstner</surname><given-names>W</given-names></name>, <name><surname>Kistler</surname><given-names>WM</given-names></name>. <article-title>Mathematical formulations of Hebbian learning</article-title>. <source>Biol Cybern</source>. <year>2002</year>;<volume>87</volume>: <fpage>404</fpage>–<lpage>415</lpage>. <pub-id pub-id-type="doi">10.1007/s00422-002-0353-y</pub-id>
<?supplied-pmid 12461630?><pub-id pub-id-type="pmid">12461630</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref033">
                <label>33</label>
                <mixed-citation publication-type="journal"><name><surname>Slotine</surname><given-names>JJE</given-names></name>. <article-title>Modular stability tools for distributed computation and control</article-title>. <source>Int J Adapt Control Signal Process</source>. <year>2003</year>;<volume>17</volume>: <fpage>397</fpage>–<lpage>416</lpage>. <pub-id pub-id-type="doi">10.1002/acs.754</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref034">
                <label>34</label>
                <mixed-citation publication-type="other">Kandel ER, Schwartz JH, Jessell TM, Jessell D of B and MBT, Siegelbaum S, Hudspeth AJ. Principles of neural science. McGraw-hill New York; 2000.</mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref035">
                <label>35</label>
                <mixed-citation publication-type="journal"><name><surname>Song</surname><given-names>S</given-names></name>, <name><surname>Sjöström</surname><given-names>PJ</given-names></name>, <name><surname>Reigl</surname><given-names>M</given-names></name>, <name><surname>Nelson</surname><given-names>S</given-names></name>, <name><surname>Chklovskii</surname><given-names>DB</given-names></name>. <article-title>Highly nonrandom features of synaptic connectivity in local cortical circuits</article-title>. <source>PLoS Biol</source>. <year>2005</year>;<volume>3</volume>: <fpage>0507</fpage>–<lpage>0519</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.0030068</pub-id>
<?supplied-pmid 15737062?><pub-id pub-id-type="pmid">15737062</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref036">
                <label>36</label>
                <mixed-citation publication-type="journal"><name><surname>Mariño</surname><given-names>J</given-names></name>, <name><surname>Schummers</surname><given-names>J</given-names></name>, <name><surname>Lyon</surname><given-names>DC</given-names></name>, <name><surname>Schwabe</surname><given-names>L</given-names></name>, <name><surname>Beck</surname><given-names>O</given-names></name>, <name><surname>Wiesing</surname><given-names>P</given-names></name>, <etal>et al</etal><article-title>Invariant computations in local cortical networks with balanced excitation and inhibition</article-title>. <source>Nat Neurosci</source>. <year>2005</year>;<volume>8</volume>: <fpage>194</fpage>–<lpage>201</lpage>. <pub-id pub-id-type="doi">10.1038/nn1391</pub-id>
<?supplied-pmid 15665876?><pub-id pub-id-type="pmid">15665876</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref037">
                <label>37</label>
                <mixed-citation publication-type="journal"><name><surname>Wehr</surname><given-names>M</given-names></name>, <name><surname>Zador</surname><given-names>AM</given-names></name>. <article-title>Balanced inhibition underlies tuning and sharpens spike timing in auditory cortex</article-title>. <source>Nature</source>. <year>2003</year>;<volume>426</volume>: <fpage>442</fpage>–<lpage>446</lpage>. <pub-id pub-id-type="doi">10.1038/nature02116</pub-id>
<?supplied-pmid 14647382?><pub-id pub-id-type="pmid">14647382</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref038">
                <label>38</label>
                <mixed-citation publication-type="journal"><name><surname>Shu</surname><given-names>Y</given-names></name>, <name><surname>Hasenstaub</surname><given-names>A</given-names></name>, <name><surname>McCormick</surname><given-names>DA</given-names></name>. <article-title>Turning on and off recurrent balanced cortical activity</article-title>. <source>Nature</source>. <year>2003</year>;<volume>423</volume>: <fpage>288</fpage>–<lpage>293</lpage>. <pub-id pub-id-type="doi">10.1038/nature01616</pub-id>
<?supplied-pmid 12748642?><pub-id pub-id-type="pmid">12748642</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref039">
                <label>39</label>
                <mixed-citation publication-type="book"><name><surname>Slotine</surname><given-names>J-JE</given-names></name>, <name><surname>Li</surname><given-names>W</given-names></name>. <source>Applied nonlinear control</source>. <publisher-name>Prentice hall</publisher-name><publisher-loc>Englewood Cliffs, NJ</publisher-loc>; <year>1991</year>.</mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref040">
                <label>40</label>
                <mixed-citation publication-type="journal"><name><surname>Murphy</surname><given-names>BK</given-names></name>, <name><surname>Miller</surname><given-names>KD</given-names></name>. <article-title>Balanced Amplification: A New Mechanism of Selective Amplification of Neural Activity Patterns</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>61</volume>: <fpage>635</fpage>–<lpage>648</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.005</pub-id>
<?supplied-pmid 19249282?><pub-id pub-id-type="pmid">19249282</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref041">
                <label>41</label>
                <mixed-citation publication-type="journal"><name><surname>Jaeger</surname><given-names>H</given-names></name>. <article-title>The “echo state” approach to analysing and training recurrent neural networks-with an erratum note</article-title>. <source>Bonn, Ger Ger Natl Res Cent Inf Technol GMD Tech Rep</source>. <year>2001</year>;<volume>148</volume>: <fpage>13</fpage>.</mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref042">
                <label>42</label>
                <mixed-citation publication-type="other">Pascanu R, Jaeger H. A Neurodynamical Model for Working Memory. <ext-link ext-link-type="uri" xlink:href="http://www.reservoir-computing.org/organic">www.reservoir-computing.org/organic</ext-link></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref043">
                <label>43</label>
                <mixed-citation publication-type="journal"><name><surname>Buonomano</surname><given-names>DV</given-names></name>, <name><surname>Maass</surname><given-names>W</given-names></name>. <source>State-dependent computations: spatiotemporal processing in cortical networks</source>. <year>2009</year> [cited 11 Mar 2019]. <pub-id pub-id-type="doi">10.1038/nrn2558</pub-id>
<?supplied-pmid 19145235?><pub-id pub-id-type="pmid">19145235</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref044">
                <label>44</label>
                <mixed-citation publication-type="journal"><name><surname>Sussillo</surname><given-names>D</given-names></name>. <article-title>Neural circuits as computational dynamical systems</article-title>. <source>Curr Opin Neurobiol</source>. <year>2014</year>;<volume>25</volume>: <fpage>156</fpage>–<lpage>163</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2014.01.008</pub-id>
<?supplied-pmid 24509098?><pub-id pub-id-type="pmid">24509098</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref045">
                <label>45</label>
                <mixed-citation publication-type="journal"><name><surname>Van Steveninck</surname><given-names>RRDR</given-names></name>, <name><surname>Lewen</surname><given-names>GD</given-names></name>, <name><surname>Strong</surname><given-names>SP</given-names></name>, <name><surname>Koberle</surname><given-names>R</given-names></name>, <name><surname>Bialek</surname><given-names>W</given-names></name>. <source>Reproducibility and Variability in Neural Spike Trains</source>. <year>1997</year>;<volume>275</volume>.</mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref046">
                <label>46</label>
                <mixed-citation publication-type="journal"><name><surname>Chatham</surname><given-names>CH</given-names></name>, <name><surname>Badre</surname><given-names>D</given-names></name>. <article-title>Multiple gates on working memory</article-title>. <source>Curr Opin Behav Sci</source>. <year>2015</year>;<volume>1</volume>: <fpage>23</fpage>–<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1016/j.cobeha.2014.08.001</pub-id>
<?supplied-pmid 26719851?><pub-id pub-id-type="pmid">26719851</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref047">
                <label>47</label>
                <mixed-citation publication-type="journal"><name><surname>Halassa</surname><given-names>MM</given-names></name>, <name><surname>Kastner</surname><given-names>S</given-names></name>. <article-title>Thalamic functions in distributed cognitive control</article-title>. <source>Nat Neurosci</source>. <year>2017</year>;<volume>20</volume>: <fpage>1669</fpage>–<lpage>1679</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-017-0020-1</pub-id>
<?supplied-pmid 29184210?><pub-id pub-id-type="pmid">29184210</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref048">
                <label>48</label>
                <mixed-citation publication-type="journal"><name><surname>Miller</surname><given-names>EK</given-names></name>, <name><surname>Cohen</surname><given-names>JD</given-names></name>. <article-title>An Integrative Theory of Prefrontal Cortex Function</article-title>. <source>Annu Rev Neurosci</source>. <year>2001</year>;<volume>24</volume>: <fpage>167</fpage>–<lpage>202</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.167</pub-id>
<?supplied-pmid 11283309?><pub-id pub-id-type="pmid">11283309</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref049">
                <label>49</label>
                <mixed-citation publication-type="journal"><name><surname>Vogelsy</surname><given-names>TP</given-names></name>, <name><surname>Froemkey</surname><given-names>RC</given-names></name>, <name><surname>Doyon</surname><given-names>N</given-names></name>, <name><surname>Gilson</surname><given-names>M</given-names></name>, <name><surname>Haas</surname><given-names>JS</given-names></name>, <name><surname>Liu</surname><given-names>R</given-names></name>, <etal>et al</etal><article-title>Inhibitory synaptic plasticity: Spike timing-dependence and putative network function</article-title>. <source>Front Neural Circuits</source>. <year>2013</year>;<volume>7</volume>: <fpage>1</fpage>–<lpage>11</lpage>.<pub-id pub-id-type="pmid">23440175</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref050">
                <label>50</label>
                <mixed-citation publication-type="journal"><name><surname>Lisman</surname><given-names>J</given-names></name>. <article-title>A mechanism for the Hebb and the anti-Hebb processes underlying learning and memory</article-title>. <source>Proc Natl Acad Sci</source>. <year>1989</year>;<volume>86</volume>: <fpage>9574</fpage>–<lpage>9578</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.86.23.9574</pub-id>
<?supplied-pmid 2556718?><pub-id pub-id-type="pmid">2556718</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref051">
                <label>51</label>
                <mixed-citation publication-type="journal"><name><surname>Kullmann</surname><given-names>DM</given-names></name>, <name><surname>Lamsa</surname><given-names>KP</given-names></name>. <article-title>Long-term synaptic plasticity in hippocampal interneurons</article-title>. <source>Nat Rev Neurosci</source>. <year>2007</year>;<volume>8</volume>: <fpage>687</fpage>–<lpage>699</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2207</pub-id>
<?supplied-pmid 17704811?><pub-id pub-id-type="pmid">17704811</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref052">
                <label>52</label>
                <mixed-citation publication-type="journal"><name><surname>Enikolopov</surname><given-names>AG</given-names></name>, <name><surname>Abbott</surname><given-names>L</given-names></name>, <name><surname>Sawtell</surname><given-names>NB</given-names></name>. <source>Internally Generated Predictions Enhance Neural and Behavioral Detection of Sensory Stimuli in an Electric Fish</source>. <year>2018</year> [cited 1 Mar 2019]. <pub-id pub-id-type="doi">10.1016/j.neuron.2018.06.006</pub-id>
<?supplied-pmid 30001507?><pub-id pub-id-type="pmid">30001507</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref053">
                <label>53</label>
                <mixed-citation publication-type="journal"><name><surname>Ruan</surname><given-names>H</given-names></name>, <name><surname>Saur</surname><given-names>T</given-names></name>, <name><surname>Yao</surname><given-names>W-D</given-names></name>. <article-title>Dopamine-enabled anti-Hebbian timing-dependent plasticity in prefrontal circuitry</article-title>. <source>Front Neural Circuits</source>. <year>2014</year>;<volume>8</volume>: <fpage>38</fpage><pub-id pub-id-type="doi">10.3389/fncir.2014.00038</pub-id><?supplied-pmid 24795571?><pub-id pub-id-type="pmid">24795571</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref054">
                <label>54</label>
                <mixed-citation publication-type="journal"><name><surname>Földiák</surname><given-names>P</given-names></name>. <article-title>Forming sparse representations by local anti-Hebbian learning</article-title>. <source>Biol Cybern</source>. <year>1990</year>;<volume>64</volume>: <fpage>165</fpage>–<lpage>170</lpage>. <pub-id pub-id-type="doi">10.1007/BF02331346</pub-id>
<?supplied-pmid 2291903?><pub-id pub-id-type="pmid">2291903</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref055">
                <label>55</label>
                <mixed-citation publication-type="journal"><name><surname>El-Boustani</surname><given-names>S</given-names></name>, <name><surname>Ip</surname><given-names>JPK</given-names></name>, <name><surname>Breton-Provencher</surname><given-names>V</given-names></name>, <name><surname>Knott</surname><given-names>GW</given-names></name>, <name><surname>Okuno</surname><given-names>H</given-names></name>, <name><surname>Bito</surname><given-names>H</given-names></name>, <etal>et al</etal><article-title>Locally coordinated synaptic plasticity of visual cortex neurons in vivo</article-title>. <source>Science (80-)</source>. <year>2018</year>;<volume>360</volume>: <fpage>1349</fpage>–<lpage>1354</lpage>. <pub-id pub-id-type="doi">10.1126/science.aao0862</pub-id>
<?supplied-pmid 29930137?><pub-id pub-id-type="pmid">29930137</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref056">
                <label>56</label>
                <mixed-citation publication-type="journal"><name><surname>Denève</surname><given-names>S</given-names></name>, <name><surname>Machens</surname><given-names>CK</given-names></name>. <article-title>Efficient codes and balanced networks</article-title>. <source>Nat Neurosci</source>. <year>2016</year>;<volume>19</volume>: <fpage>375</fpage>–<lpage>382</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4243</pub-id>
<?supplied-pmid 26906504?><pub-id pub-id-type="pmid">26906504</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref057">
                <label>57</label>
                <mixed-citation publication-type="journal"><name><surname>Hennequin</surname><given-names>G</given-names></name>, <name><surname>Vogels</surname><given-names>TP</given-names></name>, <name><surname>Gerstner</surname><given-names>W</given-names></name>. <article-title>Optimal control of transient dynamics in balanced networks supports generation of complex movements</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>82</volume>: <fpage>1394</fpage>–<lpage>1406</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.045</pub-id>
<?supplied-pmid 24945778?><pub-id pub-id-type="pmid">24945778</pub-id></mixed-citation>
              </ref>
              <ref id="pcbi.1007659.ref058">
                <label>58</label>
                <mixed-citation publication-type="journal"><name><surname>Brunel</surname><given-names>N</given-names></name>. <article-title>Dynamics of Sparsely Connected Networks of Excitatory and Inhibitory Spiking Neurons</article-title>. <source>J Comput Neurosci</source>. <year>2000</year> Available: <ext-link ext-link-type="uri" xlink:href="https://web.stanford.edu/group/brainsinsilicon/documents/BrunelSparselyConnectedNets.pdf">https://web.stanford.edu/group/brainsinsilicon/documents/BrunelSparselyConnectedNets.pdf</ext-link></mixed-citation>
              </ref>
            </ref-list>
          </back>
          <sub-article id="pcbi.1007659.r001" article-type="aggregated-review-documents">
            <front-stub>
              <article-id pub-id-type="doi">10.1371/journal.pcbi.1007659.r001</article-id>
              <title-group>
                <article-title>Decision Letter 0</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Graham</surname>
                    <given-names>Lyle J.</given-names>
                  </name>
                  <role>Deputy Editor</role>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Haith</surname>
                    <given-names>Adrian M</given-names>
                  </name>
                  <role>Associate Editor</role>
                </contrib>
              </contrib-group>
              <permissions>
                <copyright-statement>© 2020 Graham, Haith</copyright-statement>
                <copyright-year>2020</copyright-year>
                <copyright-holder>Graham, Haith</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
                </license>
              </permissions>
              <related-article id="rel-obj001" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007659" related-article-type="reviewed-article"/>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>Submission Version</meta-name>
                  <meta-value>0</meta-value>
                </custom-meta>
              </custom-meta-group>
            </front-stub>
            <body>
              <p>
                <named-content content-type="letter-date">21 Apr 2020</named-content>
              </p>
              <p>Dear Dr. Miller,</p>
              <p>Thank you very much for submitting your manuscript "Achieving stable dynamics in neural circuits" for consideration at PLOS Computational Biology.</p>
              <p>As with all papers reviewed by the journal, your manuscript was reviewed by members of the editorial board and by two independent reviewers. We apologize for the lengthy review process for this manuscript which was due to one reviewer being unable to complete their review.</p>
              <p>The reviewers appreciated the novel perspective on an important problem, but did raise a number of concerns and felt that several parts of the paper required further clarification. In light of the reviews (below this email), we would like to invite the resubmission of a significantly-revised version that takes into account the reviewers' comments.</p>
              <p>We cannot make any decision about publication until we have seen the revised manuscript and your response to the reviewers' comments. Your revised manuscript is also likely to be sent to reviewers for further evaluation.</p>
              <p>When you are ready to resubmit, please upload the following:</p>
              <p>[1] A letter containing a detailed list of your responses to the review comments and a description of the changes you have made in the manuscript. Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out.</p>
              <p>[2] Two versions of the revised manuscript: one with either highlights or tracked changes denoting where the text has been changed; the other a clean version (uploaded as the manuscript file).</p>
              <p>Important additional instructions are given below your reviewer comments.</p>
              <p>Please prepare and submit your revised manuscript within 60 days. If you anticipate any delay, please let us know the expected resubmission date by replying to this email. Please note that revised manuscripts received after the 60-day due date may require evaluation and peer review similar to newly submitted manuscripts.</p>
              <p>Thank you again for your submission. We hope that our editorial process has been constructive so far, and we welcome your feedback at any time. Please don't hesitate to contact us if you have any questions or comments.</p>
              <p>Sincerely,</p>
              <p>Adrian M Haith</p>
              <p>Associate Editor</p>
              <p>PLOS Computational Biology</p>
              <p>Lyle Graham</p>
              <p>Deputy Editor</p>
              <p>PLOS Computational Biology</p>
              <p>***********************</p>
              <p>Reviewer's Responses to Questions</p>
              <p>
                <bold>Comments to the Authors:</bold>
              </p>
              <p>
                <bold>Please note here if the review is uploaded as an attachment.</bold>
              </p>
              <p>Reviewer #1: The authors have some interesting results that would be useful for theorists/modelers in computational neuroscience to be aware of. The usefulness would be enhanced if some extra work is done to clarify the findings and state the limits on their generality or whether they could be expanded. I point out the areas where this can be achieved below:</p>
              <p>l.57 “all such trajectories converge”: this suggests the focus is on networks with a single fixed point. Are these really the interesting ones? Is there some generalization to networks with multiple fixed points where one can discuss regions of contraction? Such networks may be more versatile and useful.</p>
              <p>Eq. 1: The description “in the absence of input from other neurons” suggests that the diagonal part of the connection matrix, W_ij is contained within the first term, h(x). Is this true? Or are you only considering W_ij without diagonal self-connection terms? If not, then “other” neurons should be modified to include self-feedback.</p>
              <p>Eq. 1: It is important to indicate the range of “x” as it appears to enter the expression as if it is a firing rate which can only be non-negative. Are you allowing negative neural outputs?</p>
              <p>Eq.2: Following the above comment, if “x” represents firing rate so can only be non-negative, then the equation for changes in connection strengths prevent any stable positive connections from arising (the initial term is negative or zero and the second term has opposite sign to W). This seems wrong. Similarly if x can be negative some discussion should be made about the meaning of negative rates and how the change in synaptic strength should depend on such rates. Perhaps zero “x” corresponds to a threshold for the rate-dependent switch between synaptic depression and potentiation and the formulae would all work with shifted “x” and threshold subtracted, to represent non-negative firing rates?</p>
              <p>l. 130: I think the logic is correct but the explanation a little too brief:</p>
              <p>According to Hebb’s rule if a neuron helps cause another neuron to fire then a change in connection enhances the likelihood of such a causal event occurring again. i.e. Hebb’s rule is all about positive feedback (the synapse changes to enhance existing correlations).</p>
              <p>For excitatory neurons that means strengthening following correlated spikes.</p>
              <p>For an inhibitory neuron, it can help a downstream neuron fire by not spiking at the same time.</p>
              <p>Also, a decrease in the inhibitory synapse will help the postsynaptic neuron fire.</p>
              <p>So, firing at different times (anticorrelated) should lead to a decrease in inhibitory strength (Wij more positive) as it is then more likely that the inhibitory cell helps the excitatory cell fire -- or conversely, firing at the same time leads to an increase in inhibitory strength (Wij more negative as stated here). Perhaps a short-hand could be that a “positive feedback” process is Hebbian and a negative feedback process is anti-Hebbian? However, I am still confused by the values of “x” and their sign being perhaps negative. It may help to describe this as negative x meaning low-rate and positive x meaning high-rate. But then this rule would allow for strong plasticity when both pre- and post-synaptic rates are minimal (x very negative) in contradiction to known plasticity mechanisms. These issues should be discussed.</p>
              <p>Figure 3: Please define “Norm of Matrix Component” via an equation</p>
              <p>l. 146 typo (“of”)</p>
              <p>l. 173-174 The calculation is incorrect. You are comparing connections per neuron with total connections in the brain. The numerator should be multiplied by number of neurons (10^11).</p>
              <p>l. 197: It seems that there are two separate (but linked) systems operating on very different time scales. On the fast time scale, for a given weight matrix, one can ask if the neural firing rates of the system are contracting. At times (especially when Echo State networks are compared or the requirements for weight matrices) it seems like the goal is to generate contracting networks in terms of neural firing rates. However, l. 197 suggests that the discussions of contraction are in the components of the weight matrix. The authors should clearly separate discussions and results concerning weight matrices that contract (such that the network produces a reliable/reproducible – perhaps in this case temporally variable – set of synaptic strengths) versus whether the firing rates of neurons follow relatively stable trajectories.</p>
              <p>Eq. 4: The terms here need a little more description and explanation of how they enter. While the main point is trivial (decreasing p can lead to stability) the terms like “g_max” appear to be related to the maximum derivative of connection strength with respect to neural activation (Eq. 4.0.3) rather than what is generally termed “gain” of a neuron (gradient of f-I curve). Also, does w_max refer to maximum strength of plastic synapses (in which case why is it not in the “alpha” term) or is it simply the maximum value of the assigned fixed synapses? Perhaps explain why increasing the fraction of static to plastic synapses allows for a larger w_max.</p>
              <p>l. 211: “has” not “have”</p>
              <p>l. 218 “impact of such synapses cancel” but there are time-differences in practice that commonly lead to oscillations. I think “cancel” is an overstatement.</p>
              <p>l.224-225 “will not interfere with stability”: in fact the antisymmetric part can enhance stability (this is easy to see from the determinant of a 2x2 matrix). So, while the intent is correct, the point could be made a little more clearly. That is, if the symmetric part is stable the complete matrix will be, however, if the symmetric part is not stable the antisymmetric part could be sufficient to generate stability overall.</p>
              <p>l. 232 type “that” should be “the”</p>
              <p>l. 232 “the off-diagonal elements cancel out”. No! They impact the determinant of the matrix and hence can impact the values (and signs) of the eigenvalues, therefore impacting stability. But again, I think only in the sense of enhancing stability.</p>
              <p>Fig. 4: legend is pretty opaque and stops half-way through a sentence in my version.</p>
              <p>Section 3.4 on Echo State networks: this seems a little strange and overly expounded, especially when discussing the discrete-time nature of Echo State networks when there are almost identical Liquid State networks that gained prominence at the same time (sometimes in joint reviews) with the major difference being the Liquid State networks operate in continuous time. Why focus on Echo State not Liquid State given the latter is more similar to the current paper?</p>
              <p>Discussion: I think papers that train some chaotic networks, for example Laje and Buonomano 2012, should be mentioned at a minimum and, even better, discussed in terms of contracting trajectories given that is what they appear to do. How is their plasticity rule or network similar or dissimilar to any mentioned in this paper that lead to contraction? Or is there system not contracting? Or only contracting in a portion of phase space? It would be nice to see this sort of analysis (even if only descriptive) applied to a few prior networks that appear to achieve contraction</p>
              <p>l. 346 “increases” not “increase”</p>
              <p>Reviewer #2: Overview: The authors analyze the mechanisms that neural circuits employ in order to maintain stable dynamics in face of perturbations and plasticity through the lens of a control theoretic perspective, namely, contraction analysis. The study indicates that mechanisms such as inhibitory Hebbian plasticity, excitatory anti-Hebbian plasticity, synaptic sparsity and E-I balance can all lead to stability within the considered networks.</p>
              <p>Comments:</p>
              <p>• Contribution: The authors point out that stability of neural circuits has been considered in the literature widely, but what is less explored is how despite trial to trial variability, the brain manages to produce robust computations and therefore robust responses. The method considered here i.e., contraction analysis, states that the trajectories produced by population activity converge quickly following perturbation and therefore are robust in the face of small perturbations.</p>
              <p>The paper brings together certain established analytical methods (contraction theory) to treat a long standing problem in computational neuroscience. While some of the factors discussed (inhibitory plasticity, E-I balance) have been studied in the context of achieving stability, they are brought together here under a common analytical framework. In this sense, the paper could constitute a useful contribution to the literature. However, the technical novelty is at time ambiguous and the presentation could be more precise in this regard. I elaborate on specific concerns below:</p>
              <p>• Main concerns.</p>
              <p>o The authors frequently refer to computations in the Appendix section to narrate their findings. Important quantifications (not the extensive mathematical derivations) should be introduced in line with the claims made in the Result section to make the narrative more cohesive. For instance, in line 80 the authors introduce a matrix measure called the ‘logarithmic norm of the Jacobian’ which happens to be an important metric for the discussions that follow, but no concise mathematical form of it is provided to aid the readers.</p>
              <p>o The authors have identified two quantitative tools to characterize contraction: contraction rate and the Jacobian of the network. However, in line 85 the authors have expressed one in terms of the other. Does that mean that the quantifications are interchangeable, and the analysis can be interpreted in terms of any one? Some explicit clarifications regarding this might be helpful.</p>
              <p>o In line 86, the authors state that the idea of contraction analysis using Jacobian can be generalized to different metrics. What are some of these metrics? What are the metrics that can generate pure convergence of trajectories? What is the identity metric in line 138? A discussion along these lines including the mathematical expression of these metrics will enrich the study.</p>
              <p>o The types of perturbations considered for the study are not mentioned explicitly. Does the nature of convergence change depending upon the perturbation inflicted on the network?</p>
              <p>o In line 211 the authors state that if alpha_i = 0, then W_max must be reduced to ensure equation 4 i.e., contraction in the network. My question here is in this case do you consider a densely connected network? Since, equation 4 states that if you are sparse enough you can have strong couplings irrespective of whether the synapses are static or dynamic.</p>
              <p>o How should Figure 4 in the manuscript be interpreted? I understand that it points to the fact that E-I balance in networks (often employed by achieving E-I balance in subnetworks) facilitate stability and therefore convergence of trajectories under perturbation. But much of the preceding and succeeding discussion assumes prior knowledge of the matter. Introducing some key equations back into the text here will help justify the claims made.</p>
              <p>o Are there any neural mechanisms that have been studied experimentally to ensure stability and robustness in computation but cannot be explained solely through contraction analysis?</p>
              <p>• Minor Comments:</p>
              <p>o Figure 1: Dimensionality of the cartoon state space. The text states that it is a (N+S) dimensional space for a network with N neural units and S dynamic synaptic weights, but the figure does not match up with it or perhaps need to be restructured. What is the index j here?</p>
              <p>o Figure 2: Legends are not visible. Consider using larger font size.</p>
              <p>o Line 157: There is no red trace in Figure 3A. Is it a typographical error or a missing trace?</p>
              <p>o Caption for Figure 4 is incomplete.</p>
              <p>o The Materials and Methods section does not add much in this version of the manuscript except where to find the relevant code. Consider expanding this section to include information that are vital to interpretation of the results.</p>
              <p>o What is H in the supplementary section (Pg 13). Is it simply H = x_t x_t^T?</p>
              <p>**********</p>
              <p>
                <bold>Have all data underlying the figures and results presented in the manuscript been provided?</bold>
              </p>
              <p>Large-scale datasets should be made available via a public repository as described in the <italic>PLOS Computational Biology</italic>
<ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/ploscompbiol/s/data-availability">data availability policy</ext-link>, and numerical data that underlies graphs or summary statistics should be provided in spreadsheet form as supporting information.</p>
              <p>Reviewer #1: Yes</p>
              <p>Reviewer #2: No: Simulations are specified, but data is not submitted.</p>
              <p>**********</p>
              <p>PLOS authors have the option to publish the peer review history of their article (<ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
              <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
              <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link ext-link-type="uri" xlink:href="https://www.plos.org/privacy-policy">Privacy Policy</ext-link>.</p>
              <p>Reviewer #1: No</p>
              <p>Reviewer #2: No</p>
              <p>
                <underline>Figure Files:</underline>
              </p>
              <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <underline><ext-link ext-link-type="uri" xlink:href="https://pacev2.apexcovantage.com/" xlink:type="simple">https://pacev2.apexcovantage.com</ext-link></underline>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <underline><email xlink:type="simple">figures@plos.org</email></underline>.</p>
              <p>
                <underline>Data Requirements:</underline>
              </p>
              <p>Please note that, as a condition of publication, PLOS' data policy requires that you make available all data used to draw the conclusions outlined in your manuscript. Data must be deposited in an appropriate repository, included within the body of the manuscript, or uploaded as supporting information. This includes all numerical values that were used to generate graphs, histograms etc.. For an example in PLOS Biology see here: <ext-link ext-link-type="uri" xlink:href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5">http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5</ext-link>.</p>
              <p>
                <underline>Reproducibility:</underline>
              </p>
              <p>To enhance the reproducibility of your results, PLOS recommends that you deposit laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. For instructions, please see <underline><ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plospathogens/s/submission-guidelines" xlink:type="simple">http://journals.plos.org/compbiol/s/submission-guidelines#loc-materials-and-methods</ext-link></underline></p>
            </body>
          </sub-article>
          <sub-article id="pcbi.1007659.r002" article-type="author-comment">
            <front-stub>
              <article-id pub-id-type="doi">10.1371/journal.pcbi.1007659.r002</article-id>
              <title-group>
                <article-title>Author response to Decision Letter 0</article-title>
              </title-group>
              <related-article id="rel-obj002" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007659" related-article-type="editor-report"/>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>Submission Version</meta-name>
                  <meta-value>1</meta-value>
                </custom-meta>
              </custom-meta-group>
            </front-stub>
            <body>
              <p>
                <named-content content-type="author-response-date">1 Jun 2020</named-content>
              </p>
              <supplementary-material content-type="local-data" id="pcbi.1007659.s002">
                <label>Attachment</label>
                <caption>
                  <p>Submitted filename: <named-content content-type="submitted-filename">StableDynamicsPLOSReviewResponses3.docx</named-content></p>
                </caption>
                <media xlink:href="pcbi.1007659.s002.docx">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
            </body>
          </sub-article>
          <sub-article id="pcbi.1007659.r003" article-type="aggregated-review-documents">
            <front-stub>
              <article-id pub-id-type="doi">10.1371/journal.pcbi.1007659.r003</article-id>
              <title-group>
                <article-title>Decision Letter 1</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Graham</surname>
                    <given-names>Lyle J.</given-names>
                  </name>
                  <role>Deputy Editor</role>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Haith</surname>
                    <given-names>Adrian M</given-names>
                  </name>
                  <role>Associate Editor</role>
                </contrib>
              </contrib-group>
              <permissions>
                <copyright-statement>© 2020 Graham, Haith</copyright-statement>
                <copyright-year>2020</copyright-year>
                <copyright-holder>Graham, Haith</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
                </license>
              </permissions>
              <related-article id="rel-obj003" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007659" related-article-type="reviewed-article"/>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>Submission Version</meta-name>
                  <meta-value>1</meta-value>
                </custom-meta>
              </custom-meta-group>
            </front-stub>
            <body>
              <p>
                <named-content content-type="letter-date">27 Jun 2020</named-content>
              </p>
              <p>Dear Dr. Miller,</p>
              <p>We are pleased to inform you that your manuscript 'Achieving stable dynamics in neural circuits' has been provisionally accepted for publication in PLOS Computational Biology.</p>
              <p>Before your manuscript can be formally accepted you will need to complete some formatting changes, which you will receive in a follow up email. A member of our team will be in touch with a set of requests.</p>
              <p>Please note that your manuscript will not be scheduled for publication until you have made the required changes, so a swift response is appreciated.</p>
              <p>IMPORTANT: The editorial review process is now complete. PLOS will only permit corrections to spelling, formatting or significant scientific errors from this point onwards. Requests for major changes, or any which affect the scientific understanding of your work, will cause delays to the publication date of your manuscript.</p>
              <p>Should you, your institution's press office or the journal office choose to press release your paper, you will automatically be opted out of early publication. We ask that you notify us now if you or your institution is planning to press release the article. All press must be co-ordinated with PLOS.</p>
              <p>Thank you again for supporting Open Access publishing; we are looking forward to publishing your work in PLOS Computational Biology. </p>
              <p>Best regards,</p>
              <p>Adrian M Haith</p>
              <p>Associate Editor</p>
              <p>PLOS Computational Biology</p>
              <p>Lyle Graham</p>
              <p>Deputy Editor</p>
              <p>PLOS Computational Biology</p>
              <p>***********************************************************</p>
              <p>Reviewer's Responses to Questions</p>
              <p>
                <bold>Comments to the Authors:</bold>
              </p>
              <p>
                <bold>Please note here if the review is uploaded as an attachment.</bold>
              </p>
              <p>Reviewer #1: All prior criticisms have been addressed satisfactorily and the paper is clear enough now.</p>
              <p>Please just carefully check the grammar before sending it for any publication as there are some typos/non-English forms in the added text (e.g. l.120 "at rate called the contraction rate" should begin "at a rate"). I came across a few of these sorts of issues.</p>
              <p>Reviewer #2: The authors have addressed the points raised in my original review. I thank them for their responses and am satisfied with the edits made to the manuscript.</p>
              <p>**********</p>
              <p>
                <bold>Have all data underlying the figures and results presented in the manuscript been provided?</bold>
              </p>
              <p>Large-scale datasets should be made available via a public repository as described in the <italic>PLOS Computational Biology</italic>
<ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/ploscompbiol/s/data-availability">data availability policy</ext-link>, and numerical data that underlies graphs or summary statistics should be provided in spreadsheet form as supporting information.</p>
              <p>Reviewer #1: Yes</p>
              <p>Reviewer #2: None</p>
              <p>**********</p>
              <p>PLOS authors have the option to publish the peer review history of their article (<ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
              <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
              <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link ext-link-type="uri" xlink:href="https://www.plos.org/privacy-policy">Privacy Policy</ext-link>.</p>
              <p>Reviewer #1: <bold>Yes: </bold>Paul Miller</p>
              <p>Reviewer #2: No</p>
            </body>
          </sub-article>
          <sub-article id="pcbi.1007659.r004" article-type="editor-report">
            <front-stub>
              <article-id pub-id-type="doi">10.1371/journal.pcbi.1007659.r004</article-id>
              <title-group>
                <article-title>Acceptance letter</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Graham</surname>
                    <given-names>Lyle J.</given-names>
                  </name>
                  <role>Deputy Editor</role>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Haith</surname>
                    <given-names>Adrian M</given-names>
                  </name>
                  <role>Associate Editor</role>
                </contrib>
              </contrib-group>
              <permissions>
                <copyright-statement>© 2020 Graham, Haith</copyright-statement>
                <copyright-year>2020</copyright-year>
                <copyright-holder>Graham, Haith</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
                </license>
              </permissions>
              <related-article id="rel-obj004" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007659" related-article-type="reviewed-article"/>
            </front-stub>
            <body>
              <p>
                <named-content content-type="letter-date">30 Jul 2020</named-content>
              </p>
              <p>PCOMPBIOL-D-20-00042R1 </p>
              <p>Achieving stable dynamics in neural circuits</p>
              <p>Dear Dr Miller,</p>
              <p>I am pleased to inform you that your manuscript has been formally accepted for publication in PLOS Computational Biology. Your manuscript is now with our production department and you will be notified of the publication date in due course.</p>
              <p>The corresponding author will soon be receiving a typeset proof for review, to ensure errors have not been introduced during production. Please review the PDF proof of your manuscript carefully, as this is the last chance to correct any errors. Please note that major changes, or those which affect the scientific understanding of the work, will likely cause delays to the publication date of your manuscript. </p>
              <p>Soon after your final files are uploaded, unless you have opted out, the early version of your manuscript will be published online. The date of the early version will be your article's publication date. The final article will be published to the same URL, and all versions of the paper will be accessible to readers.</p>
              <p>Thank you again for supporting PLOS Computational Biology and open-access publishing. We are looking forward to publishing your work! </p>
              <p>With kind regards,</p>
              <p>Laura Mallard</p>
              <p>PLOS Computational Biology | Carlyle House, Carlyle Road, Cambridge CB4 3DN | United Kingdom <email>ploscompbiol@plos.org</email> | Phone +44 (0) 1223-442824 | <ext-link ext-link-type="uri" xlink:href="http://ploscompbiol.org">ploscompbiol.org</ext-link> | @PLOSCompBiol</p>
            </body>
          </sub-article>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
