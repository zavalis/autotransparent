<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2023-02-07T13:02:55Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:7749289" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:7749289</identifier>
        <datestamp>2020-12-24</datestamp>
        <setSpec>pnas</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Proc Natl Acad Sci U S A</journal-id>
              <journal-id journal-id-type="iso-abbrev">Proc Natl Acad Sci U S A</journal-id>
              <journal-id journal-id-type="hwp">pnas</journal-id>
              <journal-id journal-id-type="pmc">pnas</journal-id>
              <journal-id journal-id-type="publisher-id">PNAS</journal-id>
              <journal-title-group>
                <journal-title>Proceedings of the National Academy of Sciences of the United States of America</journal-title>
              </journal-title-group>
              <issn pub-type="ppub">0027-8424</issn>
              <issn pub-type="epub">1091-6490</issn>
              <publisher>
                <publisher-name>National Academy of Sciences</publisher-name>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC7749289</article-id>
              <article-id pub-id-type="pmcid">PMC7749289</article-id>
              <article-id pub-id-type="pmc-uid">7749289</article-id>
              <article-id pub-id-type="pmid">33257566</article-id>
              <article-id pub-id-type="pmid">33257566</article-id>
              <article-id pub-id-type="publisher-id">202021325</article-id>
              <article-id pub-id-type="doi">10.1073/pnas.2021325117</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Biological Sciences</subject>
                  <subj-group>
                    <subject>Psychological and Cognitive Sciences</subject>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Visual motion assists in social cognition</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-3694-1318</contrib-id>
                  <name>
                    <surname>Guterstam</surname>
                    <given-names>Arvid</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff10">
                    <sup>b</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7727-3475</contrib-id>
                  <name>
                    <surname>Graziano</surname>
                    <given-names>Michael S. A.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                </contrib>
                <aff id="aff1"><sup>a</sup>Department of Psychology, <institution>Princeton University</institution>, Princeton, <addr-line>NJ</addr-line> 08544;</aff>
                <aff id="aff10"><sup>b</sup>Department of Clinical Neuroscience, Karolinska Institutet, 171 77 Solna, Stockholm, <country>Sweden</country></aff>
              </contrib-group>
              <author-notes>
                <corresp id="cor1"><sup>1</sup>To whom correspondence may be addressed. Email: <email>arvidg@princeton.edu</email>.</corresp>
                <fn fn-type="edited-by">
                  <p>Edited by Ranulfo Romo, National Autonomous University of Mexico, Mexico City, D.F., Mexico, and approved November 4, 2020 (received for review October 12, 2020)</p>
                </fn>
                <fn fn-type="con">
                  <p>Author contributions: A.G. and M.S.A.G. designed research; A.G. performed research; A.G. analyzed data; and A.G. and M.S.A.G. wrote the paper.</p>
                </fn>
              </author-notes>
              <pub-date pub-type="ppub">
                <day>15</day>
                <month>12</month>
                <year>2020</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>30</day>
                <month>11</month>
                <year>2020</year>
              </pub-date>
              <pub-date pub-type="pmc-release">
                <day>30</day>
                <month>11</month>
                <year>2020</year>
              </pub-date>
              <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
              <volume>117</volume>
              <issue>50</issue>
              <fpage>32165</fpage>
              <lpage>32168</lpage>
              <permissions>
                <copyright-statement>Copyright © 2020 the Author(s). Published by PNAS.</copyright-statement>
                <copyright-year>2020</copyright-year>
                <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">
                  <ali:license_ref specific-use="vor">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
                  <license-p>This open access article is distributed under <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND)</ext-link>.</license-p>
                </license>
              </permissions>
              <self-uri xlink:title="pdf" xlink:href="pnas.202021325.pdf"/>
              <abstract abstract-type="executive-summary">
                <title>Significance</title>
                <p>Recent evidence suggests that our brains may generate subtle, fictitious motion signals streaming from other people to the objects of their attention. However, the functional significance of this internally-generated motion signal remains unclear. Here, we tested whether subthreshold motion processing plays a causal role in judging others’ attention. Participants viewed a display including faces, objects, and a subthreshold motion hidden in the background. Judgments of the attentional state of the faces were significantly altered by the motion signal. Control experiments indicated the effect was specific to judging attention. These findings suggest that a crucial aspect of social cognition, monitoring others’ attention, draws on useful but highly physically inaccurate models of social agents in the world, rooted in low-level perceptual mechanisms.</p>
              </abstract>
              <abstract>
                <p>Recent evidence suggests a link between visual motion processing and social cognition. When person A watches person B, the brain of A apparently generates a fictitious, subthreshold motion signal streaming from B to the object of B’s attention. These previous studies, being correlative, were unable to establish any functional role for the false motion signals. Here, we directly tested whether subthreshold motion processing plays a role in judging the attention of others. We asked, if we contaminate people’s visual input with a subthreshold motion signal streaming from an agent to an object, can we manipulate people’s judgments about that agent’s attention? Participants viewed a display including faces, objects, and a subthreshold motion hidden in the background. Participants’ judgments of the attentional state of the faces was significantly altered by the hidden motion signal. Faces from which subthreshold motion was streaming toward an object were judged as paying more attention to the object. Control experiments showed the effect was specific to the agent-to-object motion direction and to judging attention, not action or spatial orientation. These results suggest that when the brain models other minds, it uses a subthreshold motion signal, streaming from an individual to an object, to help represent attentional state. This type of social-cognitive model, tapping perceptual mechanisms that evolved to process physical events in the real world, may help to explain the extraordinary cultural persistence of beliefs in mind processes having physical manifestation. These findings, therefore, may have larger implications for human psychology and cultural belief.</p>
              </abstract>
              <kwd-group>
                <kwd>social cognition</kwd>
                <kwd>attention</kwd>
                <kwd>motion</kwd>
                <kwd>theory of mind</kwd>
              </kwd-group>
              <counts>
                <page-count count="4"/>
              </counts>
            </article-meta>
          </front>
          <body>
            <p content-type="flushleft">A recent series of reports suggests a link between visual motion processing and social cognition. The human brain appears to fabricate a subtle, false motion signal when looking at another person. The fictitious motion streams in a beam from the other person toward the object of that person’s attention (<xref rid="r1" ref-type="bibr">1</xref><xref rid="r2" ref-type="bibr"/>–<xref rid="r3" ref-type="bibr">3</xref>). The signal can be observed directly with functional MRI in motion-processing cortical areas, and it can be observed indirectly by how it causes a motion aftereffect in the area of a scene between a face and the object of the face’s attention. The signal, however, is perceptually subthreshold—people are not explicitly aware of it. The functional purpose, if any, of this subthreshold false motion signal is not known, although we speculated it is part of the social toolkit for modeling the attention of others. Because previous studies were correlative—showing a correlation between social cognition and an internally generated motion signal—the causal relationship is not known (<xref rid="r4" ref-type="bibr">4</xref>, <xref rid="r5" ref-type="bibr">5</xref>). To establish this new subfield of study in which social cognition taps into preexisting perceptual machinery to model the mind states of others, a direct causal experiment is needed. Here, we provide that test. We asked, if we contaminate a participant’s visual world with a subthreshold motion that streams from another person toward an object, can we manipulate the participant’s perception of that other person’s attention? The results demonstrated a behaviorally meaningful impact of subthreshold motion on social judgments. It explains why the human brain fabricates a motion signal during social cognition. Modeling the attention state of others is a crucial part of social cognition (<xref rid="r6" ref-type="bibr">6</xref><xref rid="r7" ref-type="bibr"/><xref rid="r8" ref-type="bibr"/>–<xref rid="r9" ref-type="bibr">9</xref>), and recruiting the motion-processing system evidently contributes to that model. It may have proved adaptive to co-opt the brain’s existing motion-processing mechanism to encode sources and targets of attention, in essence drawing a quick visual sketch with moving arrows to help keep track of who is attending to what in a complex environment. We suggest that the beam of motion represents, instantaneously, the relationship between an agent and the target of its attention. In this interpretation, the subthreshold motion signal balances two adaptive pressures: it is strong enough to influence social cognition in a meaningful direction while at the same time not so strong that it materially interferes with the normal motion perception of real objects. This type of social-cognitive model, borrowing low-level perceptual mechanisms that evolved to process physical events in the real world, may help to explain the extraordinary cultural persistence of beliefs in mind processes having physical manifestation. It is a common belief across time and cultures that attentive gaze comes with a palpable outward flow and that other properties of the mind are linked to specific physical auras and flows. The present findings, therefore, may have larger implications for human psychology and cultural belief.</p>
            <sec sec-type="results" id="s1">
              <title>Results</title>
              <p content-type="flushleft">In experiment 1, subjects were asked to indicate which of two faces seemed to be attending more to an object (<xref ref-type="fig" rid="fig01">Fig. 1<italic>A</italic></xref> and see <xref ref-type="sec" rid="s3"><italic>Materials and Methods</italic></xref> for details). Though subjects were not told, one face was an exact mirror reversal of the other. The faces were above and to either side of the central object, with eyeballs tilted slightly downward but not pointing directly at the object. The background was filled with random dot motion featuring 0% coherence (100% random noise). A weak motion signal was added in a beam-shaped area streaming from one face (yellow-highlighted area in <xref ref-type="fig" rid="fig01">Fig. 1<italic>A</italic></xref>, yellow coloring not visible to subjects), where 30% of the dots moved coherently toward the object. The motion signal was subtle enough that only 7 of 657 subjects across all experiments (1.1%) reported being aware of it in a postexperiment survey (those 7 subjects were excluded from analysis). On half of the randomly interleaved trials, the beam emanated from the left head, and on the other half, it emanated from the right head. Subjects were tested using an online, remote platform (Prolific) (<xref rid="r10" ref-type="bibr">10</xref>) due to restrictions on research imposed by the coronavirus epidemic (see <xref ref-type="sec" rid="s3"><italic>Materials and Methods</italic></xref> for details of sample sizes and exclusion criteria).</p>
              <fig fig-type="featured" id="fig01" orientation="portrait" position="float">
                <label>Fig. 1.</label>
                <caption>
                  <p>The effect of motion on social cognition. (<italic>A</italic>) Subjects viewed two faces turned toward an object, presented on a background of random dot motion noise (0% dot coherence). In a region connecting one face to the object (yellow highlight, color not seen by subjects), 30% of dots moved coherently toward the object, constituting a subtle, subthreshold motion signal (98.9% of subjects reported being unaware of it). The beam extended from the left or right face on interleaved trials. In experiments 1 to 3, on each trial, subjects indicated which face they perceived as attending more to the object. In experiment 6, subjects indicated which person they perceived as more likely to reach for the object first. (<italic>B</italic>) The same except that motion was from object to face. (<italic>C</italic>) The same except compass needles replaced the faces and subjects reported which needle appeared to point more toward the object. (<italic>D</italic>) The proportion of congruent responses (responses in which the subjects selected the face or compass needle adjacent to the motion stream). Error bars represent 95% CI based on a bootstrap distribution. *<italic>P</italic> &lt; 0.05, **<italic>P</italic> &lt; 0.01. (<italic>E</italic>) Meta-analysis. An analysis of all data from experiments 1 to 3 combined is shown. A null distribution of group means was generated based on permutation testing (10,000 iterations) with shuffled condition labels. The red vertical line represents the true mean using the correct condition labels. (<italic>F</italic>) Between-group meta-analysis. An analysis of combined data from all test experiments (1 to 3) and all control experiments (4 to 6) created by generating two bootstrap distributions (10,000 iterations) is shown. exp, experiment.</p>
                </caption>
                <graphic id="gra1" xlink:href="pnas.2021325117fig01"/>
              </fig>
              <p><xref ref-type="fig" rid="fig01">Fig. 1<italic>D</italic></xref> summarizes the results. In experiment 1 (<italic>n</italic> = 88), subjects were more likely to choose the face from which the motion streamed toward the object (termed the “congruent face”) than the face without motion streaming from it (the “incongruent face”) (mean proportion of congruent responses, 53.0%; mean proportion of incongruent responses, 47.0%; <italic>P</italic> = 0.006 by permutation testing). Thus, the motion beam connecting the congruent face to the object, though not overtly noticed, made subjects 6% more likely to choose the congruent face rather than the incongruent face as directing more attention to the object. Experiment 2 was a replication that showed a similar result (<italic>n</italic> = 123; mean proportion of congruent responses, 52.1%; <italic>P</italic> = 0.028 by permutation testing). Experiment 3 was another replication (<italic>n</italic> = 68; mean proportion of congruent responses, 52.5%; <italic>P</italic> = 0.040 by permutation testing). <xref ref-type="fig" rid="fig01">Fig. 1<italic>E</italic></xref> shows an analysis combining experiments 1 to 3 (<italic>n</italic> = 279). On average, subjects chose the congruent face 2.5% more often than the expected chance level of 50%, or, equivalently, subjects were 5.0% more likely to choose the congruent face over the incongruent face as directing more attention to the object (mean proportion of congruent responses, 52.5%; <italic>P</italic> = 0.0002 by permutation testing). The subthreshold motion signal had a significant effect on social decisions about other people’s attention.</p>
              <p>Experiment 4 was identical to experiments 1 through 3 except that the subthreshold motion was reversed, streaming from the central object to the face (<xref ref-type="fig" rid="fig01">Fig. 1<italic>B</italic></xref>). The effect on social judgment disappeared (<xref ref-type="fig" rid="fig01">Fig. 1<italic>D</italic></xref>; <italic>n</italic> = 87; mean proportion of congruent responses, 50.8%; <italic>P</italic> = 0.257 by permutation testing). This result suggests that the findings in experiments 1 through 3 were unlikely to be caused by low-level features of the motion stream or by the presence of motion on one side of the screen drawing subjects’ attention or gaze more to one face.</p>
              <p>Experiment 5 was identical to experiments 1 through 3 except that instead of faces, subjects saw two compass needles (<xref ref-type="fig" rid="fig01">Fig. 1<italic>C</italic></xref>). The needles pointed downward to the same degree that the eyeballs had in the previous experiments. Subjects were asked which compass appeared to be pointed more directly to the object. The motion beam had no significant effect on responses (<italic>n</italic> = 80; mean proportion of congruent responses, 49.3%; <italic>P</italic> = 0.661 by permutation testing). This result shows that the effect observed in experiments 1 through 3 was not the result of a low-level visual illusion in which the motion beam caused a general distortion in the perception of tilt angles.</p>
              <p>Experiment 6 was identical to experiments 1 through 3 except for the instructions. Rather than judging attention, subjects were asked to judge which person seemed more likely to reach for the object first. The motion beam had no significant effect on responses (<italic>n</italic> = 80; mean proportion of congruent responses, 50.5%; <italic>P</italic> = 0.379 by permutation testing). The result obtained in experiments 1 through 3 was therefore specific to judging the attention of agents, not the actions of agents, even potential actions that involve a motion from the agent toward the object, such as reaching. Because the stimuli were identical to those in experiment 1, the result also suggests that the effect cannot be explained by the motion beam biasing the subject’s attention in any simple manner or by any other simple or low-level feature of the visual display.</p>
              <p><xref ref-type="fig" rid="fig01">Fig. 1<italic>F</italic></xref> shows an analysis of all six experiments, comparing the three control conditions (experiments 4 to 6, <italic>n</italic> = 247) to the three attention beam conditions (experiments 1 to 3, <italic>n</italic> = 279). The mean proportion of congruent responses was significantly greater for the attention beam distribution than for the control distribution (52.5% versus 50.2%, <italic>P</italic> = 0.008 by bootstrap testing). In the control experiments, subjects were 0.4% more likely to pick the congruent choice than the incongruent choice, whereas in the attention beam experiments, subjects were 5.0% more likely to pick the congruent choice than the incongruent choice.</p>
            </sec>
            <sec sec-type="discussion" id="s2">
              <title>Discussion</title>
              <p content-type="flushleft">Our results demonstrate that it is possible to manipulate people’s perception of an agent’s attentional state in a predictable manner by artificially introducing a subthreshold motion signal emanating from that agent and streaming toward an object. The control experiments suggest that the effect cannot be explained by factors such as low-level features of the motion stream (experiments 4 and 6), a low-level visual illusion in which the motion stream distorts the general perception of angles (experiment 5), the subjects covertly thinking about any active relationship between the face and the object (experiment 6), and the subject’s attention being biased toward one choice by the motion beam (experiments 4 to 6). Instead, the effect appears to be specific to judging a face, when the motion flows specifically from the face to the object, and when the subjects judge the attention state of the face. The finding suggests that motion plays a causal role in coding other people’s attention by enhancing or highlighting the directional connection between agent and object. The finding helps explain why the human brain generates a motion signal when processing other people’s attentive gaze (<xref rid="r2" ref-type="bibr">2</xref>, <xref rid="r3" ref-type="bibr">3</xref>). The link between social cognition and motion processing is not an epiphenomenal trait but rather a useful mechanism that facilitates the tracking of other people’s attention. It is an example of how the brain can construct models of the world that serve some adaptive behavioral function, even if those models are not physically accurate depictions of the world. A prediction to be tested in future experiments is that the brain will rely more heavily on this motion-highlighting mechanism in complex social environments that require simultaneous tracking of multiple sources and targets of attention.</p>
            </sec>
            <sec sec-type="materials|methods" id="s3">
              <title>Materials and Methods</title>
              <sec id="s4">
                <title>Participants.</title>
                <p>For each experiment, participants were recruited through the online behavioral testing platform Prolific (<xref rid="r10" ref-type="bibr">10</xref>). Using the tools available on the Prolific platform, we restricted participation such that no subject could take part in more than one of our experiments. Thus, all subjects were naive to the paradigm when tested. All participants indicated normal or corrected-to-normal vision, English as a first language, and no history of mental illness or cognitive impairment. All experimental methods and procedures were approved by the Princeton University Institutional Review Board, and all participants confirmed that they had read and understood a consent form outlining their risks, benefits, compensation, and confidentiality and that they agreed to participate in the experiment. Each subject completed a single experiment in a 3- to 5-min session in exchange for monetary compensation. As is standard for online experiments, because of greater expected variation than for in-laboratory experiments, relatively large numbers of subjects were tested. A target sample size of 100 subjects per experiment was chosen arbitrarily before data collection began. Because of stringent criteria for eliminating those who did not follow all instructions (see below), initial total sample sizes were larger than 100, and final sample sizes for those included in the analysis varied between experiments (experiment 1, <italic>n</italic><sub>total</sub> = 107, <italic>n</italic><sub>included</sub> = 88, 36 female, mean age 29 [SD, 11]; experiment 2, <italic>n</italic><sub>total</sub> = 153, <italic>n</italic><sub>included</sub> = 123, 58 female, mean age 34 [SD, 14]; experiment 3, <italic>n</italic><sub>total</sub> = 103, <italic>n</italic><sub>included</sub> = 68, 22 female, mean age 27 [SD, 9]; experiment 4, <italic>n</italic><sub>total</sub> = 106, <italic>n</italic><sub>included</sub> = 87, 36 female, mean age 29 [SD, 11]; experiment 5, <italic>n</italic><sub>total</sub> = 108, <italic>n</italic><sub>included</sub> = 80, 22 female, mean age 25 [SD, 8]; experiment 6, <italic>n</italic><sub>total</sub> = 102, <italic>n</italic><sub>included</sub> = 80, 35 female, mean age 27 [SD, 9]).</p>
              </sec>
              <sec id="s5">
                <title>Exclusion Criteria.</title>
                <p>Subjects were excluded based on a set of predefined criteria. On average, across all six experiments, the exclusion rate was 23% (experiment 1, <italic>n</italic><sub>excluded</sub> = 19 [18%]; experiment 2, 30 [20%], experiment 3, 35 [34%]; experiment 4, 19 [18%]; experiment 5, 28 [26%]; experiment 6, 22 [22%]). The most common reason for exclusion was failure to pass the instructional manipulation check (IMC) (experiment 1, 18; experiment 2, 25; experiment 3, 34; experiment 4, 17; experiment 5, 28; experiment 6, 18). The IMC was adapted from ref. <xref rid="r11" ref-type="bibr">11</xref> and was included to ensure that subjects read and adequately understood the instructions. It consisted of the following sentence inserted at the end of the instructions page: “In order to demonstrate that you have read these instructions carefully, please ignore the ‘Continue’ button below, and click on the ‘x’ to start the practice session.” Two buttons were presented at the bottom of the screen, “Continue” and “x,” and clicking on “Continue” resulted in a failed IMC. Because the intent was for subjects to judge the two faces carefully, we also excluded people with fast (&lt;500 ms) average response times (experiment 1, 1; experiment 2, 0; experiment 3, 0; experiment 4, 1; experiment 5, 0; experiment 6, 0) and subjects who missed (failed to respond within 5 s) more than 30% of trials (experiment 1, 0; experiment 2, 2; experiment 3, 1; experiment 4, 0; experiment 5, 0; experiment 6, 0). One subject in experiment 2 was excluded due to software failure during testing. Finally, subjects who, in the postexperiment survey, reported awareness of the coherent dot motion were excluded from analysis (experiment 1, 0; experiment 2, 2; experiment 3, 0; experiment 4, 1; experiment 5, 0; experiment 6, 4).</p>
              </sec>
              <sec id="s6">
                <title>Apparatus.</title>
                <p>After agreeing to participate, subjects were redirected to a website where stimulus presentation and data collection were controlled by custom software based on Hypertext Markup Language, Cascading Style Sheets, JavaScript (using the jsPsych javascript library (<xref rid="r12" ref-type="bibr">12</xref>)), and PHP: Hypertext Processor. Subjects were required to complete the experiment in full-screen mode. Exiting full screen resulted in the termination of the experiment and no payment. Because the visual stimuli were rendered on participants’ own web browsers, viewing distance, screen size, and display resolutions varied. We therefore report stimulus dimensions below using pixel (px) values.</p>
              </sec>
              <sec id="s7">
                <title>Visual Stimuli.</title>
                <p>The visual stimuli (1280 px wide × 370 px high) were created using MATLAB (MathWorks) and then converted to high-resolution, high–frame-rate video files for presentation online. Images of two identical, mirror-reversed cartoon faces (250 px wide × 273 px high), located equidistant on either side of a central circular object (200 px diameter), were presented on top of a random dot motion stimulus and a gray (red: 210, green: 210, blue: 210) background (<xref ref-type="fig" rid="fig01">Fig. 1<italic>A</italic></xref>). The faces were located slightly (90 px) above the object, making the angle between the eyes of the faces and the object 21°. The pupils of the eyeballs were tilted down only 6°, creating an ambiguous impression of whether the faces were looking at the object or not. To increase the variety of the visual stimuli across trials (to prevent subject boredom), we used one of four different objects in each trial: a soccer ball, a pizza, a beach ball, or a dartboard.</p>
                <p>The random dot motion stimulus featured black dots. Dot size was 2 px. Dot density was 30 per 100 px × 100 px square. Dot speed was 95 px/s. Dot lifetime was 200 ms. Motion direction of dots was 100% random across the entire display, except for an area forming the “beam” connecting one face to the object (yellow area, <xref ref-type="fig" rid="fig01">Fig. 1<italic>A</italic></xref>). Within the beam area, 30% of the dots moved in a coherent direction toward the object (21° downward motion in experiments 1 to 3 and 5 to 6) or toward the face (21° upward motion in experiment 4), while 70% of the dots moved in random directions. The 30% coherence level was chosen based on the results of a pilot experiment (<italic>n</italic> = 7) conducted in the laboratory, showing that this motion signal was below the threshold of awareness (none out of seven subjects reported being aware of the coherently moving dots while performing the task).</p>
                <p>The visual stimuli generated in MATLAB were converted to video files using on-screen recording software capturing 60 frames per second. The videos were edited in Final Cut Pro (Apple Inc.) to generate one video clip per trial type, the size of which was ∼20 MB per video. The videos used in all experiments are available at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/dataset/Visual_motion_assists_in_social_cognition/12665078/1" xlink:show="new">https://figshare.com/articles/dataset/Visual_motion_assists_in_social_cognition/12665078/1</ext-link> (<xref rid="bib13" ref-type="bibr">13</xref>). During playback, the video encompassed 90% of the width of the subject’s computer screen (which ran the experiment in full-screen mode).</p>
              </sec>
              <sec id="s8">
                <title>Experimental Design.</title>
                <sec id="s9">
                  <title>Experiment 1.</title>
                  <p>The experiment consisted of an initial instructions page, followed by three practice trials, the experimental session, and a postexperiment survey. The written instructions were as follows: “Humans are extremely sensitive to where other people are attending. In this experiment, you will view images of two people located on either side of an object. Your task is to determine who is attending more to the object. In each trial, indicate the person that you think is paying more attention to the object by hitting Q for the person on the left, or P for the person on the right. Respond as quickly as you can. You must respond within 5 secs.”</p>
                  <p>When the subject pressed the response key to indicate one of the two faces, the trial terminated and the next trial began. If a response was not made within 5 s, the trial terminated and the warning “Too Slow!” was displayed.</p>
                  <p>There were eight randomly interleaved trial types (motion beam on the left or right side × four different objects), each repeated twice, yielding a total of 16 trials per subject. In online experiments, for better subject compliance, it is advantageous to use few trials per subject, thus minimizing each subject’s time commitment while testing a larger sample of subject. For analysis, all trial types were collapsed, and we calculated, for each subject, the proportion of trials in which the subject’s response (left or right) was congruent with the location of the motion beam (left or right). At the group level, we used permutation testing for statistical inference. To create a null distribution, we randomly shuffled the motion beam condition labels (left or right) 10,000 times and calculated the group mean proportion of congruent responses for each iteration. We then calculated a <italic>P</italic> value by comparing the true group mean proportion of congruent responses with the null distribution using the following equation: <italic>P</italic> = (1 + number of permuted group mean values &gt; true value)/(1 + total number of permutations). A 95% CI was calculated using bootstrapping, where individual means were resampled with replacement generating a bootstrap distribution of group means (10,000 iterations).</p>
                  <p>After the experiment, the subjects completed a survey. They were first asked two open-ended questions: “What do you think the hypothesis of the experiment was?” and “How did you perceive the background motion during the experiment?” They were then given the binary yes-or-no question, “Did you perceive the background motion as completely random?” Subjects who responded “no” were asked, “Please describe in what way the background motion wasn’t random.” Subjects who in any way reported that they had perceived coherently moving dots or streams of motion were excluded from the analysis.</p>
                </sec>
                <sec id="s10">
                  <title>Experiment 2.</title>
                  <p>Experiment 2 was a replication of experiment 1 and used the same procedures.</p>
                </sec>
                <sec id="s11">
                  <title>Experiment 3.</title>
                  <p>Experiment 3 was a replication of experiment 1 with one modification to the instructions: a monetary incentive was added to motivate subjects to perform with greater focus. The written instructions contained the following addition: “The amount of payment will depend on your score, which is an indication of the effort you are putting into the task. If you are correct on at least 80% of the trials, you will receive a 20% bonus pay.” There were, of course, no formally correct answers, and at the end of the experiment, all subjects were awarded the bonus pay.</p>
                </sec>
                <sec id="s12">
                  <title>Experiment 4.</title>
                  <p>The design, procedures, and statistical analysis of experiment 4 were identical to those of experiment 1, with one exception: the coherent dots within the motion beam moved from the object toward the face.</p>
                </sec>
                <sec id="s13">
                  <title>Experiment 5.</title>
                  <p>To exclude low-level visual effects of subthreshold motion on perceived visual angles, in experiment 5, we substituted the two faces with two compass needles (<xref ref-type="fig" rid="fig01">Fig. 1<italic>C</italic></xref>). The compasses matched the face images in position and size. The compass needles matched the eyes in angle (6° downward tilt) and location (the tip of the compass needle was in the same location as the pupil). Subjects were asked to indicate which compass (left or right) pointed more directly to the object: “Humans are extremely sensitive to where things are pointing. In this experiment, you will view images of two compasses located on either side of an object. Your task is to determine which compass is pointing more at the object. In each trial, indicate the compass that you think is pointing more at the object by hitting Q for the compass on the left, or P for the compass on the right. Respond as quickly as you can. You must respond within 5 secs.”</p>
                </sec>
                <sec id="s14">
                  <title>Experiment 6.</title>
                  <p>The visual stimuli in experiment 6 were identical to those in experiment 1. The procedures were the same in all respects except for the instructions, which read as follows: “Humans are extremely good at predicting when other people are about to reach for something. In this experiment, you will view images of two faces located on either side of an object. The faces may be subtly different in ways that you do not consciously notice. The two people are both planning to reach for the object. Your task is to decide, based on your intuition about the faces, which person is most likely to reach for the object first.”</p>
                </sec>
              </sec>
              <sec id="s15">
                <title>Between-Group Meta-Analysis.</title>
                <p>In a between-group meta-analysis (<xref ref-type="fig" rid="fig01">Fig. 1<italic>F</italic></xref>), we combined the responses across all test experiments (<italic>n</italic> = 279; experiments 1 to 3) and compared them to the responses across all control experiments (<italic>n</italic> = 247; experiments 4 to 6). We first generated two bootstrap distributions (resampling with replacement, 10,000 iterations) centered around the mean proportion of congruent responses for each group. We then calculated a <italic>P</italic> value by comparing the two bootstrap distributions (iteration by iteration in random order) using the following equation: <italic>P</italic> = (1 + number of bootstrap iterations for which group mean<sub>exp</sub>
<sub>4</sub>
<sub>to</sub>
<sub>6</sub> &gt; group mean<sub>exp</sub>
<sub>1</sub>
<sub>to</sub>
<sub>3</sub>)/(1 + total number of bootstrap iterations).</p>
              </sec>
            </sec>
          </body>
          <back>
            <ack>
              <p>This work was supported by the Princeton Neuroscience Institute Innovation Fund. A.G. was supported by the Wenner-Gren Foundation, the Sweden-America Foundation, the Swedish Brain Foundation, and the Promobilia Foundation.</p>
            </ack>
            <fn-group>
              <fn fn-type="COI-statement">
                <p>The authors declare no competing interest.</p>
              </fn>
              <fn fn-type="other">
                <p>This article is a PNAS Direct Submission.</p>
              </fn>
            </fn-group>
            <sec id="s16" sec-type="data-availability">
              <title>Data Availability.</title>
              <p>The data that support the findings of this study are available on Figshare at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/dataset/Visual_motion_assists_in_social_cognition/12665078/1" xlink:show="new">https://figshare.com/articles/dataset/Visual_motion_assists_in_social_cognition/12665078/1</ext-link> (<xref rid="bib13" ref-type="bibr">13</xref>).</p>
            </sec>
            <ref-list>
              <ref id="r1">
                <label>1</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guterstam</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Kean</surname><given-names>H. H.</given-names></name>, <name name-style="western"><surname>Webb</surname><given-names>T. W.</given-names></name>, <name name-style="western"><surname>Kean</surname><given-names>F. S.</given-names></name>, <name name-style="western"><surname>Graziano</surname><given-names>M. S. A.</given-names></name></person-group>, <article-title>Implicit model of other people’s visual attention as an invisible, force-carrying beam projecting from the eyes</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><volume>116</volume>, <fpage>328</fpage>–<lpage>333</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">30559179</pub-id></mixed-citation>
              </ref>
              <ref id="r2">
                <label>2</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guterstam</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Wilterson</surname><given-names>A. I.</given-names></name>, <name name-style="western"><surname>Wachtell</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Graziano</surname><given-names>M. S. A.</given-names></name></person-group>, <article-title>Other people’s gaze encoded as implied motion in the human brain</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><volume>117</volume>, <fpage>13162</fpage>–<lpage>13167</lpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32457153</pub-id></mixed-citation>
              </ref>
              <ref id="r3">
                <label>3</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guterstam</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Graziano</surname><given-names>M. S. A.</given-names></name></person-group>, <article-title>Implied motion as a possible mechanism for encoding other people’s attention</article-title>. <source>Prog. Neurobiol.</source><volume>190</volume>, <fpage>101797</fpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32217129</pub-id></mixed-citation>
              </ref>
              <ref id="r4">
                <label>4</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Görner</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Ramezanpour</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Chong</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Thier</surname><given-names>P.</given-names></name></person-group>, <article-title>Does the brain encode the gaze of others as beams emitted by their eyes?</article-title><source>Proc. Natl. Acad. Sci. U.S.A.</source><volume>117</volume>, <fpage>20375</fpage>–<lpage>20376</lpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32843563</pub-id></mixed-citation>
              </ref>
              <ref id="r5">
                <label>5</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guterstam</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Graziano</surname><given-names>M. S. A.</given-names></name></person-group>, <article-title>Reply to Görner et al.: Encoding gaze as implied motion</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><volume>117</volume>, <fpage>20377</fpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32843564</pub-id></mixed-citation>
              </ref>
              <ref id="r6">
                <label>6</label>
                <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Baron-Cohen</surname><given-names>S.</given-names></name></person-group>, <source>Mindblindness: An Essay on Autism and Theory of Mind</source> (<publisher-name>MIT Press</publisher-name>, <year>1997</year>).</mixed-citation>
              </ref>
              <ref id="r7">
                <label>7</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Graziano</surname><given-names>M. S. A.</given-names></name>, <name name-style="western"><surname>Kastner</surname><given-names>S.</given-names></name></person-group>, <article-title>Human consciousness and its relationship to social neuroscience: A novel hypothesis</article-title>. <source>Cogn. Neurosci.</source><volume>2</volume>, <fpage>98</fpage>–<lpage>113</lpage> (<year>2011</year>).<pub-id pub-id-type="pmid">22121395</pub-id></mixed-citation>
              </ref>
              <ref id="r8">
                <label>8</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Frischen</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Bayliss</surname><given-names>A. P.</given-names></name>, <name name-style="western"><surname>Tipper</surname><given-names>S. P.</given-names></name></person-group>, <article-title>Gaze cueing of attention: Visual attention, social cognition, and individual differences</article-title>. <source>Psychol. Bull.</source><volume>133</volume>, <fpage>694</fpage>–<lpage>724</lpage> (<year>2007</year>).<pub-id pub-id-type="pmid">17592962</pub-id></mixed-citation>
              </ref>
              <ref id="r9">
                <label>9</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Calder</surname><given-names>A. J.</given-names></name><etal/></person-group>, <article-title>Reading the mind from eye gaze</article-title>. <source>Neuropsychologia</source><volume>40</volume>, <fpage>1129</fpage>–<lpage>1138</lpage> (<year>2002</year>).<pub-id pub-id-type="pmid">11931917</pub-id></mixed-citation>
              </ref>
              <ref id="r10">
                <label>10</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Palan</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Schitter</surname><given-names>C.</given-names></name></person-group>, <article-title>Prolific.ac—A subject pool for online experiments</article-title>. <source>J. Behav. Exp. Finance</source><volume>17</volume>, <fpage>22</fpage>–<lpage>27</lpage> (<year>2018</year>).</mixed-citation>
              </ref>
              <ref id="r11">
                <label>11</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Oppenheimer</surname><given-names>D. M.</given-names></name>, <name name-style="western"><surname>Meyvis</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Davidenko</surname><given-names>N.</given-names></name></person-group>, <article-title>Instructional manipulation checks: Detecting satisficing to increase statistical power</article-title>. <source>J. Exp. Soc. Psychol.</source><volume>45</volume>, <fpage>867</fpage>–<lpage>872</lpage> (<year>2009</year>).</mixed-citation>
              </ref>
              <ref id="r12">
                <label>12</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>de Leeuw</surname><given-names>J. R.</given-names></name></person-group>, <article-title>jsPsych: A JavaScript library for creating behavioral experiments in a Web browser</article-title>. <source>Behav. Res. Methods</source><volume>47</volume>, <fpage>1</fpage>–<lpage>12</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">24683129</pub-id></mixed-citation>
              </ref>
              <ref id="bib13">
                <label>13.</label>
                <mixed-citation publication-type="data"><person-group person-group-type="author"><name name-style="western"><surname>Guterstam</surname><given-names>A.</given-names></name></person-group>, <article-title>Visual motion assists in social cognition</article-title>. <source>Figshare</source>. Dataset. <pub-id pub-id-type="doi">10.6084/m9.figshare.12665078.v1</pub-id>. Deposited 30 October 2020.</mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
